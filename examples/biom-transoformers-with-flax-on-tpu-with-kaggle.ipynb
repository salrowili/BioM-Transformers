{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Step#1 : Installing Required Libraries**","metadata":{"_uuid":"321aca5d-54d3-4a52-b188-02a6cd095dfa","_cell_guid":"2d5d4774-3156-4d37-9b88-d02ffe3ac88d","trusted":true}},{"cell_type":"code","source":"!pip3 install transformers==4.34.1\n!pip install datasets==2.14.6\n!pip install evaluate==0.4.1\n!pip3 install protobuf==3.20.3\n!pip3 install accelerate==0.24.0\n!pip3 install wandb\n!pip3 install sentencepiece\n!pip3 install seqeval\n!wandb disabled ## If you want to log your experiment, remove this line","metadata":{"_uuid":"a723919c-eb26-4ae1-8e67-4900779b53e3","_cell_guid":"3dd12a70-7b65-40c0-af50-580bdf4afc7a","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-04T20:52:46.589872Z","iopub.execute_input":"2023-11-04T20:52:46.590615Z","iopub.status.idle":"2023-11-04T20:53:32.082274Z","shell.execute_reply.started":"2023-11-04T20:52:46.590576Z","shell.execute_reply":"2023-11-04T20:53:32.081135Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.34.1 in /usr/local/lib/python3.8/site-packages (4.34.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (2023.10.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (23.1)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (0.14.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (3.12.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (4.65.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (0.4.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.8/site-packages (from transformers==4.34.1) (0.17.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (4.7.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.34.1) (2023.5.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.34.1) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.34.1) (1.26.16)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.34.1) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: datasets==2.14.6 in /usr/local/lib/python3.8/site-packages (2.14.6)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (3.4.1)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (0.3.7)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (4.65.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (23.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (3.8.6)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (0.70.15)\nRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (14.0.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (0.17.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (6.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (2.31.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (1.23.5)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.8/site-packages (from datasets==2.14.6) (2023.10.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (1.3.1)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (3.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (6.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (23.1.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.14.6) (1.9.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.7.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6) (2023.5.7)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.14.6) (2023.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.14.6) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.14.6) (2023.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.8/site-packages (0.4.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (2023.10.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (23.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (0.3.7)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (4.65.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (2.0.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (2.14.6)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (0.70.15)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (2.31.0)\nRequirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (0.18.0)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (3.4.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from evaluate==0.4.1) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (14.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.8.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (4.7.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1) (3.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1) (2023.5.7)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/site-packages (from pandas->evaluate==0.4.1) (2023.3)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas->evaluate==0.4.1) (2023.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/site-packages (from pandas->evaluate==0.4.1) (2.8.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.9.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (6.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.1) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.8/site-packages (3.20.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: accelerate==0.24.0 in /usr/local/lib/python3.8/site-packages (0.24.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from accelerate==0.24.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from accelerate==0.24.0) (23.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.8/site-packages (from accelerate==0.24.0) (5.9.5)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/site-packages (from accelerate==0.24.0) (0.17.3)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/site-packages (from accelerate==0.24.0) (2.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/site-packages (from accelerate==0.24.0) (6.0)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.4.0.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (1.12)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.7.99)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (3.12.2)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (10.9.0.58)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.7.4.91)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.10.3.66)\nRequirement already satisfied: networkx in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (3.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.7.99)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (2.14.3)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (2.0.0)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.7.101)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (3.1.2)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (10.2.10.91)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (11.7.91)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.24.0) (4.7.1)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.24.0) (0.40.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.24.0) (57.5.0)\nRequirement already satisfied: lit in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.24.0) (16.0.6)\nRequirement already satisfied: cmake in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.24.0) (3.26.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/site-packages (from huggingface-hub->accelerate==0.24.0) (4.65.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface-hub->accelerate==0.24.0) (2023.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from huggingface-hub->accelerate==0.24.0) (2.31.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate==0.24.0) (2.1.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate==0.24.0) (1.26.16)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate==0.24.0) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate==0.24.0) (3.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate==0.24.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate==0.24.0) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: wandb in /usr/local/lib/python3.8/site-packages (0.12.16)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/site-packages (from wandb) (5.9.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from wandb) (57.5.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/site-packages (from wandb) (2.8.2)\nRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/site-packages (from wandb) (2.3)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/site-packages (from wandb) (8.1.4)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: pathtools in /usr/local/lib/python3.8/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/site-packages (from wandb) (1.27.1)\nRequirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/site-packages (from wandb) (1.0.11)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.8/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/site-packages (from wandb) (6.0)\nRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/site-packages (0.1.99)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/site-packages (from seqeval) (1.23.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/site-packages (from seqeval) (1.3.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=01fa32b11f3da1184c7bacf7ac4bf6326b48754216b9f9f83b2a34c7e0acd633\n  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nW&B disabled.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Step#2 : Download the pre-processed BLURB dataset and FLAX codes**","metadata":{"_uuid":"ad8d38df-243b-4576-95c4-2c8d7314585b","_cell_guid":"43879f6b-2e17-4d9d-aa57-0a1ce9ce0f6f","trusted":true}},{"cell_type":"markdown","source":"Here we will download the pre-processed dataset for BLURB benchmark from LinkBERT team.\nWe need also to download our FLAX code, which is based on Hugginggface pytorch examples and LinkBERT team codes, so all credits for both.","metadata":{"_uuid":"2ed3b742-db5d-450f-a067-c464327a0b74","_cell_guid":"8e1a1656-2d1c-4831-95d4-658c80e2ac9d","trusted":true}},{"cell_type":"code","source":"!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n!unzip data.zip\n!wget https://raw.githubusercontent.com/salrowili/BioM-Transformers/main/misc/run_flax_seq.py\n!wget https://raw.githubusercontent.com/salrowili/BioM-Transformers/main/misc/run_flax_ner.py","metadata":{"_uuid":"f624960a-8e78-4615-99bc-187285dc4cac","_cell_guid":"305f3d8b-336f-4051-a6db-8c883ebd90e2","collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true,"execution":{"iopub.status.busy":"2023-11-04T19:59:24.969574Z","iopub.execute_input":"2023-11-04T19:59:24.970014Z","iopub.status.idle":"2023-11-04T20:00:05.323050Z","shell.execute_reply.started":"2023-11-04T19:59:24.969978Z","shell.execute_reply":"2023-11-04T20:00:05.321802Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2023-11-04 19:59:26--  https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 458233246 (437M) [application/zip]\nSaving to: ‘data.zip’\n\ndata.zip            100%[===================>] 437.00M  13.8MB/s    in 21s     \n\n2023-11-04 19:59:47 (20.3 MB/s) - ‘data.zip’ saved [458233246/458233246]\n\nArchive:  data.zip\n   creating: data/\n   creating: data/tokcls/\n   creating: data/tokcls/BC5CDR-disease_hf/\n  inflating: data/tokcls/BC5CDR-disease_hf/test.json  \n  inflating: data/tokcls/BC5CDR-disease_hf/train.json  \n  inflating: data/tokcls/BC5CDR-disease_hf/dev.json  \n   creating: data/tokcls/BC2GM_hf/\n  inflating: data/tokcls/BC2GM_hf/train.json  \n  inflating: data/tokcls/BC2GM_hf/dev.json  \n  inflating: data/tokcls/BC2GM_hf/test.json  \n   creating: data/tokcls/ebmnlp_hf/\n  inflating: data/tokcls/ebmnlp_hf/test.json  \n  inflating: data/tokcls/ebmnlp_hf/train.json  \n  inflating: data/tokcls/ebmnlp_hf/dev.json  \n   creating: data/tokcls/NCBI-disease_hf/\n  inflating: data/tokcls/NCBI-disease_hf/test.json  \n  inflating: data/tokcls/NCBI-disease_hf/dev.json  \n  inflating: data/tokcls/NCBI-disease_hf/train.json  \n   creating: data/tokcls/JNLPBA_hf/\n  inflating: data/tokcls/JNLPBA_hf/dev.json  \n  inflating: data/tokcls/JNLPBA_hf/train.json  \n  inflating: data/tokcls/JNLPBA_hf/test.json  \n   creating: data/tokcls/BC5CDR-chem_hf/\n  inflating: data/tokcls/BC5CDR-chem_hf/dev.json  \n  inflating: data/tokcls/BC5CDR-chem_hf/test.json  \n  inflating: data/tokcls/BC5CDR-chem_hf/train.json  \n   creating: data/mc/\n   creating: data/mc/mmlu_hf/\n   creating: data/mc/mmlu_hf/professional_medicine/\n  inflating: data/mc/mmlu_hf/professional_medicine/dev.json  \n  inflating: data/mc/mmlu_hf/professional_medicine/test.json  \n  inflating: data/mc/mmlu_hf/professional_medicine/val.json  \n   creating: data/mc/medqa_usmle_hf/\n  inflating: data/mc/medqa_usmle_hf/test.json  \n  inflating: data/mc/medqa_usmle_hf/dev.json  \n  inflating: data/mc/medqa_usmle_hf/train.json  \n   creating: data/qa/\n   creating: data/qa/naturalqa_hf/\n  inflating: data/qa/naturalqa_hf/train_0.1.json  \n  inflating: data/qa/naturalqa_hf/train.json  \n  inflating: data/qa/naturalqa_hf/test.json  \n  inflating: data/qa/naturalqa_hf/dev.json  \n   creating: data/qa/triviaqa_hf/\n  inflating: data/qa/triviaqa_hf/train_0.1.json  \n  inflating: data/qa/triviaqa_hf/train.json  \n  inflating: data/qa/triviaqa_hf/dev.json  \n  inflating: data/qa/triviaqa_hf/test.json  \n   creating: data/qa/squad_hf/\n  inflating: data/qa/squad_hf/test.json  \n  inflating: data/qa/squad_hf/dev.json  \n  inflating: data/qa/squad_hf/train_0.1.json  \n  inflating: data/qa/squad_hf/train.json  \n   creating: data/qa/newsqa_hf/\n  inflating: data/qa/newsqa_hf/dev.json  \n  inflating: data/qa/newsqa_hf/test.json  \n  inflating: data/qa/newsqa_hf/train.json  \n   creating: data/qa/searchqa_hf/\n  inflating: data/qa/searchqa_hf/test.json  \n  inflating: data/qa/searchqa_hf/train.json  \n  inflating: data/qa/searchqa_hf/dev.json  \n   creating: data/qa/hotpot_hf/\n  inflating: data/qa/hotpot_hf/test.json  \n  inflating: data/qa/hotpot_hf/dev.json  \n  inflating: data/qa/hotpot_hf/train.json  \n  inflating: data/qa/hotpot_hf/train_0.1.json  \n   creating: data/seqcls/\n   creating: data/seqcls/hoc_hf/\n  inflating: data/seqcls/hoc_hf/dev.json  \n  inflating: data/seqcls/hoc_hf/train.json  \n  inflating: data/seqcls/hoc_hf/test.json  \n   creating: data/seqcls/BIOSSES_hf/\n  inflating: data/seqcls/BIOSSES_hf/train.json  \n  inflating: data/seqcls/BIOSSES_hf/dev.json  \n  inflating: data/seqcls/BIOSSES_hf/test.json  \n   creating: data/seqcls/GAD_hf/\n  inflating: data/seqcls/GAD_hf/train.json  \n  inflating: data/seqcls/GAD_hf/dev.json  \n  inflating: data/seqcls/GAD_hf/test.json  \n   creating: data/seqcls/HoC_hf/\n  inflating: data/seqcls/HoC_hf/dev.json  \n  inflating: data/seqcls/HoC_hf/test.json  \n  inflating: data/seqcls/HoC_hf/train.json  \n   creating: data/seqcls/chemprot_hf/\n  inflating: data/seqcls/chemprot_hf/train.json  \n  inflating: data/seqcls/chemprot_hf/dev.json  \n  inflating: data/seqcls/chemprot_hf/test.json  \n   creating: data/seqcls/bioasq_hf/\n  inflating: data/seqcls/bioasq_hf/test.json  \n  inflating: data/seqcls/bioasq_hf/dev.json  \n  inflating: data/seqcls/bioasq_hf/train.json  \n   creating: data/seqcls/pubmedqa_hf/\n  inflating: data/seqcls/pubmedqa_hf/dev.json  \n  inflating: data/seqcls/pubmedqa_hf/test.json  \n  inflating: data/seqcls/pubmedqa_hf/train.json  \n   creating: data/seqcls/DDI_hf/\n  inflating: data/seqcls/DDI_hf/train.json  \n  inflating: data/seqcls/DDI_hf/dev.json  \n  inflating: data/seqcls/DDI_hf/test.json  \n--2023-11-04 20:00:03--  https://raw.githubusercontent.com/salrowili/BioM-Transformers/main/misc/run_flax_seq.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 34359 (34K) [text/plain]\nSaving to: ‘run_flax_seq.py’\n\nrun_flax_seq.py     100%[===================>]  33.55K  --.-KB/s    in 0.004s  \n\n2023-11-04 20:00:03 (8.28 MB/s) - ‘run_flax_seq.py’ saved [34359/34359]\n\n--2023-11-04 20:00:05--  https://raw.githubusercontent.com/salrowili/BioM-Transformers/main/misc/run_flax_ner.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 36885 (36K) [text/plain]\nSaving to: ‘run_flax_ner.py’\n\nrun_flax_ner.py     100%[===================>]  36.02K  --.-KB/s    in 0.004s  \n\n2023-11-04 20:00:05 (8.98 MB/s) - ‘run_flax_ner.py’ saved [36885/36885]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We need to disable PJRT_DEVICE setting, otherwise the notebook will throw an \"Unexpected PJRT_Api size\" error. Please make sure to run the cell below everytime you restart this notebook.","metadata":{"_uuid":"39d63d06-c8dc-44c9-bd35-a9436bd22c6d","_cell_guid":"a1c6f9aa-b750-47e5-8b89-f3467a0c425b","trusted":true}},{"cell_type":"code","source":"import os\n# 아직 PJRT를 쓰면 UNEXPECTED PJRT_Api size 오류가 남\nos.environ.pop('PJRT_DEVICE', None)\nos.environ['XRT_TPU_CONFIG'] = 'localservice;0;localhost:51011'","metadata":{"_uuid":"b89261c1-ae86-4a12-bb6f-412ccad83233","_cell_guid":"1382cb9e-f79e-47c1-abb5-ef0a5621887c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-04T20:00:39.083340Z","iopub.execute_input":"2023-11-04T20:00:39.083795Z","iopub.status.idle":"2023-11-04T20:00:39.088768Z","shell.execute_reply.started":"2023-11-04T20:00:39.083756Z","shell.execute_reply":"2023-11-04T20:00:39.088090Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Step#3 : FineTuning BioM-Transformers with FLAX on BLURB dataset**","metadata":{}},{"cell_type":"markdown","source":"Here we pick couple of examples with BioM-Transformers on a list of tasks. After each run, the predictions will be saved as BLURB format. After the code is finished, go to the right panel of this Kaggle, and you will see \"output\" section, showing the saved model and the file that saves the predictions (e.g, BioM-ALBERT-xxlarge_ChemProt.json).","metadata":{}},{"cell_type":"code","source":"!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n--train_file data/seqcls/chemprot_hf/train.json \\\n--validation_file data/seqcls/chemprot_hf/dev.json \\\n--test_file data/seqcls/chemprot_hf/test.json \\\n--do_train \\\n--do_eval \\\n--per_device_train_batch_size 3 \\\n--per_device_eval_batch_size 8 \\\n--learning_rate 3e-5 \\\n--num_train_epochs 6 \\\n--max_seq_length 256 \\\n--save_strategy no \\\n--evaluation_strategy no \\\n--output_dir out \\\n--overwrite_output_dir \\\n--save_strategy no \\\n--evaluation_strategy no \\\n--eval_steps 100000 \\\n--metric_name PRF1 \\\n--blurb_task ChemProt","metadata":{"_uuid":"69ca9353-f305-40d9-af5e-2ce287b7f7ad","_cell_guid":"b2f5a91e-4fc6-475a-b7c1-a0f15e4f4e40","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-04T20:08:09.026221Z","iopub.execute_input":"2023-11-04T20:08:09.026675Z","iopub.status.idle":"2023-11-04T20:48:00.192873Z","shell.execute_reply.started":"2023-11-04T20:08:09.026627Z","shell.execute_reply":"2023-11-04T20:48:00.191609Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\nYou can remove this warning by passing 'token=None' instead.\n  warnings.warn(\nDownloading data files: 100%|██████████████████| 3/3 [00:00<00:00, 16578.28it/s]\nExtracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1577.20it/s]\nGenerating train split: 18035 examples [00:00, 639898.09 examples/s]\nGenerating validation split: 11268 examples [00:00, 964912.57 examples/s]\nGenerating test split: 15745 examples [00:00, 913318.44 examples/s]\nDownloading (…)lve/main/config.json: 100%|██████| 706/706 [00:00<00:00, 145kB/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/config.json\nModel config AlbertConfig {\n  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 4096,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\"\n  },\n  \"initializer_range\": 0.01,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 16384,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"layers_to_keep\": [],\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 64,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\nCould not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/config.json\nModel config AlbertConfig {\n  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.01,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 16384,\n  \"layer_norm_eps\": 1e-12,\n  \"layers_to_keep\": [],\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 64,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\nDownloading spiece.model: 100%|██████████████| 778k/778k [00:00<00:00, 9.91MB/s]\nloading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/spiece.model\nloading file tokenizer.json from cache at None\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/config.json\nModel config AlbertConfig {\n  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.01,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 16384,\n  \"layer_norm_eps\": 1e-12,\n  \"layers_to_keep\": [],\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 64,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/config.json\nModel config AlbertConfig {\n  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n  \"architectures\": [\n    \"AlbertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0,\n  \"bos_token_id\": 2,\n  \"classifier_dropout_prob\": 0.1,\n  \"down_scale_factor\": 1,\n  \"embedding_size\": 128,\n  \"eos_token_id\": 3,\n  \"gap_size\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0,\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.01,\n  \"inner_group_num\": 1,\n  \"intermediate_size\": 16384,\n  \"layer_norm_eps\": 1e-12,\n  \"layers_to_keep\": [],\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"albert\",\n  \"net_structure_type\": 0,\n  \"num_attention_heads\": 64,\n  \"num_hidden_groups\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_memory_blocks\": 0,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30000\n}\n\nDownloading pytorch_model.bin: 100%|█████████| 893M/893M [00:12<00:00, 68.9MB/s]\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/pytorch_model.bin\nLoading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/0ee529e434858b43bbf795853e272a1e77b65bc9/pytorch_model.bin\nPyTorch checkpoint contains 227,028,962 parameters.\nSome weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'dense', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'decoder', 'kernel'), ('predictions', 'bias')}\n- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nMap: 100%|███████████████████████| 18035/18035 [00:02<00:00, 6210.08 examples/s]\nMap: 100%|███████████████████████| 11268/11268 [00:02<00:00, 5380.90 examples/s]\nMap: 100%|███████████████████████| 15745/15745 [00:02<00:00, 6288.37 examples/s]\nINFO:__main__:Sample 9550 of the training set: {'input_ids': [2, 26, 286, 23, 21, 6388, 4899, 7056, 2562, 21, 301, 3793, 10, 229, 1292, 24, 21, 196, 2623, 251, 23, 886, 26, 2223, 24, 22, 1014, 3352, 24, 1014, 238, 10, 2623, 251, 21, 5, 18647, 10, 2623, 251, 6, 3129, 23, 3417, 160, 1214, 25, 21, 6388, 3567, 7056, 23, 61, 44, 204, 854, 407, 26, 22, 816, 24, 26, 2185, 106, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\nINFO:__main__:Sample 1471 of the training set: {'input_ids': [2, 521, 23, 68, 59, 110, 36, 21, 7024, 89, 10, 10392, 23, 28, 21, 6388, 4899, 7056, 4175, 23, 2846, 771, 1096, 218, 238, 21, 5, 6388, 3567, 7056, 6, 10, 699, 3796, 674, 26, 153, 1096, 57, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\nINFO:__main__:Sample 11060 of the training set: {'input_ids': [2, 22, 947, 2851, 268, 21, 11965, 8963, 23841, 4531, 48, 85, 1310, 119, 131, 21, 5, 7059, 2250, 38, 131, 6, 23, 1305, 947, 15259, 10, 39, 23, 21, 6388, 3567, 7056, 21, 5, 85, 13817, 6, 23, 21, 6388, 4899, 7056, 10, 85, 13817, 23, 25, 21, 2129, 30, 23457, 4000, 119, 44, 204, 32, 3252, 25, 2821, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\nDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 20.4MB/s]\nINFO:__main__:===== Starting training (6 epochs) =====\nStep... (500/4506 | Training Loss: 0.5012228488922119, Learning Rate: 2.6677762434701435e-05)\nTraining...:  66%|██████████████████▌         | 499/751 [03:51<01:39,  2.53it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating ...: 100%|█████████████████████████| 177/177 [00:59<00:00,  2.95it/s]\nINFO:__main__:{'eval_precision': 0.8088567555354722, 'eval_recall': 0.7433554817275747, 'eval_F1': 0.7747240856957368}| Step... (751/4506 | Eval metrics: {'accuracy': 0.913471778487753}) \nTraining...: 100%|████████████████████████████| 751/751 [06:36<00:00,  1.89it/s]\nStep... (1000/4506 | Training Loss: 0.0969313532114029, Learning Rate: 2.3348868126049638e-05)\nTraining...:  33%|█████████▏                  | 248/751 [01:39<03:22,  2.49it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (1500/4506 | Training Loss: 0.044815197587013245, Learning Rate: 2.0019971998408437e-05)\nTraining...: 100%|███████████████████████████▉| 748/751 [05:07<00:01,  2.49it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating ...: 100%|█████████████████████████| 177/177 [00:53<00:00,  3.33it/s]\nINFO:__main__:{'eval_precision': 0.8117229129662522, 'eval_recall': 0.7591362126245847, 'eval_F1': 0.784549356223176}| Step... (1502/4506 | Eval metrics: {'accuracy': 0.9136492722754703}) \nTraining...: 100%|████████████████████████████| 751/751 [06:07<00:00,  2.05it/s]\nStep... (2000/4506 | Training Loss: 0.26218318939208984, Learning Rate: 1.6691079508746043e-05)\nTraining...:  66%|██████████████████▌         | 497/751 [03:20<01:41,  2.50it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating ...: 100%|█████████████████████████| 177/177 [00:53<00:00,  3.33it/s]\nINFO:__main__:{'eval_precision': 0.803030303030303, 'eval_recall': 0.792358803986711, 'eval_F1': 0.7976588628762542}| Step... (2253/4506 | Eval metrics: {'accuracy': 0.917199148029819}) \nTraining...: 100%|████████████████████████████| 751/751 [06:00<00:00,  2.08it/s]\nStep... (2500/4506 | Training Loss: 0.030790988355875015, Learning Rate: 1.3362183381104842e-05)\nTraining...:  33%|█████████▏                  | 246/751 [01:39<03:21,  2.51it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (3000/4506 | Training Loss: 0.03681471198797226, Learning Rate: 1.0033289072453044e-05)\nTraining...:  99%|███████████████████████████▊| 746/751 [05:06<00:01,  2.50it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating ...: 100%|█████████████████████████| 177/177 [00:53<00:00,  3.31it/s]\nINFO:__main__:{'eval_precision': 0.8196078431372549, 'eval_recall': 0.7811461794019934, 'eval_F1': 0.7999149479055921}| Step... (3004/4506 | Eval metrics: {'accuracy': 0.9191515796947107}) \nTraining...: 100%|████████████████████████████| 751/751 [06:07<00:00,  2.04it/s]\nStep... (3500/4506 | Training Loss: 0.007853616029024124, Learning Rate: 6.704393854306545e-06)\nTraining...:  66%|██████████████████▍         | 495/751 [03:18<01:41,  2.52it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating ...: 100%|█████████████████████████| 177/177 [00:53<00:00,  3.32it/s]\nINFO:__main__:{'eval_precision': 0.8051091882983107, 'eval_recall': 0.8114617940199336, 'eval_F1': 0.8082730093071354}| Step... (3755/4506 | Eval metrics: {'accuracy': 0.9205715299964501}) \nTraining...: 100%|████████████████████████████| 751/751 [06:00<00:00,  2.08it/s]\nStep... (4000/4506 | Training Loss: 0.0006774761131964624, Learning Rate: 3.3754990909073967e-06)\nTraining...:  32%|█████████                   | 244/751 [01:38<03:23,  2.50it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (4500/4506 | Training Loss: 0.003953217063099146, Learning Rate: 4.660427421754321e-08)\nTraining...:  99%|███████████████████████████▋| 744/751 [05:04<00:02,  2.49it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating ...: 100%|█████████████████████████| 177/177 [00:53<00:00,  3.33it/s]\nINFO:__main__:{'eval_precision': 0.809002944888515, 'eval_recall': 0.7985880398671097, 'eval_F1': 0.8037617554858935}| Step... (4506/4506 | Eval metrics: {'accuracy': 0.9195065672701456}) \nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nTraining...: 100%|████████████████████████████| 751/751 [06:12<00:00,  2.02it/s]\nEpoch ... 6/6: 100%|█████████████████████████████| 6/6 [37:05<00:00, 370.90s/it]\nEvaluating on Test Data ...: 100%|████████████| 247/247 [01:13<00:00,  3.34it/s]\nINFO:__main__: test results : {'test_precision': 0.8358348968105066, 'test_recall': 0.7793002915451895, 'test_F1': 0.8065781532890767}\n","output_type":"stream"}]},{"cell_type":"markdown","source":" As you can see here, we achieved an F1 score (test_f1) of 80.65 on the chemport task.","metadata":{}},{"cell_type":"code","source":"!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n--train_file data/tokcls/BC5CDR-disease_hf/train.json \\\n--validation_file data/tokcls/BC5CDR-disease_hf/dev.json \\\n--test_file data/tokcls/BC5CDR-disease_hf/test.json \\\n--do_train \\\n--do_eval \\\n--do_test \\\n--per_device_train_batch_size 1 \\\n--learning_rate 7e-5 \\\n--num_train_epochs 7 \\\n--output_dir out \\\n--overwrite_output_dir \\\n--blurb_task BC5-disease \\\n--max_seq_length 512 \\\n--return_entity_level_metrics \\\n--eval_steps 1000","metadata":{"execution":{"iopub.status.busy":"2023-11-04T21:28:10.096845Z","iopub.execute_input":"2023-11-04T21:28:10.097274Z","iopub.status.idle":"2023-11-04T21:37:21.038791Z","shell.execute_reply.started":"2023-11-04T21:28:10.097242Z","shell.execute_reply":"2023-11-04T21:37:21.037583Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\nYou can remove this warning by passing 'token=None' instead.\n  warnings.warn(\nDownloading data files: 100%|██████████████████| 3/3 [00:00<00:00, 17476.27it/s]\nExtracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1695.12it/s]\nGenerating train split: 4560 examples [00:00, 242731.47 examples/s]\nGenerating validation split: 4581 examples [00:00, 416196.05 examples/s]\nGenerating test split: 4797 examples [00:00, 449209.12 examples/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"finetuning_task\": \"ner\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"B\",\n    \"1\": \"I\",\n    \"2\": \"O\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"B\": 0,\n    \"I\": 1,\n    \"O\": 2\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nCould not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nloading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/vocab.txt\nloading file tokenizer.json from cache at None\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/pytorch_model.bin\nLoading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/pytorch_model.bin\nPyTorch checkpoint contains 108,233,985 parameters.\nSome weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense', 'bias')}\n- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nRunning tokenizer on dataset: 100%|█| 4560/4560 [00:02<00:00, 2163.43 examples/s\nRunning tokenizer on dataset: 100%|█| 4581/4581 [00:01<00:00, 2357.41 examples/s\nRunning tokenizer on dataset: 100%|█| 4797/4797 [00:02<00:00, 2389.85 examples/s\nINFO:__main__:Sample 3319 of the training set: {'input_ids': [2, 1805, 5762, 6504, 1760, 5612, 20843, 1883, 2193, 11301, 1717, 3061, 5897, 16, 2326, 2959, 5388, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 0, 2, 2, 2, -100, -100, -100, 2, 2, 2, 0, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\nINFO:__main__:Sample 243 of the training set: {'input_ids': [2, 1802, 2255, 42, 2632, 1685, 42, 5126, 5459, 3296, 3650, 10070, 2386, 2829, 14522, 16, 2326, 19696, 1760, 8579, 1701, 1680, 8420, 1685, 14522, 1682, 10257, 1715, 28183, 1690, 6534, 9742, 2837, 5941, 2543, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\nINFO:__main__:Sample 4392 of the training set: {'input_ids': [2, 1680, 2587, 1685, 1805, 1901, 1734, 1701, 3969, 1805, 6739, 3099, 1690, 1701, 3357, 2187, 2446, 9946, 1680, 4108, 1685, 6739, 3099, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\nINFO:__main__:===== Starting training (7 epochs) =====\nStep... (500/3990 | Training Loss: 0.002334518125280738, Learning Rate: 6.124561332399026e-05)\nTraining...:  88%|████████████████████████▌   | 499/570 [01:49<00:04, 14.92it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:15<00:00,  4.78it/s]\nINFO:__main__:Step... (570/3990 | Validation metrics: {'__precision': 0.7550200803212851, '__recall': 0.8412623645784267, '__f1': 0.7958115183246074, '__number': 4246, 'overall_precision': 0.7550200803212851, 'overall_recall': 0.8412623645784267, 'overall_f1': 0.7958115183246074, 'overall_accuracy': 0.9789873396166977}\nTraining...: 100%|████████████████████████████| 570/570 [02:14<00:00,  4.25it/s]\nStep... (1000/3990 | Training Loss: 0.001392582431435585, Learning Rate: 5.2473686082521453e-05)\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.77it/s]\nINFO:__main__:Step... (1000/3990 | Validation metrics: {'__precision': 0.8116612679315132, '__recall': 0.826189354686764, '__f1': 0.8188608776844071, '__number': 4246, 'overall_precision': 0.8116612679315132, 'overall_recall': 0.826189354686764, 'overall_f1': 0.8188608776844071, 'overall_accuracy': 0.9817969741087924}\nConfiguration saved in /kaggle/working/out/config.json\nTraining...:  75%|█████████████████████       | 428/570 [00:41<00:08, 15.91it/s]Model weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.96it/s]\nTraining...: 100%|███████████████████████████▉| 568/570 [01:01<00:00, 15.51it/s]INFO:__main__:Step... (1140/3990 | Validation metrics: {'__precision': 0.8188026405645344, '__recall': 0.8471502590673575, '__f1': 0.8327352702859128, '__number': 4246, 'overall_precision': 0.8188026405645344, 'overall_recall': 0.8471502590673575, 'overall_f1': 0.8327352702859128, 'overall_accuracy': 0.9838318306045822}\nTraining...: 100%|████████████████████████████| 570/570 [01:02<00:00,  9.10it/s]\nStep... (1500/3990 | Training Loss: 0.00047164291027002037, Learning Rate: 4.3701755203073844e-05)\nTraining...:  63%|█████████████████▌          | 358/570 [00:23<00:13, 15.34it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.86it/s]\nINFO:__main__:Step... (1710/3990 | Validation metrics: {'__precision': 0.806206742576468, '__recall': 0.8504474799811588, '__f1': 0.8277363896848139, '__number': 4246, 'overall_precision': 0.806206742576468, 'overall_recall': 0.8504474799811588, 'overall_f1': 0.8277363896848139, 'overall_accuracy': 0.9813712719130205}\nTraining...: 100%|████████████████████████████| 570/570 [00:51<00:00, 11.13it/s]\nStep... (2000/3990 | Training Loss: 7.35913054086268e-05, Learning Rate: 3.492982796160504e-05)\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.87it/s]\nINFO:__main__:Step... (2000/3990 | Validation metrics: {'__precision': 0.8316808854046576, '__recall': 0.8495054168629298, '__f1': 0.8404986601421415, '__number': 4246, 'overall_precision': 0.8316808854046576, 'overall_recall': 0.8495054168629298, 'overall_f1': 0.8404986601421415, 'overall_accuracy': 0.9845129541178174}\nConfiguration saved in /kaggle/working/out/config.json\nTraining...:  51%|██████████████▏             | 288/570 [00:31<00:18, 15.11it/s]Model weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.80it/s]\nTraining...: 100%|███████████████████████████▉| 568/570 [01:01<00:00, 15.75it/s]INFO:__main__:Step... (2280/3990 | Validation metrics: {'__precision': 0.8280254777070064, '__recall': 0.8572774375883184, '__f1': 0.8423975931497338, '__number': 4246, 'overall_precision': 0.8280254777070064, 'overall_recall': 0.8572774375883184, 'overall_f1': 0.8423975931497338, 'overall_accuracy': 0.9850067686649128}\nTraining...: 100%|████████████████████████████| 570/570 [01:02<00:00,  9.19it/s]\nStep... (2500/3990 | Training Loss: 2.7820531158795347e-06, Learning Rate: 2.6157897082157433e-05)\nTraining...:  38%|██████████▋                 | 218/570 [00:14<00:22, 15.95it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.94it/s]\nTraining...: 100%|███████████████████████████▉| 568/570 [00:49<00:00, 15.69it/s]INFO:__main__:Step... (2850/3990 | Validation metrics: {'__precision': 0.8368200836820083, '__recall': 0.8478568064060292, '__f1': 0.8423022929340196, '__number': 4246, 'overall_precision': 0.8368200836820083, 'overall_recall': 0.8478568064060292, 'overall_f1': 0.8423022929340196, 'overall_accuracy': 0.9850578529284054}\nTraining...: 100%|████████████████████████████| 570/570 [00:50<00:00, 11.28it/s]\nStep... (3000/3990 | Training Loss: 5.689356839866377e-06, Learning Rate: 1.7385966202709824e-05)\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.89it/s]\nINFO:__main__:Step... (3000/3990 | Validation metrics: {'__precision': 0.8366042902784117, '__recall': 0.8634008478568064, '__f1': 0.8497913769123783, '__number': 4246, 'overall_precision': 0.8366042902784117, 'overall_recall': 0.8634008478568064, 'overall_f1': 0.8497913769123783, 'overall_accuracy': 0.9848705439622658}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.78it/s]\nINFO:__main__:Step... (3420/3990 | Validation metrics: {'__precision': 0.8442850532161037, '__recall': 0.8593970796043335, '__f1': 0.8517740429505135, '__number': 4246, 'overall_precision': 0.8442850532161037, 'overall_recall': 0.8593970796043335, 'overall_f1': 0.8517740429505135, 'overall_accuracy': 0.9855005832120082}\nTraining...: 100%|████████████████████████████| 570/570 [01:01<00:00,  9.32it/s]\nStep... (3500/3990 | Training Loss: 1.5268409470081679e-06, Learning Rate: 8.614036232756916e-06)\nTraining...:  14%|███▉                         | 78/570 [00:05<00:31, 15.39it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|█████████████████| 72/72 [00:09<00:00,  7.87it/s]\nINFO:__main__:Step... (3990/3990 | Validation metrics: {'__precision': 0.84746554543331, '__recall': 0.8544512482336316, '__f1': 0.8509440600445642, '__number': 4246, 'overall_precision': 0.84746554543331, 'overall_recall': 0.8544512482336316, 'overall_f1': 0.8509440600445642, 'overall_accuracy': 0.9854324708606848}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nTraining...: 100%|████████████████████████████| 570/570 [00:52<00:00, 10.78it/s]\nEpoch ... 7/7: 100%|██████████████████████████████| 7/7 [07:54<00:00, 67.83s/it]\nEvaluating on Dev Set ...: 72it [00:09,  7.84it/s]                              \nINFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.84746554543331, '__recall': 0.8544512482336316, '__f1': 0.8509440600445642, '__number': 4246, 'overall_precision': 0.84746554543331, 'overall_recall': 0.8544512482336316, 'overall_f1': 0.8509440600445642, 'overall_accuracy': 0.9854324708606848}\nEvaluating on Test Set...: 75it [00:09,  7.77it/s]                              \nINFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8290727902946274, '__recall': 0.8650542495479204, '__f1': 0.8466814159292035, '__number': 4424, 'overall_precision': 0.8290727902946274, 'overall_recall': 0.8650542495479204, 'overall_f1': 0.8466814159292035, 'overall_accuracy': 0.9846733466933868}\n","output_type":"stream"}]},{"cell_type":"code","source":"!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n--train_file data/tokcls/ebmnlp_hf/train.json \\\n--validation_file data/tokcls/ebmnlp_hf/dev.json \\\n--test_file data/tokcls/ebmnlp_hf/test.json \\\n--do_train \\\n--do_eval \\\n--do_test \\\n--per_device_train_batch_size 1 \\\n--learning_rate 1e-5 \\\n--num_train_epochs 1 \\\n--output_dir out \\\n--overwrite_output_dir \\\n--blurb_task \"EBM PICO\" \\\n--max_seq_length 512 \\\n--return_macro_metrics \\\n--eval_steps 1000","metadata":{"execution":{"iopub.status.busy":"2023-11-04T21:46:55.650374Z","iopub.execute_input":"2023-11-04T21:46:55.651218Z","iopub.status.idle":"2023-11-04T21:58:46.335895Z","shell.execute_reply.started":"2023-11-04T21:46:55.651186Z","shell.execute_reply":"2023-11-04T21:58:46.334806Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\nYou can remove this warning by passing 'token=None' instead.\n  warnings.warn(\nDownloading data files: 100%|██████████████████| 3/3 [00:00<00:00, 21509.25it/s]\nExtracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1635.63it/s]\nGenerating train split: 40935 examples [00:00, 378016.22 examples/s]\nGenerating validation split: 10386 examples [00:00, 553998.90 examples/s]\nGenerating test split: 2076 examples [00:00, 265444.47 examples/s]\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"finetuning_task\": \"ner\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"B-INT\",\n    \"1\": \"B-OUT\",\n    \"2\": \"B-PAR\",\n    \"3\": \"O\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"B-INT\": 0,\n    \"B-OUT\": 1,\n    \"B-PAR\": 2,\n    \"O\": 3\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nCould not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nloading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/vocab.txt\nloading file tokenizer.json from cache at None\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/config.json\nModel config ElectraConfig {\n  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.34.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28895\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/pytorch_model.bin\nLoading PyTorch weights from /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/720be754b9e8a6531556756a5254c52c35e51a06/pytorch_model.bin\nPyTorch checkpoint contains 108,233,985 parameters.\nSome weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('discriminator_predictions', 'dense_prediction', 'bias'), ('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense', 'kernel')}\n- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nRunning tokenizer on dataset: 100%|█| 40935/40935 [00:17<00:00, 2275.61 examples\nRunning tokenizer on dataset: 100%|█| 10386/10386 [00:05<00:00, 2052.62 examples\nRunning tokenizer on dataset: 100%|█| 2076/2076 [00:00<00:00, 2355.01 examples/s\nINFO:__main__:Sample 27747 of the training set: {'input_ids': [2, 3822, 2770, 2434, 1690, 4532, 9827, 1682, 3155, 17016, 17928, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\nINFO:__main__:Sample 35950 of the training set: {'input_ids': [2, 3801, 1685, 2042, 2884, 9469, 1690, 25584, 2437, 1682, 10722, 1725, 11871, 1685, 4097, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 0, -100, -100, 3, 0, -100, 3, 1, 3, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\nINFO:__main__:Sample 35397 of the training set: {'input_ids': [2, 3314, 1734, 2150, 3680, 8, 1682, 1948, 13302, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 1, 3, 3, 3, 3, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\nINFO:__main__:===== Starting training (1 epochs) =====\nStep... (500/5116 | Training Loss: 0.054065871983766556, Learning Rate: 9.024628525367007e-06)\nTraining...:  10%|██▋                        | 499/5116 [01:50<05:03, 15.19it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (1000/5116 | Training Loss: 0.02042941376566887, Learning Rate: 8.04730188974645e-06)\nEvaluating on Dev Set...: 100%|███████████████| 163/163 [00:27<00:00,  5.87it/s]\nINFO:__main__:Step... (1000/5116 | Validation metrics: {'macro_precision': 0.7872471285456162, 'macro_recall': 0.5963090551135369, 'macro_f1': 0.6775279587736475}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (1500/5116 | Training Loss: 0.024192551150918007, Learning Rate: 7.069976163620595e-06)\nTraining...:  29%|███████▌                  | 1498/5116 [03:34<03:51, 15.64it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (2000/5116 | Training Loss: 0.014572443440556526, Learning Rate: 6.092649982747389e-06)\nEvaluating on Dev Set...: 100%|███████████████| 163/163 [00:21<00:00,  7.76it/s]\nINFO:__main__:Step... (2000/5116 | Validation metrics: {'macro_precision': 0.7506531991455972, 'macro_recall': 0.6701909413776117, 'macro_f1': 0.7074970118660445}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (2500/5116 | Training Loss: 0.01757798157632351, Learning Rate: 5.115324256621534e-06)\nTraining...:  49%|████████████▋             | 2499/5116 [05:11<02:46, 15.67it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (3000/5116 | Training Loss: 0.024873683229088783, Learning Rate: 4.137998075748328e-06)\nEvaluating on Dev Set...: 100%|███████████████| 163/163 [00:21<00:00,  7.70it/s]\nINFO:__main__:Step... (3000/5116 | Validation metrics: {'macro_precision': 0.7411625316304167, 'macro_recall': 0.6890369945481852, 'macro_f1': 0.7141263157564207}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (3500/5116 | Training Loss: 0.04038568586111069, Learning Rate: 3.1606721222487977e-06)\nTraining...:  68%|█████████████████▊        | 3498/5116 [06:49<01:42, 15.78it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (4000/5116 | Training Loss: 0.012971609830856323, Learning Rate: 2.1833461687492672e-06)\nEvaluating on Dev Set...: 100%|███████████████| 163/163 [00:21<00:00,  7.75it/s]\nINFO:__main__:Step... (4000/5116 | Validation metrics: {'macro_precision': 0.7363953192250978, 'macro_recall': 0.7019087402749294, 'macro_f1': 0.7179690917622169}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (4500/5116 | Training Loss: 0.01645631529390812, Learning Rate: 1.206020101562899e-06)\nTraining...:  88%|██████████████████████▊   | 4498/5116 [08:26<00:43, 14.37it/s]Configuration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nStep... (5000/5116 | Training Loss: 0.032999277114868164, Learning Rate: 2.2869407700909505e-07)\nEvaluating on Dev Set...: 100%|███████████████| 163/163 [00:21<00:00,  7.75it/s]\nINFO:__main__:Step... (5000/5116 | Validation metrics: {'macro_precision': 0.7422835868780334, 'macro_recall': 0.696906108130284, 'macro_f1': 0.718562230718638}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nEvaluating on Dev Set...: 100%|███████████████| 163/163 [00:21<00:00,  7.74it/s]\nINFO:__main__:Step... (5116/5116 | Validation metrics: {'macro_precision': 0.7427751552440504, 'macro_recall': 0.6960990199730723, 'macro_f1': 0.7183757808324703}\nConfiguration saved in /kaggle/working/out/config.json\nModel weights saved in /kaggle/working/out/flax_model.msgpack\ntokenizer config file saved in out/tokenizer_config.json\nSpecial tokens file saved in out/special_tokens_map.json\nTraining...: 100%|██████████████████████████| 5116/5116 [10:07<00:00,  8.42it/s]\nEpoch ... 1/1: 100%|█████████████████████████████| 1/1 [10:07<00:00, 607.57s/it]\nEvaluating on Dev Set ...: 163it [00:21,  7.73it/s]                             \nINFO:__main__:Evaluation Results on Dev Set : {'macro_precision': 0.7427751552440504, 'macro_recall': 0.6960990199730723, 'macro_f1': 0.7183757808324703}\nEvaluating on Test Set...: 33it [00:04,  7.88it/s]                              \nINFO:__main__:Evaluation Results on Test Set : {'macro_precision': 0.7390326984123895, 'macro_recall': 0.7661115267083591, 'macro_f1': 0.7422359927971797}\n","output_type":"stream"}]}]}