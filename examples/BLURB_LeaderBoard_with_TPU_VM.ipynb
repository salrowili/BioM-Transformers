{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1BYtC95RNi7W",
        "L1kpBFervxMz",
        "xnquHEZAyriJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Google Colab aims to replicate our results with the BLURB leaderboard. This colab uses JAX hugging face code. We connected this Google colab to the local machine (our TPUv3-8 device). To connect your TPU VM machine with google colab follow this page: https://research.google.com/colaboratory/local-runtimes.html.\n",
        "\n",
        "Part of our code is modified from BioLinkBERT Github repo : https://github.com/michiyasunaga/LinkBERT.\n",
        "\n",
        "Please also note that you can run this code also on GPU without any further modification. However, running the code with the given parameters here in this colab may not reproduce the same results on the GPU since here we use TPU. Thus you need to change the hyperparameters choices. See this post for more detail: https://github.com/google/jax/issues/4823\n",
        "\n",
        "Also note that you can run codes inside each cell directly on the TPU VM machine with Jax installed to reproduce our results. If you do not want to log results no need to connect to the local machine.\n",
        "\n",
        "1. first step lets download the pre-processed BLURB dataset from the BioLinkBERT team GitHub :"
      ],
      "metadata": {
        "id": "YV4kW634EKld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "95gRbBoOvze6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. run both [run_flax_seq.py, run_flax_ner.py] cells to write the code for each file in our machine. This code uses wandb to log each run. Our project name is \"blurb-flax\". you can change it from lines 43-44:\n",
        "\n",
        "```\n",
        "import wandb\n",
        "wandb.init(project=\"blurb-flax\")\n",
        "```\n",
        "\n",
        "You can disabled wandb by running :\n",
        "\n",
        "`!wandb disabled`\n",
        "\n"
      ],
      "metadata": {
        "id": "tqFNy6PsFDyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run_flax_seq.py\n",
        "%%writefile run_flax_seq.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Finetuning a 🤗 Flax Transformers model for sequence classification on GLUE.\"\"\"\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Dict, Optional, Tuple\n",
        "\n",
        "import datasets\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "from datasets import load_dataset\n",
        "from flax import struct, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, replicate, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard\n",
        "from huggingface_hub import Repository, create_repo\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "wandb.init(project=\"blurb-flax\")\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForSequenceClassification,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    TrainingArguments,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import check_min_version, get_full_repo_name, send_example_telemetry\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
        "check_min_version(\"4.27.0.dev0\")\n",
        "\n",
        "Array = Any\n",
        "Dataset = datasets.arrow_dataset.Dataset\n",
        "PRNGKey = Any\n",
        "\n",
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    use_slow_tokenizer: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\"},\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    task_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": f\"The name of the glue task to train on. choices {list(task_to_keys.keys())}\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    train_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n",
        "    )\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n",
        "    )\n",
        "    text_column_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
        "    )\n",
        "    label_column_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    blurb_task: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": f\"The name of the glue task to train on. choices {list(task_to_keys.keys())}\"}\n",
        "    )\n",
        "    metric_name: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the metric\"},\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.task_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        self.task_name = self.task_name.lower() if type(self.task_name) == str else self.task_name\n",
        "\n",
        "\n",
        "def create_train_state(\n",
        "    model: FlaxAutoModelForSequenceClassification,\n",
        "    learning_rate_fn: Callable[[int], float],\n",
        "    is_regression: bool,\n",
        "    num_labels: int,\n",
        "    weight_decay: float,\n",
        ") -> train_state.TrainState:\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "\n",
        "    class TrainState(train_state.TrainState):\n",
        "        \"\"\"Train state with an Optax optimizer.\n",
        "\n",
        "        The two functions below differ depending on whether the task is classification\n",
        "        or regression.\n",
        "\n",
        "        Args:\n",
        "          logits_fn: Applied to last layer to obtain the logits.\n",
        "          loss_fn: Function to compute the loss.\n",
        "        \"\"\"\n",
        "\n",
        "        logits_fn: Callable = struct.field(pytree_node=False)\n",
        "        loss_fn: Callable = struct.field(pytree_node=False)\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    tx = optax.adamw(\n",
        "        learning_rate=learning_rate_fn, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn\n",
        "    )\n",
        "\n",
        "    if is_regression:\n",
        "\n",
        "        def mse_loss(logits, labels):\n",
        "            return jnp.mean((logits[..., 0] - labels) ** 2)\n",
        "\n",
        "        return TrainState.create(\n",
        "            apply_fn=model.__call__,\n",
        "            params=model.params,\n",
        "            tx=tx,\n",
        "            logits_fn=lambda logits: logits[..., 0],\n",
        "            loss_fn=mse_loss,\n",
        "        )\n",
        "    else:  # Classification.\n",
        "\n",
        "        def cross_entropy_loss(logits, labels):\n",
        "            xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n",
        "            return jnp.mean(xentropy)\n",
        "\n",
        "        return TrainState.create(\n",
        "            apply_fn=model.__call__,\n",
        "            params=model.params,\n",
        "            tx=tx,\n",
        "            logits_fn=lambda logits: logits.argmax(-1),\n",
        "            loss_fn=cross_entropy_loss,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def glue_train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n",
        "    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "    perms = jax.random.permutation(rng, len(dataset))\n",
        "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for perm in perms:\n",
        "        batch = dataset[perm]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def glue_eval_data_collator(dataset: Dataset, batch_size: int):\n",
        "    \"\"\"Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.\"\"\"\n",
        "    batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "    batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    wandb.log({\"blurb_task\":data_args.blurb_task,\"model_name\":model_args.model_name_or_path,\n",
        "    \"per_device_train_batch_size\":training_args.per_device_train_batch_size,\n",
        "    \"learning_rate\": training_args.learning_rate, \"weight_decay\": training_args.weight_decay,\n",
        "    \"adam_beta1\":training_args.adam_beta1, \"adam_beta2\" : training_args.adam_beta2,\n",
        "    \"adam_epsilon\":training_args.adam_epsilon,\"label_smoothing_factor\": training_args.label_smoothing_factor,\n",
        "     \"num_train_epochs\":training_args.num_train_epochs, \"warmup_steps\":training_args.warmup_steps,\"seed\": training_args.seed})\n",
        "\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_glue\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        create_repo(repo_name, exist_ok=True, token=training_args.hub_token)\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name, token=training_args.hub_token)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n",
        "\n",
        "    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n",
        "    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n",
        "    # label if at least two columns are provided.\n",
        "\n",
        "    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n",
        "    # single column. You can easily tweak this behavior (see below)\n",
        "\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    if data_args.task_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        raw_datasets = load_dataset(\n",
        "            \"glue\",\n",
        "            data_args.task_name,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        # Loading the dataset from local csv or json file.\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "        extension = (data_args.train_file if data_args.train_file is not None else data_args.valid_file).split(\".\")[-1]\n",
        "        raw_datasets = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Labels\n",
        "    if data_args.task_name is not None:\n",
        "        is_regression = data_args.task_name == \"stsb\"\n",
        "        if not is_regression:\n",
        "            label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
        "            num_labels = len(label_list)\n",
        "        else:\n",
        "            num_labels = 1\n",
        "    else:\n",
        "        # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
        "        is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
        "        if is_regression:\n",
        "            num_labels = 1\n",
        "        else:\n",
        "            # A useful fast method:\n",
        "            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
        "            label_list = raw_datasets[\"train\"].unique(\"label\")\n",
        "            label_list.sort()  # Let's sort it for determinism\n",
        "            num_labels = len(label_list)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=data_args.task_name,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        use_fast=not model_args.use_slow_tokenizer,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    model = FlaxAutoModelForSequenceClassification.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        config=config,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "        from_pt=True\n",
        "    )\n",
        "\n",
        "    # Preprocessing the datasets\n",
        "    if data_args.task_name is not None:\n",
        "        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
        "    else:\n",
        "        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
        "        non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
        "        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
        "            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
        "        elif \"sentence\" in non_label_column_names:\n",
        "            sentence1_key, sentence2_key = \"sentence\", None\n",
        "        else:\n",
        "            if len(non_label_column_names) >= 2:\n",
        "                sentence1_key, sentence2_key = non_label_column_names[:2]\n",
        "            else:\n",
        "                sentence1_key, sentence2_key = non_label_column_names[0], None\n",
        "\n",
        "    # Some models have set the order of the labels to use, so let's make sure we do use it.\n",
        "    label_to_id = None\n",
        "    id_to_label=None\n",
        "    if (\n",
        "        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
        "        and data_args.task_name is not None\n",
        "        and not is_regression\n",
        "    ):\n",
        "        # Some have all caps in their config, some don't.\n",
        "        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
        "        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
        "            logger.info(\n",
        "                f\"The configuration of the model provided the following label correspondence: {label_name_to_id}. \"\n",
        "                \"Using it!\"\n",
        "            )\n",
        "            label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n",
        "        else:\n",
        "            logger.warning(\n",
        "                \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
        "                f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
        "                \"\\nIgnoring the model labels as a result.\",\n",
        "            )\n",
        "    elif data_args.task_name is None and is_regression==False:\n",
        "        label_to_id = {v: i for i, v in enumerate(label_list)}\n",
        "\n",
        "    if not is_regression:\n",
        "     id_to_label={v: i for  v,i in enumerate(label_to_id)}\n",
        "    if \"chemprot\" in str(data_args.train_file).lower():\n",
        "     id_to_label[0]=\"false\"\n",
        "    if \"ddi\" in str(data_args.train_file).lower():\n",
        "     id_to_label[0]=\"DDI-false\"\n",
        "    def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        texts = (\n",
        "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "        )\n",
        "        result = tokenizer(*texts, padding=\"max_length\", max_length=data_args.max_seq_length, truncation=True)\n",
        "\n",
        "        if \"label\" in examples:\n",
        "            if label_to_id is not None:\n",
        "                # Map labels to IDs (not necessary for GLUE tasks)\n",
        "                result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
        "            else:\n",
        "                # In all cases, rename the column to labels because the model will expect that.\n",
        "                result[\"labels\"] = examples[\"label\"]\n",
        "        return result\n",
        "\n",
        "    processed_datasets = raw_datasets.map(\n",
        "        preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    train_dataset = processed_datasets[\"train\"]\n",
        "    eval_dataset = processed_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
        "    test_dataset = processed_datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    # Define a summary writer\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(training_args.output_dir)\n",
        "            summary_writer.hparams({**training_args.to_dict(), **vars(model_args), **vars(data_args)})\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "    def compute_metrics(preds, labels):\n",
        "          # Get the metric function\n",
        "        if data_args.task_name is not None:\n",
        "          metric = evaluate.load(\"glue\", data_args.task_name)\n",
        "        else:\n",
        "          metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "        if data_args.metric_name == \"hoc\":\n",
        "            from utils_hoc import eval_hoc\n",
        "            labels = np.array(p.label_ids).astype(int) #[num_ex, num_class]\n",
        "            preds = (np.array(preds) > 0).astype(int)  #[num_ex, num_class]\n",
        "            ids = eval_dataset[\"id\"]\n",
        "            return eval_hoc(labels.tolist(), preds.tolist(), list(ids))\n",
        "        if data_args.task_name is not None:\n",
        "            result = metric.compute(predictions=preds, references=labels)\n",
        "            if len(result) > 1:\n",
        "                result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
        "            return result\n",
        "        elif data_args.metric_name == \"pearsonr\":\n",
        "            from scipy.stats import pearsonr as scipy_pearsonr\n",
        "            pearsonr = float(scipy_pearsonr(labels, preds)[0])\n",
        "            return {\"pearsonr\": pearsonr}\n",
        "        elif data_args.metric_name == \"PRF1\":\n",
        "            TP = ((preds == labels) & (preds != 0)).astype(int).sum().item()\n",
        "            P_total = (preds != 0).astype(int).sum().item()\n",
        "            L_total = (labels != 0).astype(int).sum().item()\n",
        "            P = TP / P_total if P_total else 0\n",
        "            R = TP / L_total if L_total else 0\n",
        "            F1 = 2 * P * R / (P + R) if (P + R) else 0\n",
        "            #confirm=precision_recall_fscore_support(labels, preds, average='micro',labels=[1])\n",
        "            return {\"precision\": P, \"recall\": R, \"F1\": F1}\n",
        "        elif is_regression:\n",
        "            return {\"mse\": ((preds - labels) ** 2).mean().item()}\n",
        "        else:\n",
        "            return {\"accuracy\": (preds == labels).astype(np.float32).mean().item()}\n",
        "\n",
        "    def write_train_metric(summary_writer, train_metrics, train_time, step):\n",
        "        summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "        train_metrics = get_metrics(train_metrics)\n",
        "        for key, vals in train_metrics.items():\n",
        "            tag = f\"train_{key}\"\n",
        "            for i, val in enumerate(vals):\n",
        "                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    def write_eval_metric(summary_writer, eval_metrics, step):\n",
        "        for metric_name, value in eval_metrics.items():\n",
        "            summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "    train_batch_size = int(training_args.per_device_train_batch_size) * jax.local_device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n",
        "\n",
        "    learning_rate_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    state = create_train_state(\n",
        "        model, learning_rate_fn, is_regression, num_labels=num_labels, weight_decay=training_args.weight_decay\n",
        "    )\n",
        "\n",
        "    # define step functions\n",
        "    def train_step(\n",
        "        state: train_state.TrainState, batch: Dict[str, Array], dropout_rng: PRNGKey\n",
        "    ) -> Tuple[train_state.TrainState, float]:\n",
        "        \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "        targets = batch.pop(\"labels\")\n",
        "\n",
        "        def loss_fn(params):\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss = state.loss_fn(logits, targets)\n",
        "            return loss\n",
        "\n",
        "        grad_fn = jax.value_and_grad(loss_fn)\n",
        "        loss, grad = grad_fn(state.params)\n",
        "        grad = jax.lax.pmean(grad, \"batch\")\n",
        "        new_state = state.apply_gradients(grads=grad)\n",
        "        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}, axis_name=\"batch\")\n",
        "        return new_state, metrics, new_dropout_rng\n",
        "\n",
        "    p_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
        "\n",
        "    def eval_step(state, batch):\n",
        "        logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
        "        return state.logits_fn(logits)\n",
        "\n",
        "    p_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
        "\n",
        "    if data_args.task_name is not None:\n",
        "        metric = evaluate.load(\"glue\", data_args.task_name)\n",
        "    else:\n",
        "        metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    logger.info(f\"===== Starting training ({num_epochs} epochs) =====\")\n",
        "    train_time = 0\n",
        "\n",
        "    # make sure weights are replicated on each device\n",
        "    state = replicate(state)\n",
        "\n",
        "    steps_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_steps = steps_per_epoch * num_epochs\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (0/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "        train_start = time.time()\n",
        "        train_metrics = []\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "        # train\n",
        "        train_loader = glue_train_data_collator(input_rng, train_dataset, train_batch_size)\n",
        "        for step, batch in enumerate(\n",
        "            tqdm(\n",
        "                train_loader,\n",
        "                total=steps_per_epoch,\n",
        "                desc=\"Training...\",\n",
        "                position=0, leave=True,\n",
        "            ),\n",
        "        ):\n",
        "            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "            cur_step = (epoch * steps_per_epoch) + (step + 1)\n",
        "\n",
        "            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n",
        "                # Save metrics\n",
        "                train_metric = unreplicate(train_metric)\n",
        "                train_time += time.time() - train_start\n",
        "                if has_tensorboard and jax.process_index() == 0:\n",
        "                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n",
        "\n",
        "                epochs.write(\n",
        "                    f\"Step... ({cur_step}/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "                    f\" {train_metric['learning_rate']})\"\n",
        "                )\n",
        "\n",
        "                train_metrics = []\n",
        "\n",
        "            if (cur_step % training_args.eval_steps == 0 or cur_step % steps_per_epoch == 0) and cur_step > 0:\n",
        "                # evaluate\n",
        "                eval_loader = glue_eval_data_collator(eval_dataset, eval_batch_size)\n",
        "                preds_list=[]\n",
        "                labels_list=[]\n",
        "                for batch in tqdm(\n",
        "                    eval_loader,\n",
        "                    total=math.ceil(len(eval_dataset) / eval_batch_size),\n",
        "                    desc=\"Evaluating ...\",\n",
        "                    position=0, leave=True,\n",
        "                ):\n",
        "                    labels = batch.pop(\"labels\")\n",
        "                    predictions = pad_shard_unpad(p_eval_step)(\n",
        "                        state, batch, min_device_batch=per_device_eval_batch_size\n",
        "                    )\n",
        "                    for pred_example in predictions:\n",
        "                      preds_list.append(pred_example)\n",
        "                    for label_example in labels:\n",
        "                      labels_list.append(label_example)\n",
        "                    metric.add_batch(predictions=np.array(predictions), references=labels)\n",
        "\n",
        "                preds_list=np.array(preds_list)\n",
        "                labels_list=np.array(labels_list)\n",
        "                blurb_result=compute_metrics(preds_list,labels_list)\n",
        "                blurb_result={\"eval_\" + str(key): val for key, val in blurb_result.items()}\n",
        "                wandb.log(blurb_result)\n",
        "                eval_metric = metric.compute()\n",
        "                logger.info(f\"{blurb_result}| Step... ({cur_step}/{total_steps} | Eval metrics: {eval_metric}) \")\n",
        "\n",
        "                if has_tensorboard and jax.process_index() == 0:\n",
        "                    write_eval_metric(summary_writer, eval_metric, cur_step)\n",
        "            if 7==0:\n",
        "            #if (cur_step % training_args.save_steps == 0 and cur_step > 0) or (cur_step == total_steps):\n",
        "                # save checkpoint after each epoch and push checkpoint to the hub\n",
        "                if jax.process_index() == 0:\n",
        "                    params = jax.device_get(unreplicate(state.params))\n",
        "                    model.save_pretrained(training_args.output_dir, params=params)\n",
        "                    tokenizer.save_pretrained(training_args.output_dir)\n",
        "                    if training_args.push_to_hub:\n",
        "                        repo.push_to_hub(commit_message=f\"Saving weights and logs of step {cur_step}\", blocking=False)\n",
        "            epochs.desc = f\"Epoch ... {epoch + 1}/{num_epochs}\"\n",
        "\n",
        "    # save the eval metrics in json\n",
        "    if data_args.test_file:\n",
        "                ########### the only change here from dev code is that we pass \"test_dataset\" to glue_eval_data_collator\n",
        "                eval_loader = glue_eval_data_collator(test_dataset, eval_batch_size)\n",
        "                preds_list=[]\n",
        "                preds_dict={}\n",
        "                labels_list=[]\n",
        "                for batch in tqdm(\n",
        "                    eval_loader,\n",
        "                    total=math.ceil(len(test_dataset) / eval_batch_size),\n",
        "                    desc=\"Evaluating on Test Data ...\",\n",
        "                     position=0, leave=True\n",
        "                ):\n",
        "                    labels = batch.pop(\"labels\")\n",
        "                    predictions = pad_shard_unpad(p_eval_step)(\n",
        "                        state, batch, min_device_batch=per_device_eval_batch_size\n",
        "                    )\n",
        "                    ##### prepare predictions and labels to metric compute function ####\n",
        "                    for pred_example in predictions:\n",
        "                      preds_list.append(pred_example)\n",
        "                    for label_example in labels:\n",
        "                      labels_list.append(label_example)\n",
        "                    metric.add_batch(predictions=np.array(predictions), references=labels)\n",
        "                    ####################################################################\n",
        "                ############ write prediction in BLURB Leaderboard Format ##########\n",
        "                raw_ids=[]\n",
        "                for row in raw_datasets[\"test\"]:\n",
        "                 raw_ids.append(row[\"id\"])\n",
        "                for label_id,pred_value in zip(raw_ids,preds_list):\n",
        "                      if is_regression:\n",
        "                        preds_dict[label_id]=str(pred_value)\n",
        "                      else:\n",
        "                        preds_dict[label_id]=str(id_to_label[pred_value])\n",
        "                if \"/\" in model_args.model_name_or_path:\n",
        "                   model_name_hf=model_args.model_name_or_path.split(\"/\")[-1]\n",
        "                else:\n",
        "                   model_name_hf=model_args.model_name_or_path\n",
        "                final_entry={\"dataset_name\": data_args.blurb_task, \"model_name\":model_name_hf, \"predictions\":preds_dict}\n",
        "                with open(model_args.model_name_or_path.split(\"/\")[-1]+\"_\"+str(data_args.blurb_task)+\".json\", \"w\") as f:\n",
        "                  json.dump(final_entry, f, indent=4, sort_keys=True)\n",
        "                #####################################################################\n",
        "                preds_list=np.array(preds_list)\n",
        "                labels_list=np.array(labels_list)\n",
        "                blurb_result=compute_metrics(preds_list,labels_list)\n",
        "                blurb_result={\"test_\" + str(key): val for key, val in blurb_result.items()}\n",
        "                wandb.log(blurb_result)\n",
        "                eval_metric = metric.compute()\n",
        "                logger.info(f\" test results : {blurb_result}\")\n",
        "    if jax.process_index() == 0:\n",
        "        eval_metric = {f\"eval_{metric_name}\": value for metric_name, value in eval_metric.items()}\n",
        "        path = os.path.join(training_args.output_dir, \"eval_results.json\")\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(eval_metric, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9kAPC04HUcQu",
        "outputId": "21e690f5-4b73-43d4-e8f5-daaa4cbf7d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_flax_seq.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run_flax_ner.py\n",
        "%%writefile run_flax_ner.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Fine-tuning a 🤗 Flax Transformers model on token classification tasks (NER, POS, CHUNKS)\"\"\"\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from enum import Enum\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Dict, Optional, Tuple\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import ClassLabel, load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import evaluate\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import transformers\n",
        "from flax import struct, traverse_util\n",
        "from flax.jax_utils import pad_shard_unpad, replicate, unreplicate\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot, shard\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    FlaxAutoModelForTokenClassification,\n",
        "    HfArgumentParser,\n",
        "    is_tensorboard_available,\n",
        ")\n",
        "from transformers.utils import check_min_version, get_full_repo_name, send_example_telemetry\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
        "check_min_version(\"4.26.0.dev0\")\n",
        "\n",
        "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/token-classification/requirements.txt\")\n",
        "\n",
        "Array = Any\n",
        "Dataset = datasets.arrow_dataset.Dataset\n",
        "PRNGKey = Any\n",
        "import wandb\n",
        "wandb.init(project=\"blurb-flax\")\n",
        "wandb.init()\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    output_dir: str = field(\n",
        "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
        "    )\n",
        "    overwrite_output_dir: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Overwrite the content of the output directory. \"\n",
        "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
        "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
        "    do_test: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the test set.\"})\n",
        "    per_device_train_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
        "    )\n",
        "    per_device_eval_batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
        "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
        "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
        "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
        "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
        "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
        "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
        "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
        "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
        "    push_to_hub: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
        "    )\n",
        "    hub_model_id: str = field(\n",
        "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
        "    )\n",
        "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.output_dir is not None:\n",
        "            self.output_dir = os.path.expanduser(self.output_dir)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
        "        the token values by removing their value.\n",
        "        \"\"\"\n",
        "        d = asdict(self)\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, Enum):\n",
        "                d[k] = v.value\n",
        "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
        "                d[k] = [x.value for x in v]\n",
        "            if k.endswith(\"_token\"):\n",
        "                d[k] = f\"<{k.upper()}>\"\n",
        "        return d\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    train_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n",
        "    )\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n",
        "    )\n",
        "    text_column_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
        "    )\n",
        "    label_column_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    label_all_tokens: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n",
        "                \"one (in which case the other tokens will have a padding index).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    return_entity_level_metrics: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n",
        "    )\n",
        "    return_macro_metrics: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to return macro metrics (i.e. final mean is taken across tag categories) during evaluation or just the overall ones (i.e. micro).\"},\n",
        "    )\n",
        "    blurb_task: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        self.task_name = self.task_name.lower()\n",
        "\n",
        "\n",
        "def create_train_state(\n",
        "    model: FlaxAutoModelForTokenClassification,\n",
        "    learning_rate_fn: Callable[[int], float],\n",
        "    num_labels: int,\n",
        "    training_args: TrainingArguments,\n",
        ") -> train_state.TrainState:\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "\n",
        "    class TrainState(train_state.TrainState):\n",
        "        \"\"\"Train state with an Optax optimizer.\n",
        "\n",
        "        The two functions below differ depending on whether the task is classification\n",
        "        or regression.\n",
        "\n",
        "        Args:\n",
        "          logits_fn: Applied to last layer to obtain the logits.\n",
        "          loss_fn: Function to compute the loss.\n",
        "        \"\"\"\n",
        "\n",
        "        logits_fn: Callable = struct.field(pytree_node=False)\n",
        "        loss_fn: Callable = struct.field(pytree_node=False)\n",
        "\n",
        "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
        "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
        "    # mask boolean with the same structure as the parameters.\n",
        "    # The mask is True for parameters that should be decayed.\n",
        "    def decay_mask_fn(params):\n",
        "        flat_params = traverse_util.flatten_dict(params)\n",
        "        # find out all LayerNorm parameters\n",
        "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
        "        layer_norm_named_params = set(\n",
        "            [\n",
        "                layer[-2:]\n",
        "                for layer_norm_name in layer_norm_candidates\n",
        "                for layer in flat_params.keys()\n",
        "                if layer_norm_name in \"\".join(layer).lower()\n",
        "            ]\n",
        "        )\n",
        "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
        "        return traverse_util.unflatten_dict(flat_mask)\n",
        "\n",
        "    tx = optax.adamw(\n",
        "        learning_rate=learning_rate_fn,\n",
        "        b1=training_args.adam_beta1,\n",
        "        b2=training_args.adam_beta2,\n",
        "        eps=training_args.adam_epsilon,\n",
        "        weight_decay=training_args.weight_decay,\n",
        "        mask=decay_mask_fn,\n",
        "    )\n",
        "\n",
        "    def cross_entropy_loss(logits, labels):\n",
        "        xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n",
        "        return jnp.mean(xentropy)\n",
        "\n",
        "    return TrainState.create(\n",
        "        apply_fn=model.__call__,\n",
        "        params=model.params,\n",
        "        tx=tx,\n",
        "        logits_fn=lambda logits: logits.argmax(-1),\n",
        "        loss_fn=cross_entropy_loss,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_learning_rate_fn(\n",
        "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
        ") -> Callable[[int], jnp.array]:\n",
        "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
        "    steps_per_epoch = train_ds_size // train_batch_size\n",
        "    num_train_steps = steps_per_epoch * num_train_epochs\n",
        "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
        "    decay_fn = optax.linear_schedule(\n",
        "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
        "    )\n",
        "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
        "    return schedule_fn\n",
        "\n",
        "\n",
        "def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n",
        "    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "    perms = jax.random.permutation(rng, len(dataset))\n",
        "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for perm in perms:\n",
        "        batch = dataset[perm]\n",
        "        batch = {k: np.array(v,) for k, v in batch.items()}\n",
        "        batch = shard(batch)\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def eval_data_collator(dataset: Dataset, batch_size: int):\n",
        "    \"\"\"Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.\"\"\"\n",
        "    batch_idx = np.arange(len(dataset))\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
        "    batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: np.array(v) for k, v in batch.items()}\n",
        "        yield batch\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "    wandb.log({\"blurb_task\":data_args.blurb_task,\"model_name\":model_args.model_name_or_path,\"per_device_train_batch_size\":training_args.per_device_train_batch_size, \"learning_rate\": training_args.learning_rate, \"num_train_epochs\":training_args.num_train_epochs, \"warmup_steps\":training_args.warmup_steps,\"seed\": training_args.seed})\n",
        "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "    send_example_telemetry(\"run_ner\", model_args, data_args, framework=\"flax\")\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # Setup logging, we only want one process per machine to log things on the screen.\n",
        "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
        "    if jax.process_index() == 0:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if training_args.push_to_hub:\n",
        "        if training_args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(\n",
        "                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n",
        "            )\n",
        "        else:\n",
        "            repo_name = training_args.hub_model_id\n",
        "        repo = Repository(training_args.output_dir, clone_from=repo_name)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n",
        "    # 'tokens' is found. You can easily tweak this behavior (see below).\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        raw_datasets = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        # Loading the dataset from local csv or json file.\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "        extension = (data_args.train_file if data_args.train_file is not None else data_args.valid_file).split(\".\")[-1]\n",
        "        raw_datasets = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    # See more about loading any type of standard or custom dataset at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    if raw_datasets[\"train\"] is not None:\n",
        "        column_names = raw_datasets[\"train\"].column_names\n",
        "        features = raw_datasets[\"train\"].features\n",
        "    else:\n",
        "        column_names = raw_datasets[\"validation\"].column_names\n",
        "        features = raw_datasets[\"validation\"].features\n",
        "\n",
        "    if data_args.text_column_name is not None:\n",
        "        text_column_name = data_args.text_column_name\n",
        "    elif \"tokens\" in column_names:\n",
        "        text_column_name = \"tokens\"\n",
        "    else:\n",
        "        text_column_name = column_names[0]\n",
        "\n",
        "    if data_args.label_column_name is not None:\n",
        "        label_column_name = data_args.label_column_name\n",
        "    elif f\"{data_args.task_name}_tags\" in column_names:\n",
        "        label_column_name = f\"{data_args.task_name}_tags\"\n",
        "    else:\n",
        "        label_column_name = column_names[1]\n",
        "\n",
        "    # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
        "    # unique labels.\n",
        "    def get_label_list(labels):\n",
        "        unique_labels = set()\n",
        "        for label in labels:\n",
        "            unique_labels = unique_labels | set(label)\n",
        "        label_list = list(unique_labels)\n",
        "        label_list.sort()\n",
        "        return label_list\n",
        "\n",
        "    if isinstance(features[label_column_name].feature, ClassLabel):\n",
        "        label_list = features[label_column_name].feature.names\n",
        "        # No need to convert the labels since they are already ints.\n",
        "        label_to_id = {i: i for i in range(len(label_list))}\n",
        "    else:\n",
        "        label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
        "        label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        label2id=label_to_id,\n",
        "        id2label={i: l for l, i in label_to_id.items()},\n",
        "        finetuning_task=data_args.task_name,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
        "    if config.model_type in {\"gpt2\", \"roberta\"}:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            revision=model_args.model_revision,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "            add_prefix_space=True,\n",
        "        )\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_name_or_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            revision=model_args.model_revision,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    model = FlaxAutoModelForTokenClassification.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        config=config,\n",
        "        from_pt=True,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    # Preprocessing the datasets\n",
        "    # Tokenize all texts and align the labels with them.\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[text_column_name],\n",
        "            max_length=data_args.max_seq_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
        "            is_split_into_words=True,\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "\n",
        "        for i, label in enumerate(examples[label_column_name]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "                # ignored in the loss function.\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                # We set the label for the first token of each word.\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label_to_id[label[word_idx]])\n",
        "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "                # the label_all_tokens flag.\n",
        "                else:\n",
        "                    label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)\n",
        "                previous_word_idx = word_idx\n",
        "\n",
        "            labels.append(label_ids)\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    processed_raw_datasets = raw_datasets.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        num_proc=data_args.preprocessing_num_workers,\n",
        "        load_from_cache_file=not data_args.overwrite_cache,\n",
        "        remove_columns=raw_datasets[\"train\"].column_names,\n",
        "        desc=\"Running tokenizer on dataset\",\n",
        "    )\n",
        "\n",
        "    train_dataset = processed_raw_datasets[\"train\"]\n",
        "    eval_dataset = processed_raw_datasets[\"validation\"]\n",
        "    test_dataset = processed_raw_datasets[\"test\"]\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    # Define a summary writer\n",
        "    has_tensorboard = is_tensorboard_available()\n",
        "    if has_tensorboard and jax.process_index() == 0:\n",
        "        try:\n",
        "            from flax.metrics.tensorboard import SummaryWriter\n",
        "\n",
        "            summary_writer = SummaryWriter(training_args.output_dir)\n",
        "            summary_writer.hparams({**training_args.to_dict(), **vars(model_args), **vars(data_args)})\n",
        "        except ImportError as ie:\n",
        "            has_tensorboard = False\n",
        "            logger.warning(\n",
        "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
        "            )\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
        "            \"Please run pip install tensorboard to enable.\"\n",
        "        )\n",
        "\n",
        "    def write_train_metric(summary_writer, train_metrics, train_time, step):\n",
        "        summary_writer.scalar(\"train_time\", train_time, step)\n",
        "\n",
        "        train_metrics = get_metrics(train_metrics)\n",
        "        for key, vals in train_metrics.items():\n",
        "            tag = f\"train_{key}\"\n",
        "            for i, val in enumerate(vals):\n",
        "                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
        "\n",
        "    def write_eval_metric(summary_writer, eval_metrics, step):\n",
        "        for metric_name, value in eval_metrics.items():\n",
        "            summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
        "\n",
        "    num_epochs = int(training_args.num_train_epochs)\n",
        "    rng = jax.random.PRNGKey(training_args.seed)\n",
        "    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "    train_batch_size = training_args.per_device_train_batch_size * jax.local_device_count()\n",
        "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
        "    eval_batch_size = training_args.per_device_eval_batch_size * jax.local_device_count()\n",
        "\n",
        "    learning_rate_fn = create_learning_rate_fn(\n",
        "        len(train_dataset),\n",
        "        train_batch_size,\n",
        "        training_args.num_train_epochs,\n",
        "        training_args.warmup_steps,\n",
        "        training_args.learning_rate,\n",
        "    )\n",
        "\n",
        "    state = create_train_state(model, learning_rate_fn, num_labels=num_labels, training_args=training_args)\n",
        "\n",
        "    # define step functions\n",
        "    def train_step(\n",
        "        state: train_state.TrainState, batch: Dict[str, Array], dropout_rng: PRNGKey\n",
        "    ) -> Tuple[train_state.TrainState, float]:\n",
        "        \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n",
        "        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "        targets = batch.pop(\"labels\")\n",
        "\n",
        "        def loss_fn(params):\n",
        "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "            loss = state.loss_fn(logits, targets)\n",
        "            return loss\n",
        "\n",
        "        grad_fn = jax.value_and_grad(loss_fn)\n",
        "        loss, grad = grad_fn(state.params)\n",
        "        grad = jax.lax.pmean(grad, \"batch\")\n",
        "        new_state = state.apply_gradients(grads=grad)\n",
        "        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}, axis_name=\"batch\")\n",
        "        return new_state, metrics, new_dropout_rng\n",
        "\n",
        "    p_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
        "\n",
        "    def eval_step(state, batch):\n",
        "        logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
        "        return state.logits_fn(logits)\n",
        "\n",
        "    p_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
        "\n",
        "    metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "    def get_labels(y_pred, y_true):\n",
        "        # Transform predictions and references tensos to numpy arrays\n",
        "\n",
        "        # Remove ignored index (special tokens)\n",
        "        true_predictions = [\n",
        "            [label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]\n",
        "            for pred, gold_label in zip(y_pred, y_true)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]\n",
        "            for pred, gold_label in zip(y_pred, y_true)\n",
        "        ]\n",
        "        return true_predictions, true_labels\n",
        "\n",
        "    def compute_metrics():\n",
        "        results = metric.compute()\n",
        "        if data_args.return_entity_level_metrics:\n",
        "            # Unpack nested dictionaries\n",
        "            final_results = {}\n",
        "            for key, value in results.items():\n",
        "                if isinstance(value, dict):\n",
        "                    for n, v in value.items():\n",
        "                        final_results[f\"{key}_{n}\"] = v\n",
        "                else:\n",
        "                    final_results[key] = value\n",
        "            return final_results\n",
        "        if data_args.return_macro_metrics:\n",
        "            Ps, Rs, Fs = [], [], []\n",
        "            for type_name in results:\n",
        "                if type_name.startswith(\"overall\"):\n",
        "                    continue\n",
        "                Ps.append(results[type_name][\"precision\"])\n",
        "                Rs.append(results[type_name][\"recall\"])\n",
        "                Fs.append(results[type_name][\"f1\"])\n",
        "            return {\n",
        "                \"macro_precision\": np.mean(Ps),\n",
        "                \"macro_recall\": np.mean(Rs),\n",
        "                \"macro_f1\": np.mean(Fs),\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"precision\": results[\"overall_precision\"],\n",
        "                \"recall\": results[\"overall_recall\"],\n",
        "                \"f1\": results[\"overall_f1\"],\n",
        "                \"accuracy\": results[\"overall_accuracy\"],\n",
        "            }\n",
        "\n",
        "    logger.info(f\"===== Starting training ({num_epochs} epochs) =====\")\n",
        "    train_time = 0\n",
        "\n",
        "    # make sure weights are replicated on each device\n",
        "    state = replicate(state)\n",
        "\n",
        "    train_time = 0\n",
        "    step_per_epoch = len(train_dataset) // train_batch_size\n",
        "    total_steps = step_per_epoch * num_epochs\n",
        "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0, leave=True)\n",
        "    for epoch in epochs:\n",
        "\n",
        "        train_start = time.time()\n",
        "        train_metrics = []\n",
        "\n",
        "        # Create sampling rng\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "        # train\n",
        "        for step, batch in enumerate(\n",
        "            tqdm(\n",
        "                train_data_collator(input_rng, train_dataset, train_batch_size),\n",
        "                total=step_per_epoch,\n",
        "                desc=\"Training...\",\n",
        "                position=0, leave=True,\n",
        "            )\n",
        "        ):\n",
        "            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs)\n",
        "            train_metrics.append(train_metric)\n",
        "\n",
        "            cur_step = (epoch * step_per_epoch) + (step + 1)\n",
        "\n",
        "            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n",
        "                # Save metrics\n",
        "                train_metric = unreplicate(train_metric)\n",
        "                train_time += time.time() - train_start\n",
        "                if has_tensorboard and jax.process_index() == 0:\n",
        "                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n",
        "\n",
        "                epochs.write(\n",
        "                    f\"Step... ({cur_step}/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate:\"\n",
        "                    f\" {train_metric['learning_rate']})\"\n",
        "                )\n",
        "\n",
        "                train_metrics = []\n",
        "\n",
        "            if (cur_step % training_args.eval_steps == 0 or cur_step % step_per_epoch == 0) and cur_step > 0:\n",
        "\n",
        "                eval_metrics = {}\n",
        "                # evaluate\n",
        "                for batch in tqdm(\n",
        "                    eval_data_collator(eval_dataset, eval_batch_size),\n",
        "                    total=math.ceil(len(eval_dataset) / eval_batch_size),\n",
        "                    desc=\"Evaluating on Dev Set...\",\n",
        "                    position=0, leave=True,\n",
        "                ):\n",
        "                    labels = batch.pop(\"labels\")\n",
        "                    predictions = pad_shard_unpad(p_eval_step)(\n",
        "                        state, batch, min_device_batch=per_device_eval_batch_size\n",
        "                    )\n",
        "                    predictions = np.array(predictions)\n",
        "                    labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n",
        "                    preds, refs = get_labels(predictions, labels)\n",
        "                    metric.add_batch(\n",
        "                        predictions=preds,\n",
        "                        references=refs,\n",
        "                    )\n",
        "\n",
        "                eval_metrics = compute_metrics()\n",
        "                wandb.log(eval_metrics)\n",
        "                logger.info(f\"Step... ({cur_step}/{total_steps} | Validation metrics: {eval_metrics}\")\n",
        "\n",
        "                if has_tensorboard and jax.process_index() == 0:\n",
        "                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n",
        "\n",
        "            #if (cur_step % training_args.save_steps == 0 and cur_step > 0) or (cur_step == total_steps):\n",
        "            if 10==11:\n",
        "                # save checkpoint after each epoch and push checkpoint to the hub\n",
        "                if jax.process_index() == 0:\n",
        "                    params = jax.device_get(unreplicate(state.params))\n",
        "                    model.save_pretrained(training_args.output_dir, params=params)\n",
        "                    tokenizer.save_pretrained(training_args.output_dir)\n",
        "                    if training_args.push_to_hub:\n",
        "                        repo.push_to_hub(commit_message=f\"Saving weights and logs of step {cur_step}\", blocking=False)\n",
        "        epochs.desc = f\"Epoch ... {epoch + 1}/{num_epochs}\"\n",
        "\n",
        "    # Eval after training\n",
        "    if training_args.do_eval:\n",
        "        eval_metrics = {}\n",
        "        eval_loader = eval_data_collator(eval_dataset, eval_batch_size)\n",
        "        for batch in tqdm(eval_loader, total=len(eval_dataset) // eval_batch_size, desc=\"Evaluating on Dev Set ...\", position=0, leave=True):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            predictions = pad_shard_unpad(p_eval_step)(state, batch, min_device_batch=per_device_eval_batch_size)\n",
        "            predictions = np.array(predictions)\n",
        "            labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n",
        "            preds, refs = get_labels(predictions, labels)\n",
        "            metric.add_batch(predictions=preds, references=refs)\n",
        "\n",
        "        eval_metrics = compute_metrics()\n",
        "        logger.info(f\"Evaluation Results on Dev Set : {eval_metrics}\")\n",
        "    if training_args.do_test:\n",
        "        eval_metrics = {}\n",
        "        eval_loader = eval_data_collator(test_dataset, eval_batch_size)\n",
        "        preds_list=[]\n",
        "        preds_dict={}\n",
        "        labels_list=[]\n",
        "        for batch in tqdm(eval_loader, total=len(test_dataset) // eval_batch_size, desc=\"Evaluating on Test Set...\", position=0, leave=True):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            predictions = pad_shard_unpad(p_eval_step)(state, batch, min_device_batch=per_device_eval_batch_size)\n",
        "            predictions = np.array(predictions)\n",
        "            labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n",
        "            preds, refs = get_labels(predictions, labels)\n",
        "            for pred_example in preds:\n",
        "                      preds_list.append(pred_example)\n",
        "            for label_example in refs:\n",
        "                      labels_list.append(label_example)\n",
        "            metric.add_batch(predictions=preds, references=refs)\n",
        "        raw_ids=[]\n",
        "        for row in raw_datasets[\"test\"]:\n",
        "          raw_ids.append(row[\"id\"])\n",
        "        for label_id,pred_value in zip(raw_ids,preds_list):\n",
        "              preds_dict[label_id]=pred_value\n",
        "        if \"/\" in model_args.model_name_or_path:\n",
        "                   model_name_hf=model_args.model_name_or_path.split(\"/\")[-1]\n",
        "        else:\n",
        "                   model_name_hf=model_args.model_name_or_path\n",
        "        final_entry={\"dataset_name\": data_args.blurb_task, \"model_name\":model_name_hf, \"predictions\":preds_dict}\n",
        "        with open(mmodel_name_hf+\"_\"+str(data_args.blurb_task)+\".json\", \"w\") as f:\n",
        "          json.dump(final_entry, f, indent=4, sort_keys=True)\n",
        "        eval_metrics = compute_metrics()\n",
        "        logger.info(f\"Evaluation Results on Test Set : {eval_metrics}\")\n",
        "        #if jax.process_index() == 0:\n",
        "        #    eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n",
        "        #    path = os.path.join(training_args.output_dir, \"eval_results.json\")\n",
        "        #    with open(path, \"w\") as f:\n",
        "        #        json.dump(eval_metrics, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "U9OrjrypUn-2",
        "outputId": "b6ce44cf-3488-400f-c138-64f0338ec45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_flax_ner2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that our code will show both eval and test results and also will output the prediction as BLURB format as [model-name-task.json] (e.g BioM-ALBERT-xxlarge-PMC_BC5-disease.json).\n"
      ],
      "metadata": {
        "id": "Gd0p-_e3FRqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BioM-ALBERT**"
      ],
      "metadata": {
        "id": "1BYtC95RNi7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/seqcls/chemprot_hf/train.json \\\n",
        "--validation_file data/seqcls/chemprot_hf/dev.json \\\n",
        "--test_file data/seqcls/chemprot_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 6 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task ChemProt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3cee9c-8dee-4bbe-f21c-5d32be7492c2",
        "id": "mxsMiXr-Ni7g"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-7d7c32e2c7c8f019\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-7d7c32e2c7c8f019/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 647.97it/s]\n",
            "['0', 'CPR:3', 'CPR:4', 'CPR:5', 'CPR:6', 'CPR:9']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'dense', 'kernel'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'decoder', 'bias'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'dense', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'LayerNorm', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  7.56ba/s]\n",
            "100%|███████████████████████████████████████████| 12/12 [00:01<00:00,  7.41ba/s]\n",
            "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  7.41ba/s]\n",
            "INFO:__main__:Sample 15156 of the training set: {'input_ids': [2, 1239, 4314, 22, 609, 24, 1678, 799, 85, 27, 2018, 21, 30, 804, 131, 56, 10, 699, 22920, 24, 21, 6388, 3567, 7056, 7145, 595, 36, 111, 514, 24, 21, 30, 804, 131, 56, 195, 1377, 150, 48, 21, 6388, 4899, 7056, 304, 36, 1875, 21, 30, 804, 131, 56, 18828, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 6976 of the training set: {'input_ids': [2, 1457, 4561, 14270, 4020, 24, 22, 5924, 43, 21, 6388, 3567, 7056, 5287, 25, 2400, 9754, 5287, 230, 297, 29, 14628, 20, 2316, 29, 21, 6388, 4899, 7056, 10, 229, 9595, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 4142 of the training set: {'input_ids': [2, 22, 78, 120, 36, 21, 5, 39, 6, 411, 1969, 14, 392, 410, 425, 160, 6703, 451, 4869, 3721, 197, 194, 26, 1199, 2717, 19, 21, 5, 38, 6, 160, 6703, 21, 5, 214, 25, 14, 228, 411, 1969, 14, 392, 6, 99, 264, 21, 6388, 12500, 10, 3567, 7056, 21, 5, 30, 3674, 6, 25, 2993, 14326, 21, 5, 5819, 697, 6, 650, 23, 255, 102, 6306, 21, 5, 7407, 6, 104, 108, 29, 22, 144, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (6 epochs) =====\n",
            "Step... (500/4506 | Training Loss: 0.31032097339630127, Learning Rate: 2.6677762434701435e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:53<00:00,  3.31it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.7562076749435666, 'eval_recall': 0.834717607973422, 'eval_F1': 0.793525463876826}| Step... (751/4506 | Eval metrics: {'accuracy': 0.9104543840965567}) \n",
            "Training...: 100%|████████████████████████████| 751/751 [05:45<00:00,  2.17it/s]\n",
            "Step... (1000/4506 | Training Loss: 0.04030299559235573, Learning Rate: 2.3348868126049638e-05)\n",
            "Step... (1500/4506 | Training Loss: 0.022429658100008965, Learning Rate: 2.0019971998408437e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:46<00:00,  3.77it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7509433962264151, 'eval_recall': 0.8264119601328903, 'eval_F1': 0.7868722815342033}| Step... (1502/4506 | Eval metrics: {'accuracy': 0.9073482428115016}) \n",
            "Training...: 100%|████████████████████████████| 751/751 [05:12<00:00,  2.40it/s]\n",
            "Step... (2000/4506 | Training Loss: 0.09312742203474045, Learning Rate: 1.6691079508746043e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:47<00:00,  3.75it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8314005352363961, 'eval_recall': 0.7740863787375415, 'eval_F1': 0.8017204301075269}| Step... (2253/4506 | Eval metrics: {'accuracy': 0.9207490237841676}) \n",
            "Training...: 100%|████████████████████████████| 751/751 [05:13<00:00,  2.39it/s]\n",
            "Step... (2500/4506 | Training Loss: 0.09287960827350616, Learning Rate: 1.3362183381104842e-05)\n",
            "Step... (3000/4506 | Training Loss: 0.06604659557342529, Learning Rate: 1.0033289072453044e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:47<00:00,  3.76it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8091570389388104, 'eval_recall': 0.7852990033222591, 'eval_F1': 0.7970495258166491}| Step... (3004/4506 | Eval metrics: {'accuracy': 0.917199148029819}) \n",
            "Training...: 100%|████████████████████████████| 751/751 [05:12<00:00,  2.40it/s]\n",
            "Step... (3500/4506 | Training Loss: 0.035590577870607376, Learning Rate: 6.704393854306545e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:46<00:00,  3.77it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8223684210526315, 'eval_recall': 0.7786544850498339, 'eval_F1': 0.7999146757679181}| Step... (3755/4506 | Eval metrics: {'accuracy': 0.919062832800852}) \n",
            "Training...: 100%|████████████████████████████| 751/751 [05:12<00:00,  2.40it/s]\n",
            "Step... (4000/4506 | Training Loss: 0.0007902801153250039, Learning Rate: 3.3754990909073967e-06)\n",
            "Step... (4500/4506 | Training Loss: 0.0020898955408483744, Learning Rate: 4.660427421754321e-08)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:47<00:00,  3.75it/s]\n",
            "INFO:__main__:{'eval_precision': 0.804989604989605, 'eval_recall': 0.8039867109634552, 'eval_F1': 0.8044878454186577}| Step... (4506/4506 | Eval metrics: {'accuracy': 0.9187965921192758}) \n",
            "Training...: 100%|████████████████████████████| 751/751 [05:14<00:00,  2.39it/s]\n",
            "Epoch ... 6/6: 100%|█████████████████████████████| 6/6 [31:51<00:00, 318.60s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████| 247/247 [01:05<00:00,  3.77it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8199327422806482, 'test_recall': 0.7819241982507289, 'test_F1': 0.8004775406655723}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge  \\\n",
        "--train_file data/seqcls/DDI_hf/train.json \\\n",
        "--validation_file data/seqcls/DDI_hf/dev.json \\\n",
        "--test_file data/seqcls/DDI_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 3 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task DDI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08377a01-fdf2-4221-b14a-83803371aa63",
        "id": "kPVHGThlNi7g"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-0ecf918e6c735628\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 673.89it/s]\n",
            "['0', 'DDI-advise', 'DDI-effect', 'DDI-int', 'DDI-mechanism']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'kernel'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'bias'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-1dc724a890889973.arrow\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  9.10ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-27fde02185bedc59.arrow\n",
            "INFO:__main__:Sample 17555 of the training set: {'input_ids': [2, 376, 2676, 903, 7814, 6442, 11982, 23, 21, 8755, 18398, 8334, 594, 15166, 44, 2676, 422, 24, 22, 340, 8248, 20, 21, 10, 5239, 46, 1634, 21, 14018, 906, 162, 41, 5523, 14, 4744, 21, 14018, 906, 21, 5, 6388, 4928, 7056, 6, 23, 21, 6209, 6209, 906, 21, 5, 3738, 197, 15159, 23, 2151, 6, 23, 10932, 21, 14018, 906, 21, 5, 15594, 11973, 89, 6, 23, 21, 6388, 4928, 7056, 21, 5, 7117, 914, 6, 23, 46, 23405, 687, 14018, 906, 21, 5, 16003, 375, 10, 4413, 2477, 6, 19, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 7104 of the training set: {'input_ids': [2, 14573, 808, 20, 6667, 569, 1336, 21, 6388, 4928, 7056, 79, 961, 28, 3792, 2128, 24, 22, 14479, 176, 27, 2960, 10, 4924, 21, 6388, 4928, 7056, 23, 23852, 46, 5795, 808, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "INFO:__main__:Sample 8245 of the training set: {'input_ids': [2, 310, 719, 20, 7063, 9399, 388, 5976, 1353, 79, 3105, 29, 291, 616, 23, 21, 440, 12845, 3627, 952, 21, 5, 2132, 290, 196, 6, 20, 21, 2132, 290, 196, 193, 9530, 842, 21, 6388, 4928, 7056, 21, 21422, 21, 10, 44, 1296, 1217, 21, 10, 21, 130, 2191, 13531, 10197, 21, 10, 2098, 20, 7063, 9399, 388, 5976, 1353, 25, 2098, 601, 4933, 21, 13078, 30, 965, 21, 10, 20264, 9181, 20, 7063, 9399, 388, 5976, 1353, 798, 22, 129, 24, 21, 6388, 4928, 7056, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (3 epochs) =====\n",
            "Step... (500/9486 | Training Loss: 0.5325804352760315, Learning Rate: 1.8947923308587633e-05)\n",
            "Step... (1000/9486 | Training Loss: 0.5035398602485657, Learning Rate: 1.7893738913699053e-05)\n",
            "Step... (1500/9486 | Training Loss: 0.6641604900360107, Learning Rate: 1.683955269982107e-05)\n",
            "Step... (2000/9486 | Training Loss: 0.01098472997546196, Learning Rate: 1.5785366485943086e-05)\n",
            "Step... (2500/9486 | Training Loss: 0.17612741887569427, Learning Rate: 1.4731183000549208e-05)\n",
            "Step... (3000/9486 | Training Loss: 0.0232503954321146, Learning Rate: 1.3676996786671225e-05)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:17<00:00,  2.27it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.838495575221239, 'eval_recall': 0.8081023454157783, 'eval_F1': 0.8230184581976113}| Step... (3162/9486 | Eval metrics: {'accuracy': 0.9415064102564102}) \n",
            "Training...: 100%|██████████████████████████| 3162/3162 [10:09<00:00,  5.19it/s]\n",
            "Step... (3500/9486 | Training Loss: 0.019379079341888428, Learning Rate: 1.2622811482287943e-05)\n",
            "Step... (4000/9486 | Training Loss: 0.08171368390321732, Learning Rate: 1.1568627087399364e-05)\n",
            "Step... (4500/9486 | Training Loss: 0.05295616388320923, Learning Rate: 1.0514442692510784e-05)\n",
            "Step... (5000/9486 | Training Loss: 0.0029894239269196987, Learning Rate: 9.460257388127502e-06)\n",
            "Step... (5500/9486 | Training Loss: 0.004378994461148977, Learning Rate: 8.406071174249519e-06)\n",
            "Step... (6000/9486 | Training Loss: 0.006957344710826874, Learning Rate: 7.35188723410829e-06)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:10<00:00,  3.73it/s]\n",
            "INFO:__main__:{'eval_precision': 0.888641425389755, 'eval_recall': 0.8507462686567164, 'eval_F1': 0.869281045751634}| Step... (6324/9486 | Eval metrics: {'accuracy': 0.9567307692307693}) \n",
            "Training...: 100%|██████████████████████████| 3162/3162 [09:37<00:00,  5.48it/s]\n",
            "Step... (6500/9486 | Training Loss: 0.00893176719546318, Learning Rate: 6.297701474977657e-06)\n",
            "Step... (7000/9486 | Training Loss: 0.008705218322575092, Learning Rate: 5.243517080089077e-06)\n",
            "Step... (7500/9486 | Training Loss: 0.015874136239290237, Learning Rate: 4.189331320958445e-06)\n",
            "Step... (8000/9486 | Training Loss: 0.007779440376907587, Learning Rate: 3.1351457892014878e-06)\n",
            "Step... (8500/9486 | Training Loss: 0.0009259102516807616, Learning Rate: 2.080961394312908e-06)\n",
            "Step... (9000/9486 | Training Loss: 0.0037473735865205526, Learning Rate: 1.0267758625559509e-06)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:10<00:00,  3.73it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8607068607068608, 'eval_recall': 0.8827292110874201, 'eval_F1': 0.8715789473684211}| Step... (9486/9486 | Eval metrics: {'accuracy': 0.9571314102564102}) \n",
            "Training...: 100%|██████████████████████████| 3162/3162 [09:37<00:00,  5.48it/s]\n",
            "Epoch ... 3/3: 100%|█████████████████████████████| 3/3 [29:24<00:00, 588.25s/it]\n",
            "Evaluating on Test Data ...: 100%|██████████████| 90/90 [00:23<00:00,  3.78it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8456725755995829, 'test_recall': 0.8283963227783453, 'test_F1': 0.8369453044375644}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge  \\\n",
        "--train_file data/seqcls/GAD_hf/train.json \\\n",
        "--validation_file data/seqcls/GAD_hf/dev.json \\\n",
        "--test_file data/seqcls/GAD_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 8 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task GAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68f679d-e1f8-4cd0-d689-b623b6eaa2be",
        "id": "WWEtyA8QNi7h"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-5058072c4f44a661\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-5058072c4f44a661/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 854.12it/s]\n",
            "['0', '1']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('sop_classifier', 'classifier', 'bias'), ('albert', 'embeddings', 'position_ids'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00,  9.29ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.57ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.30ba/s]\n",
            "INFO:__main__:Sample 3694 of the training set: {'input_ids': [2, 22, 21, 6388, 3567, 7056, 6, 279, 165, 79, 824, 54, 45, 28, 750, 1039, 26, 22, 466, 2415, 1025, 21, 6388, 14970, 7056, 26, 153, 25, 1126, 3782, 26, 7465, 23, 882, 69, 78, 292, 45, 497, 41, 3513, 834, 27, 22, 364, 445, 24, 227, 577, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "INFO:__main__:Sample 2391 of the training set: {'input_ids': [2, 69, 78, 233, 36, 22, 21, 6388, 3567, 7056, 21, 10, 39, 9650, 788, 14, 347, 2282, 79, 45, 103, 29, 1741, 27, 21, 6388, 14970, 7056, 26, 22, 21, 7104, 5645, 11280, 280, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 4180 of the training set: {'input_ids': [2, 41, 21, 128, 20344, 554, 1506, 59, 521, 74, 110, 27, 274, 1463, 106, 26, 2543, 334, 25, 27, 2591, 27, 21, 6175, 26950, 23, 50, 1717, 761, 36, 21, 6388, 3567, 7056, 79, 836, 28, 181, 26, 21, 6388, 14970, 7056, 1445, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (8 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:09<00:00,  1.02s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.7802197802197802, 'eval_recall': 0.726962457337884, 'eval_F1': 0.7526501766784452}| Step... (177/1416 | Eval metrics: {'accuracy': 0.7383177570093458}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:34<00:00,  1.87it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.35it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8267148014440433, 'eval_recall': 0.7815699658703071, 'eval_F1': 0.8035087719298245}| Step... (354/1416 | Eval metrics: {'accuracy': 0.7906542056074767}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.70it/s]\n",
            "Step... (500/1416 | Training Loss: 0.31069961190223694, Learning Rate: 1.2951976714248303e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8391608391608392, 'eval_recall': 0.8191126279863481, 'eval_F1': 0.8290155440414507}| Step... (531/1416 | Eval metrics: {'accuracy': 0.8149532710280374}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8031746031746032, 'eval_recall': 0.863481228668942, 'eval_F1': 0.8322368421052632}| Step... (708/1416 | Eval metrics: {'accuracy': 0.8093457943925234}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7748538011695907, 'eval_recall': 0.9044368600682594, 'eval_F1': 0.8346456692913385}| Step... (885/1416 | Eval metrics: {'accuracy': 0.8037383177570093}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Step... (1000/1416 | Training Loss: 0.005597257986664772, Learning Rate: 5.8898303905152716e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.39it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8402777777777778, 'eval_recall': 0.825938566552901, 'eval_F1': 0.8330464716006885}| Step... (1062/1416 | Eval metrics: {'accuracy': 0.8186915887850468}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.70it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.36it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8205980066445183, 'eval_recall': 0.8430034129692833, 'eval_F1': 0.8316498316498316}| Step... (1239/1416 | Eval metrics: {'accuracy': 0.8130841121495327}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8211920529801324, 'eval_recall': 0.8464163822525598, 'eval_F1': 0.8336134453781514}| Step... (1416/1416 | Eval metrics: {'accuracy': 0.8149532710280374}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Epoch ... 8/8: 100%|██████████████████████████████| 8/8 [09:12<00:00, 69.05s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 9/9 [00:02<00:00,  3.81it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8166666666666667, 'test_recall': 0.8718861209964412, 'test_F1': 0.8433734939759037}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge  \\\n",
        "--train_file data/seqcls/bioasq_hf/train.json \\\n",
        "--validation_file data/seqcls/bioasq_hf/dev.json \\\n",
        "--test_file data/seqcls/bioasq_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 5 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--blurb_task BioASQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb571f11-791c-45f9-c662-91804e72f7bc",
        "id": "OkGJ-ULBNi7h"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-25f663af987ecd04\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-25f663af987ecd04/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 809.19it/s]\n",
            "['no', 'yes']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('albert', 'embeddings', 'position_ids'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'kernel'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.72ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.67ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.47ba/s]\n",
            "INFO:__main__:Sample 166 of the training set: {'input_ids': [2, 44, 28, 5358, 1558, 7306, 3023, 3, 28, 5358, 21, 210, 2334, 30, 26, 22, 153, 7355, 28, 5358, 1558, 84, 45, 7306, 26, 88, 125, 3504, 23, 37, 88, 806, 1809, 30, 21, 13, 820, 28, 5358, 21, 210, 2334, 30, 13, 44, 7306, 37, 8579, 21, 5009, 43, 82, 2713, 1373, 21, 13, 9311, 28, 5358, 21, 210, 2334, 30, 13, 44, 7306, 37, 8579, 21, 1783, 41, 690, 24, 109, 10, 25, 123, 10, 1857, 10, 5095, 21, 210, 2334, 30, 708, 122, 59, 307, 36, 90, 867, 25, 6413, 28, 5358, 21, 210, 2334, 30, 836, 28, 341, 181, 26, 345, 2010, 816, 24, 165, 106, 28, 5358, 21, 210, 2334, 30, 7306, 43, 69, 1558, 44, 159, 48, 166, 107, 48, 219, 76, 218, 75, 399, 402, 273, 185, 82, 3005, 28, 5358, 21, 210, 2334, 30, 44, 406, 27, 2591, 22, 15224, 268, 21, 30, 6024, 117, 14, 539, 102, 207, 24, 1809, 21, 5009, 7306, 28, 5358, 21, 210, 2334, 26, 4520, 756, 252, 68, 83, 7828, 2665, 105, 27, 336, 22, 207, 24, 1809, 21, 5009, 3712, 28, 5358, 21, 210, 2334, 25, 110, 47, 102, 106, 24, 28, 5358, 21, 210, 2334, 26, 4520, 756, 3357, 21, 210, 2334, 11288, 24, 6413, 28, 5358, 1558, 26, 22, 153, 7355, 7306, 28, 5358, 1013, 84, 2622, 5567, 721, 37, 5505, 203, 7879, 26, 22, 7238, 1285, 24, 21, 24587, 21, 210, 2334, 30, 23, 28, 10, 375, 10, 196, 7514, 44, 8691, 213, 7306, 4491, 46, 7983, 28, 5358, 1013, 28, 5358, 10, 910, 21, 210, 2334, 11288, 79, 45, 28, 435, 32, 19817, 123, 6894, 3208, 23, 54, 28783, 32, 3054, 22, 266, 24, 7306, 28, 8032, 1457, 107, 24, 28, 5358, 21, 210, 2334, 96, 125, 273, 24, 402, 86, 21, 12799, 1417, 1446, 370, 715, 28, 5358, 106, 26, 153, 76, 889, 25, 82, 5944, 3695, 4595, 197, 198, 28, 5358, 106, 591, 5478, 37, 76, 256, 23, 218, 273, 25, 2403, 636, 22, 7238, 1285, 24, 28, 5358, 2866, 1309, 7306, 37, 21, 210, 2334, 8579, 21, 5009, 1082, 204, 571, 605, 32, 5944, 3695, 4595, 25, 22, 1285, 24, 1309, 488, 28, 5358, 2866, 26, 22, 1177, 21, 5, 6858, 128, 92, 21, 381, 85, 6, 8488, 27, 1033, 28, 5358, 24049, 68, 233, 36, 22, 1908, 1013, 4612, 43, 147, 28, 5358, 1558, 25, 416, 30, 301, 23368, 30, 870, 54, 2053, 50, 2519, 23, 25, 6592, 36, 150, 28, 364, 3020, 24, 162, 1013, 84, 45, 7306, 26, 455, 15, 69, 5653, 233, 36, 291, 28, 5358, 554, 1013, 44, 4206, 466, 1558, 36, 84, 1154, 85, 3312, 27, 203, 3824, 2866, 77, 41, 47, 1838, 28, 788, 23246, 5062, 24, 47, 21, 210, 2334, 7306, 43, 22, 28, 5358, 554, 2519, 37, 21, 210, 2334, 1809, 21, 5009, 15, 11370, 25, 7113, 4383, 44, 22669, 29, 14631, 24, 13210, 5899, 24, 28, 5358, 25, 21, 251, 39, 1558, 9316, 37, 5944, 4595, 23, 21, 196, 15, 130, 15, 23, 37, 1908, 394, 22096, 24, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "INFO:__main__:Sample 644 of the training set: {'input_ids': [2, 35, 21, 5010, 5804, 131, 11472, 363, 32, 1776, 138, 3023, 3, 3245, 20, 26, 42, 29, 21, 7024, 2250, 1129, 46, 21, 7024, 2250, 1083, 10, 565, 25, 21, 2179, 388, 92, 7965, 10, 1930, 1776, 3111, 297, 29, 21, 5010, 5804, 131, 11472, 23, 850, 10, 820, 308, 31, 1256, 63, 26, 42, 29, 21, 7024, 2250, 1083, 10, 565, 21, 2179, 388, 166, 3111, 15, 92, 21, 2179, 388, 35, 103, 29, 176, 27, 22, 8266, 85, 774, 21, 5010, 5804, 131, 11472, 26, 21, 7024, 2250, 1083, 10, 565, 1776, 138, 15, 21, 5010, 5804, 131, 11472, 4953, 32, 1776, 138, 15, 22, 2320, 4012, 4953, 22, 8266, 85, 774, 21, 5010, 5804, 131, 11472, 27, 2108, 232, 29, 1229, 1776, 138, 167, 59, 3001, 74, 297, 29, 48, 642, 88, 29116, 25, 59, 28, 21, 7024, 2250, 39, 46, 21, 7024, 2250, 38, 165, 1011, 220, 37, 47, 4953, 19128, 733, 262, 15, 21, 5010, 5804, 131, 11472, 21, 5, 9575, 7112, 1151, 205, 6, 35, 47, 729, 23, 364, 1571, 23, 928, 21, 5, 2193, 85, 10, 10509, 6, 1809, 774, 532, 302, 37, 21, 4779, 5073, 7583, 23, 26, 217, 15, 21, 5, 251, 14895, 4527, 23, 587, 23, 21, 606, 131, 6, 32, 22, 70, 24, 1541, 2762, 15, 137, 73, 74, 4953, 26, 22, 21, 606, 131, 41, 6671, 32, 22, 70, 24, 42, 29, 7141, 21, 7024, 2250, 1011, 21, 5, 6068, 205, 1217, 25, 14, 228, 3746, 6, 103, 1229, 1776, 138, 167, 59, 74, 297, 29, 88, 46, 91, 29116, 15, 26, 3458, 23, 22, 21, 606, 907, 25, 310, 602, 4953, 175, 433, 616, 32, 22, 70, 24, 1541, 3877, 10, 7121, 1151, 13962, 26, 817, 42, 29, 2122, 10, 6823, 5042, 23, 21, 906, 7273, 18150, 32, 22, 70, 24, 1848, 138, 23, 25, 21, 5010, 5804, 131, 11472, 32, 22, 70, 24, 1776, 138, 19, 86, 26, 3458, 23, 22, 140, 24, 521, 4953, 4387, 808, 21, 5, 4330, 21, 906, 7273, 18150, 6, 31, 4786, 139, 604, 203, 161, 892, 15, 5599, 20, 5010, 5804, 131, 11472, 31, 21304, 25, 87, 104, 26, 42, 29, 7965, 10, 1930, 9765, 7024, 2250, 39, 14, 38, 10, 18211, 306, 190, 3829, 15, 163, 251, 1049, 27471, 1536, 14, 251, 1049, 20, 27, 332, 22, 13037, 23, 984, 23, 640, 23, 25, 22, 181, 24, 21, 5010, 5804, 131, 11472, 26, 22, 70, 24, 9586, 23, 1229, 1776, 138, 15, 1536, 7024, 1049, 1536, 251, 1049, 7287, 205, 2214, 1536, 14, 251, 1049, 20, 28, 189, 24, 119, 454, 21, 196, 14, 1783, 727, 25, 80, 454, 21, 1783, 912, 59, 323, 22, 984, 25, 640, 24, 729, 21, 5010, 5804, 131, 11472, 26, 1776, 138, 15, 21, 5010, 5804, 131, 11472, 31, 110, 27, 45, 1063, 143, 4050, 26, 113, 727, 23, 29, 22, 147, 326, 946, 667, 532, 3758, 23, 3152, 23, 25, 6305, 15, 1536, 7024, 1049, 1536, 251, 1049, 1999, 15242, 941, 1536, 14, 251, 1049, 20, 21, 5010, 5804, 131, 11472, 1170, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "INFO:__main__:Sample 27 of the training set: {'input_ids': [2, 890, 22, 21, 4925, 13707, 109, 587, 10, 27436, 29, 28002, 3023, 3, 27, 489, 28002, 10, 2798, 10, 4925, 13707, 21, 5, 13078, 217, 6, 343, 667, 26, 455, 68, 6319, 28002, 25, 21, 4925, 13707, 23, 41, 143, 41, 28, 2650, 24, 252, 10, 385, 25, 8634, 2010, 4965, 77, 13976, 10, 10453, 26, 277, 780, 356, 15, 26, 475, 27, 694, 24, 22, 1177, 638, 28002, 25, 21, 4925, 13707, 25746, 23, 788, 14366, 561, 15547, 29, 22, 343, 24, 10719, 4965, 25, 8508, 10, 17221, 30, 25, 44, 99, 103, 29, 356, 10, 385, 603, 267, 15, 1416, 68, 395, 36, 28002, 30, 25746, 29, 21, 4925, 13707, 48, 88, 822, 16753, 2866, 23, 22, 21, 128, 9541, 39, 10, 14176, 49, 25, 22, 3588, 13078, 1329, 39, 14, 1034, 13078, 1329, 39, 4438, 39, 2866, 15, 37, 140, 24, 153, 4520, 756, 57, 21, 5, 5990, 85, 190, 38, 6, 23, 68, 110, 36, 356, 10, 385, 862, 182, 25746, 29, 28002, 1891, 24, 21, 4925, 13707, 48, 356, 10, 385, 1524, 36, 44, 1044, 43, 177, 110, 26, 564, 138, 57, 535, 28002, 84, 25746, 29, 21, 4925, 13707, 23, 68, 192, 3446, 518, 6054, 32, 22, 28002, 2224, 21, 9928, 837, 25, 110, 4586, 25, 643, 10, 385, 21, 9928, 837, 3403, 27, 21, 4925, 13707, 26, 112, 21, 4042, 2866, 1416, 68, 274, 36, 6260, 5499, 697, 39, 35, 1100, 2083, 37, 28002, 25, 21, 5503, 4925, 217, 343, 238, 21, 5, 4925, 13707, 6, 48, 22, 160, 39, 25, 160, 38, 6239, 23, 25, 213, 22, 7800, 60, 160, 39, 25, 160, 38, 15, 22, 23960, 343, 561, 32, 28002, 25, 21, 4925, 13707, 15547, 29, 3918, 3058, 36, 8167, 8508, 10, 440, 441, 23, 25, 88, 24, 22, 28002, 14, 4925, 13707, 561, 22802, 41, 21, 19232, 30, 26, 47, 26, 455, 585, 22, 220, 28002, 25, 21, 4925, 13707, 343, 561, 44, 591, 27, 45, 5068, 10, 8796, 1558, 21, 5, 6321, 30, 6, 32, 5499, 697, 39, 1196, 317, 86, 14144, 21, 210, 2334, 1809, 21, 1783, 21, 5, 210, 11115, 1783, 6, 15, 68, 59, 110, 36, 21, 4925, 13707, 25, 28002, 44, 508, 4360, 48, 22, 13008, 25, 1953, 6599, 3208, 32, 22, 21, 301, 5384, 39, 25, 21, 301, 5384, 38, 131, 267, 23, 75, 137, 35, 54, 1348, 406, 548, 21, 4925, 13707, 25, 28002, 79, 8706, 173, 2378, 69, 3208, 21, 3778, 12919, 381, 871, 24, 6768, 21, 4925, 13707, 14, 1296, 17346, 89, 2866, 20, 28, 443, 181, 26, 5808, 10327, 1614, 26, 2294, 4383, 227, 105, 294, 20, 80, 6, 6768, 21, 4925, 13707, 2866, 33, 1425, 26, 6768, 1007, 3446, 25, 633, 587, 10, 11751, 29, 6768, 28002, 2866, 26, 334, 23, 28, 1368, 24, 21, 4925, 13707, 25, 28002, 343, 561, 15547, 23, 969, 29, 22, 198, 32, 452, 719, 60, 28002, 25, 21, 4925, 13707, 75, 86, 452, 8030, 24, 806, 818, 1416, 23, 68, 1341, 40, 22, 2881, 1444, 24, 21, 4925, 13707, 25, 22, 28002, 26, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (5 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:08<00:00,  4.15s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_accuracy': 0.8266666531562805}| Step... (83/415 | Eval metrics: {'accuracy': 0.8266666666666667}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:53<00:00,  1.57it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8266666531562805}| Step... (166/415 | Eval metrics: {'accuracy': 0.8266666666666667}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (249/415 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9333333373069763}| Step... (332/415 | Eval metrics: {'accuracy': 0.9333333333333333}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (415/415 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n",
            "Epoch ... 5/5: 100%|██████████████████████████████| 5/5 [02:23<00:00, 28.61s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 3/3 [00:01<00:00,  1.87it/s]\n",
            "INFO:__main__: test results : {'test_accuracy': 0.9357143044471741}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/seqcls/pubmedqa_hf/train.json \\\n",
        "--validation_file data/seqcls/pubmedqa_hf/dev.json \\\n",
        "--test_file data/seqcls/pubmedqa_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 7 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--blurb_task PubMedQA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e162e229-16dc-48e4-eb25-97d7930be111",
        "id": "S4E1a6CyNi7h"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-d26b3028a530c846\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-d26b3028a530c846/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 839.08it/s]\n",
            "['maybe', 'no', 'yes']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'decoder', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.23ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.17ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.30ba/s]\n",
            "INFO:__main__:Sample 71 of the training set: {'input_ids': [2, 3426, 27, 2657, 14138, 20, 28, 3840, 1108, 27, 123, 9609, 1849, 1449, 32, 22, 396, 10, 243, 24, 28261, 3023, 3, 3426, 27, 2657, 14138, 21, 5, 10569, 6, 35, 28, 203, 1128, 10, 1289, 10, 249, 468, 491, 36, 1570, 1380, 2128, 29, 2000, 185, 26, 1109, 414, 15, 22, 943, 24, 50, 53, 31, 27, 1043, 21, 10569, 29, 123, 9609, 1849, 1449, 21, 5, 14366, 4925, 6, 25, 27, 446, 134, 198, 181, 32, 22, 396, 10, 243, 24, 42, 29, 28261, 26, 28, 123, 24260, 1605, 15, 28, 1369, 332, 24, 42, 29, 28261, 48, 227, 4111, 36, 629, 21, 14366, 4925, 25, 21, 10569, 43, 21, 7636, 173, 3476, 27, 21, 14375, 10089, 381, 3477, 31, 192, 15, 555, 1633, 21, 5, 4487, 6, 36, 231, 54, 2995, 70, 46, 7242, 6677, 33, 1713, 2182, 27, 88, 8790, 12231, 23, 167, 1018, 7242, 2337, 23, 445, 482, 21, 5, 1749, 5, 38, 6, 6, 23, 1095, 7242, 791, 21, 5, 1749, 6, 23, 25, 1734, 23, 32, 90, 21, 10569, 25, 21, 14366, 4925, 15, 258, 261, 329, 21, 5010, 33, 108, 15, 198, 960, 2201, 7242, 525, 156, 23, 215, 7242, 445, 25, 410, 473, 635, 21, 5, 251, 4219, 6, 23, 33, 323, 15, 16073, 1689, 31, 354, 77, 22, 17564, 543, 2278, 27, 1020, 22, 5123, 24, 782, 705, 37, 22, 12231, 15, 21, 10569, 25, 21, 14366, 4925, 307, 265, 7242, 525, 342, 26, 1015, 24, 7242, 2767, 25, 7242, 482, 676, 5, 38, 6, 15, 24, 22, 4758, 21, 4487, 408, 23, 2466, 21, 4487, 120, 3851, 7242, 2767, 40, 21, 10569, 25, 21, 14366, 4925, 15, 22, 258, 413, 26, 7242, 482, 31, 562, 15, 58, 676, 5, 38, 6, 21, 5, 10, 56, 15, 67, 27, 2195, 15, 58, 6, 23, 160, 64, 52, 15, 397, 39, 15, 22, 258, 445, 24, 22, 3465, 7242, 40, 21, 14366, 4925, 25, 21, 10569, 31, 881, 15, 981, 25, 457, 15, 1671, 676, 23, 170, 15, 7242, 445, 25, 21, 251, 4219, 231, 54, 601, 28, 111, 413, 26, 7242, 525, 342, 15, 16073, 1689, 120, 28, 994, 543, 60, 12231, 25, 2267, 5307, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "INFO:__main__:Sample 199 of the training set: {'input_ids': [2, 574, 2699, 223, 26, 461, 7583, 20, 44, 6215, 10, 249, 534, 22, 8005, 3023, 3, 461, 7583, 35, 21, 3327, 14031, 118, 361, 1764, 27, 21485, 223, 25, 632, 161, 430, 23, 29, 28, 424, 24, 6215, 10, 249, 534, 83, 20530, 15, 22, 1137, 13, 526, 31, 27, 336, 22, 182, 2201, 22, 978, 25, 140, 24, 28, 6215, 10, 249, 1496, 180, 32, 461, 7583, 26, 22, 28, 606, 12324, 2309, 1605, 15, 22, 1137, 528, 552, 2852, 25, 148, 461, 7583, 1803, 23, 2092, 23, 25, 10169, 26, 82, 70, 1605, 48, 235, 1978, 26, 125, 5772, 3473, 15, 22, 6215, 10, 249, 180, 1460, 28, 341, 181, 26, 12555, 7583, 70, 746, 5587, 561, 15, 494, 23, 134, 140, 5478, 834, 27, 626, 1734, 23, 7160, 1444, 23, 25, 961, 15, 28, 424, 24, 1817, 566, 40, 3092, 13, 3379, 1514, 25, 140, 24, 22, 6215, 10, 249, 180, 15, 204, 182, 44, 7160, 10, 385, 21, 5, 4132, 23, 82, 611, 32, 11542, 25, 4242, 24, 1082, 24, 127, 6, 46, 1065, 21, 5, 4132, 23, 626, 2569, 40, 1496, 140, 23, 28639, 24, 22, 180, 23, 25, 22, 2518, 24, 1131, 11813, 23, 162, 41, 1981, 2507, 30, 6, 15, 22, 207, 24, 1180, 541, 1003, 22, 180, 31, 86, 110, 27, 45, 5644, 27, 134, 3479, 140, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 119 of the training set: {'input_ids': [2, 9445, 827, 345, 1088, 20, 44, 246, 422, 851, 1757, 3023, 3, 27, 1477, 22, 851, 1757, 24, 9445, 827, 32, 975, 26, 22, 391, 643, 24, 990, 43, 1088, 15, 1205, 2021, 9857, 23, 2450, 10, 7847, 144, 81, 805, 29, 1952, 759, 15, 3644, 1088, 5607, 15, 5702, 569, 345, 179, 1088, 167, 18164, 3922, 5488, 63, 412, 1088, 15, 618, 2450, 37, 884, 24, 24416, 25, 1097, 1508, 15, 10354, 324, 10, 6465, 3555, 24, 3922, 40, 28, 9445, 46, 4579, 8754, 6380, 15, 965, 21, 5, 251, 11455, 965, 635, 6, 23, 3039, 25, 757, 3080, 21, 5, 9570, 566, 1272, 114, 15, 65, 10677, 6, 33, 408, 2618, 23, 48, 22, 655, 24, 296, 1153, 21, 5, 2578, 419, 6, 25, 802, 419, 1216, 15, 121, 111, 413, 31, 110, 60, 151, 32, 422, 2190, 1020, 15, 22, 28, 25562, 27, 489, 767, 126, 26, 240, 81, 110, 121, 111, 248, 26, 22, 144, 81, 19, 494, 26, 22, 70, 81, 111, 2766, 216, 127, 32, 965, 21, 5, 85, 64, 52, 15, 2950, 23, 160, 572, 15, 321, 6, 23, 3039, 21, 5, 85, 64, 52, 15, 5474, 6, 25, 757, 3080, 21, 5, 85, 64, 52, 15, 3904, 6, 33, 307, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (7 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:07<00:00,  7.86s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_accuracy': 0.5400000214576721}| Step... (56/392 | Eval metrics: {'accuracy': 0.54}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:46<00:00,  1.22it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.5400000214576721}| Step... (112/392 | Eval metrics: {'accuracy': 0.54}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.69it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6600000262260437}| Step... (168/392 | Eval metrics: {'accuracy': 0.66}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.72it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.7200000286102295}| Step... (224/392 | Eval metrics: {'accuracy': 0.72}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.7400000095367432}| Step... (280/392 | Eval metrics: {'accuracy': 0.74}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.699999988079071}| Step... (336/392 | Eval metrics: {'accuracy': 0.7}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.70it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.699999988079071}| Step... (392/392 | Eval metrics: {'accuracy': 0.7}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.72it/s]\n",
            "Epoch ... 7/7: 100%|██████████████████████████████| 7/7 [02:16<00:00, 19.52s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 8/8 [00:04<00:00,  1.83it/s]\n",
            "INFO:__main__: test results : {'test_accuracy': 0.7440000176429749}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/seqcls/BIOSSES_hf/train.json \\\n",
        "--validation_file data/seqcls/BIOSSES_hf/dev.json \\\n",
        "--test_file data/seqcls/BIOSSES_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 70 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name pearsonr \\\n",
        "--blurb_task BIOSSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4dc0dc8-65d0-485b-ffa7-814230a44284",
        "id": "orbKOLfB9BoF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-6281a72ec163028f\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 849.28it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5558092b12b4ee59.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2060c7815aa60e68.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4adaef11c1a650d2.arrow\n",
            "INFO:__main__:Sample 40 of the training set: {'input_ids': [2, 27, 4025, 22, 1444, 24, 21, 10058, 2334, 30, 26, 19865, 23, 389, 21, 10058, 2334, 30, 33, 817, 21, 5, 10058, 10, 7914, 23, 21, 10058, 10, 17971, 23, 21, 10058, 10, 14384, 23, 25, 21, 10058, 10, 1152, 39, 6, 535, 82, 106, 1820, 27, 45, 1078, 27, 16769, 15, 3, 26, 9038, 16714, 23, 21, 10058, 10, 14384, 73, 74, 384, 27, 2378, 9315, 199, 26, 90, 1494, 25, 230, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2.0}.\n",
            "INFO:__main__:Sample 63 of the training set: {'input_ids': [2, 22, 4855, 10, 352, 796, 21, 5, 23101, 6, 774, 21, 5814, 1296, 8973, 1117, 73, 74, 236, 27, 1715, 10, 14425, 22, 283, 10, 7345, 109, 968, 4753, 10, 39, 3, 708, 463, 26, 158, 534, 25, 393, 20651, 2077, 73, 740, 36, 106, 24, 968, 4753, 10, 39, 35, 28, 750, 5443, 24, 458, 27, 11067, 347, 10, 93, 1326, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2.0}.\n",
            "INFO:__main__:Sample 32 of the training set: {'input_ids': [2, 32, 22, 105, 24, 106, 48, 125, 1297, 24, 859, 23, 98, 33, 263, 32, 2482, 780, 309, 26, 1036, 6143, 21, 23, 7747, 43, 562, 122, 29, 21, 11728, 450, 6446, 17867, 5570, 130, 56, 702, 131, 4905, 21708, 23, 41, 83, 26, 47, 21, 18685, 56, 106, 5279, 53, 6968, 2435, 21, 462, 217, 30, 23, 21836, 756, 76, 889, 23, 25, 400, 293, 3114, 6257, 15, 3, 68, 300, 22, 721, 24, 106, 24, 22, 21, 18685, 56, 10, 910, 98, 957, 26, 57, 48, 125, 1297, 24, 859, 23, 215, 2725, 756, 23, 2725, 1036, 57, 23, 3085, 1602, 3181, 23, 25, 399, 3114, 76, 560, 23, 40, 22, 828, 24, 13789, 30, 98, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 3.6}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (70 epochs) =====\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:07<00:00,  7.53s/it]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5390715949026228}| Step... (8/560 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:31<00:00,  3.95s/it]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.3683514542443945}| Step... (16/560 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.3040766769357789}| Step... (24/560 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.47245389069491983}| Step... (32/560 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.4773969017523775}| Step... (40/560 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5682823691438572}| Step... (48/560 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6006629748316674}| Step... (56/560 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5748225522560781}| Step... (64/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5858260327256719}| Step... (72/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6572607284685041}| Step... (80/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6770346597702684}| Step... (88/560 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.49778913483664133}| Step... (96/560 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.41206794732825125}| Step... (104/560 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.4041653699565293}| Step... (112/560 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.45164134378846477}| Step... (120/560 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5907741478546031}| Step... (128/560 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6233186729066467}| Step... (136/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6167322775192718}| Step... (144/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.3679201961627081}| Step... (152/560 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.38218399563470123}| Step... (160/560 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.4644910008178571}| Step... (168/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5983899759120972}| Step... (176/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6719062785405869}| Step... (184/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.84it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6712292103718787}| Step... (192/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6791100686375429}| Step... (200/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7045454013114248}| Step... (208/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7745743270476361}| Step... (216/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7354129568098551}| Step... (224/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7404668183653876}| Step... (232/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8247254208161154}| Step... (240/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7664187409109919}| Step... (248/560 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7781063372333017}| Step... (256/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7909322144797595}| Step... (264/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7706108228249985}| Step... (272/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7742765782739898}| Step... (280/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7649674470688661}| Step... (288/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7750588985737263}| Step... (296/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7805798882953355}| Step... (304/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7909985416782267}| Step... (312/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7824454812843002}| Step... (320/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7859809077767224}| Step... (328/560 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7833267706319262}| Step... (336/560 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7792788329533582}| Step... (344/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7781935811855512}| Step... (352/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7592342623217935}| Step... (360/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.78it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7622475185222343}| Step... (368/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.83it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7717515887822866}| Step... (376/560 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7880773909562592}| Step... (384/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.790951677295044}| Step... (392/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7892113493586652}| Step... (400/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7773499785265235}| Step... (408/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.76905319987734}| Step... (416/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7438012244754301}| Step... (424/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7444238355295384}| Step... (432/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7714547025053778}| Step... (440/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7752683978347401}| Step... (448/560 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7836924022481213}| Step... (456/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7919742689505952}| Step... (464/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7864101407504943}| Step... (472/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7875272297045378}| Step... (480/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.84it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7806658028066381}| Step... (488/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7767055572511005}| Step... (496/560 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Step... (500/560 | Training Loss: 0.02720199152827263, Learning Rate: 5.446428076538723e-06)\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7753197911820373}| Step... (504/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7717093041715519}| Step... (512/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7701613780511802}| Step... (520/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7709417544455073}| Step... (528/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7720579071650189}| Step... (536/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.80it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7711118232970149}| Step... (544/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.771101782304292}| Step... (552/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7705502760262232}| Step... (560/560 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Epoch ... 70/70: 100%|██████████████████████████| 70/70 [03:43<00:00,  3.19s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 1/1 [00:00<00:00,  1.92it/s]\n",
            "INFO:__main__: test results : {'test_pearsonr': 0.8996540687479233}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/tokcls/BC5CDR-chem_hf/train.json \\\n",
        "--validation_file data/tokcls/BC5CDR-chem_hf/dev.json \\\n",
        "--test_file data/tokcls/BC5CDR-chem_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 6 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC5CDR-chem \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 10000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625f5206-6d74-4b54-b76e-dcca5d014bfd",
        "id": "ImHProWtu89J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-a8f8745a9e3dc8a7\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-a8f8745a9e3dc8a7/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 754.82it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForTokenClassification: {('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids'), ('albert', 'pooler', 'kernel'), ('predictions', 'decoder', 'kernel'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'dense', 'kernel'), ('predictions', 'decoder', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  3.12ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.68ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.86ba/s]\n",
            "INFO:__main__:Sample 408 of the training set: {'input_ids': [2, 68, 395, 28, 266, 24, 22729, 21, 10, 451, 393, 4311, 13736, 26, 28, 5552, 24, 28, 13295, 555, 5828, 2328, 29, 393, 5828, 1122, 213, 1251, 901, 24, 414, 27, 22, 310, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 0, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 1691 of the training set: {'input_ids': [2, 655, 21, 10, 2642, 563, 1086, 1516, 1342, 5444, 48, 22, 164, 21, 10, 2420, 21, 14, 2541, 329, 707, 18755, 1628, 32, 22, 6643, 21, 10, 297, 81, 31, 542, 21, 15, 80, 21, 11, 21, 14, 21, 10, 119, 21, 15, 200, 676, 741, 821, 21, 15, 114, 21, 11, 21, 14, 21, 10, 119, 21, 15, 235, 676, 32, 144, 325, 21, 5, 160, 247, 63, 52, 21, 15, 21, 428, 21, 6, 21, 15, 21, 5, 12565, 7976, 48, 2736, 4981, 21, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, 2, 2, 0, 2, 2, 2, 0, 2, -100, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, -100, 2, -100, 2, 2, -100, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, -100, 2, -100, 2, 2, -100, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 2947 of the training set: {'input_ids': [2, 50, 395, 2161, 22, 179, 1293, 266, 24, 788, 16220, 2853, 25, 575, 103, 29, 9549, 602, 24, 13621, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (6 epochs) =====\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:48<00:00,  1.49it/s]\n",
            "INFO:__main__:Step... (190/1140 | Validation metrics: {'__precision': 0.838783894823336, '__recall': 0.9545539554890593, '__f1': 0.8929321203638908, '__number': 5347, 'overall_precision': 0.838783894823336, 'overall_recall': 0.9545539554890593, 'overall_f1': 0.8929321203638908, 'overall_accuracy': 0.9877738329374303}\n",
            "Training...: 100%|████████████████████████████| 190/190 [03:16<00:00,  1.03s/it]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (380/1140 | Validation metrics: {'__precision': 0.9176751008434176, '__recall': 0.9360389003179352, '__f1': 0.9267660401814647, '__number': 5347, 'overall_precision': 0.9176751008434176, 'overall_recall': 0.9360389003179352, 'overall_f1': 0.9267660401814647, 'overall_accuracy': 0.9901322231020068}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Step... (500/1140 | Training Loss: 0.0006224995013326406, Learning Rate: 2.8114032829762436e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (570/1140 | Validation metrics: {'__precision': 0.9110275689223057, '__recall': 0.951748644099495, '__f1': 0.9309430165553827, '__number': 5347, 'overall_precision': 0.9110275689223057, 'overall_recall': 0.951748644099495, 'overall_f1': 0.9309430165553827, 'overall_accuracy': 0.9913412173379991}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (760/1140 | Validation metrics: {'__precision': 0.9272626318711826, '__recall': 0.9369740041144567, '__f1': 0.932093023255814, '__number': 5347, 'overall_precision': 0.9272626318711826, 'overall_recall': 0.9369740041144567, 'overall_f1': 0.932093023255814, 'overall_accuracy': 0.9918690880607562}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (950/1140 | Validation metrics: {'__precision': 0.9279395503133063, '__recall': 0.9416495230970637, '__f1': 0.9347442680776014, '__number': 5347, 'overall_precision': 0.9279395503133063, 'overall_recall': 0.9416495230970637, 'overall_f1': 0.9347442680776014, 'overall_accuracy': 0.9920819391586422}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:45<00:00,  1.15it/s]\n",
            "Step... (1000/1140 | Training Loss: 2.1626861780532636e-05, Learning Rate: 6.184208359627519e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (1140/1140 | Validation metrics: {'__precision': 0.9296138924810641, '__recall': 0.9410884608191509, '__f1': 0.9353159851301115, '__number': 5347, 'overall_precision': 0.9296138924810641, 'overall_recall': 0.9410884608191509, 'overall_f1': 0.9353159851301115, 'overall_accuracy': 0.9921074812903885}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:45<00:00,  1.14it/s]\n",
            "Epoch ... 6/6: 100%|█████████████████████████████| 6/6 [17:07<00:00, 171.17s/it]\n",
            "Evaluating on Dev Set ...: 72it [00:41,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.9296138924810641, '__recall': 0.9410884608191509, '__f1': 0.9353159851301115, '__number': 5347, 'overall_precision': 0.9296138924810641, 'overall_recall': 0.9410884608191509, 'overall_f1': 0.9353159851301115, 'overall_accuracy': 0.9921074812903885}\n",
            "Evaluating on Test Set...: 75it [00:42,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.914051094890511, '__recall': 0.9301764159702879, '__f1': 0.9220432581684308, '__number': 5385, 'overall_precision': 0.914051094890511, 'overall_recall': 0.9301764159702879, 'overall_f1': 0.9220432581684308, 'overall_accuracy': 0.9914709418837675}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/tokcls/BC5CDR-disease_hf/train.json \\\n",
        "--validation_file data/tokcls/BC5CDR-disease_hf/dev.json \\\n",
        "--test_file data/tokcls/BC5CDR-disease_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 5 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC5-disease \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff323ac-2b59-41fb-e2a6-80896e9bf9d1",
        "id": "XgABnz9Du89K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-741663b27c9f6158\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-741663b27c9f6158/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 776.00it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForTokenClassification: {('predictions', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('albert', 'pooler', 'bias'), ('predictions', 'dense', 'kernel'), ('albert', 'pooler', 'kernel'), ('albert', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-741663b27c9f6158/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4fb3ac47a7add137.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-741663b27c9f6158/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ce20d38c02f55e31.arrow\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.99ba/s]\n",
            "INFO:__main__:Sample 3218 of the training set: {'input_ids': [2, 22, 526, 24, 22, 53, 31, 27, 4025, 22, 522, 25, 1031, 24, 946, 627, 126, 27, 50, 70, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 3894 of the training set: {'input_ids': [2, 330, 968, 4606, 262, 32, 13300, 120, 6316, 24, 22, 161, 21, 13, 21, 30, 1772, 27, 13300, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 2477 of the training set: {'input_ids': [2, 68, 193, 54, 1606, 422, 763, 1276, 24, 21, 26800, 21, 10, 451, 21, 18100, 1548, 14318, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, 2, 0, -100, -100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (5 epochs) =====\n",
            "Step... (500/2850 | Training Loss: 0.0004891881253570318, Learning Rate: 3.2996493246173486e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:48<00:00,  1.48it/s]\n",
            "INFO:__main__:Step... (570/2850 | Validation metrics: {'__precision': 0.759798784321945, '__recall': 0.8537447008949599, '__f1': 0.8040368193412443, '__number': 4246, 'overall_precision': 0.759798784321945, 'overall_recall': 0.8537447008949599, 'overall_f1': 0.8040368193412443, 'overall_accuracy': 0.9814819544839213}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:37<00:00,  2.62it/s]\n",
            "Step... (1000/2850 | Training Loss: 0.0021058940328657627, Learning Rate: 2.5978946723625995e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (1000/2850 | Validation metrics: {'__precision': 0.8446441239063608, '__recall': 0.8412623645784267, '__f1': 0.8429498525073746, '__number': 4246, 'overall_precision': 0.8446441239063608, 'overall_recall': 0.8412623645784267, 'overall_f1': 0.8429498525073746, 'overall_accuracy': 0.9847683754352805}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (1140/2850 | Validation metrics: {'__precision': 0.8039741013619112, '__recall': 0.8480923221855864, '__f1': 0.8254441260744985, '__number': 4246, 'overall_precision': 0.8039741013619112, 'overall_recall': 0.8480923221855864, 'overall_f1': 0.8254441260744985, 'overall_accuracy': 0.9834401845844721}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:50<00:00,  2.48it/s]\n",
            "Step... (1500/2850 | Training Loss: 0.00018523330800235271, Learning Rate: 1.8961402020067908e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (1710/2850 | Validation metrics: {'__precision': 0.8359592215013901, '__recall': 0.8497409326424871, '__f1': 0.842793739780425, '__number': 4246, 'overall_precision': 0.8359592215013901, 'overall_recall': 0.8497409326424871, 'overall_f1': 0.842793739780425, 'overall_accuracy': 0.9851089371918981}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:07<00:00,  3.04it/s]\n",
            "Step... (2000/2850 | Training Loss: 3.191951691405848e-05, Learning Rate: 1.194385731650982e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/2850 | Validation metrics: {'__precision': 0.8431786216596343, '__recall': 0.8471502590673575, '__f1': 0.8451597744360902, '__number': 4246, 'overall_precision': 0.8431786216596343, 'overall_recall': 0.8471502590673575, 'overall_f1': 0.8451597744360902, 'overall_accuracy': 0.9857049202659788}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (2280/2850 | Validation metrics: {'__precision': 0.8492664458116422, '__recall': 0.8452661328308997, '__f1': 0.847261567516525, '__number': 4246, 'overall_precision': 0.8492664458116422, 'overall_recall': 0.8452661328308997, 'overall_f1': 0.847261567516525, 'overall_accuracy': 0.9852281338067141}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:50<00:00,  2.48it/s]\n",
            "Step... (2500/2850 | Training Loss: 1.5043214261822868e-05, Learning Rate: 4.926314431941137e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (2850/2850 | Validation metrics: {'__precision': 0.8510587675469903, '__recall': 0.8424399434762129, '__f1': 0.8467274233637118, '__number': 4246, 'overall_precision': 0.8510587675469903, 'overall_recall': 0.8424399434762129, 'overall_f1': 0.8467274233637118, 'overall_accuracy': 0.9852707040262914}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:07<00:00,  3.05it/s]\n",
            "Epoch ... 5/5: 100%|█████████████████████████████| 5/5 [17:32<00:00, 210.51s/it]\n",
            "Evaluating on Dev Set ...: 72it [00:41,  1.73it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8510587675469903, '__recall': 0.8424399434762129, '__f1': 0.8467274233637118, '__number': 4246, 'overall_precision': 0.8510587675469903, 'overall_recall': 0.8424399434762129, 'overall_f1': 0.8467274233637118, 'overall_accuracy': 0.9852707040262914}\n",
            "Evaluating on Test Set...: 75it [00:43,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8330757341576507, '__recall': 0.8528481012658228, '__f1': 0.842845973416732, '__number': 4424, 'overall_precision': 0.8330757341576507, 'overall_recall': 0.8528481012658228, 'overall_f1': 0.842845973416732, 'overall_accuracy': 0.9850741482965932}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/tokcls/BC2GM_hf/train.json \\\n",
        "--validation_file data/tokcls/BC2GM_hf/dev.json \\\n",
        "--test_file data/tokcls/BC2GM_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--learning_rate 7e-5 \\\n",
        "--num_train_epochs 9 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC2GM \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23866057-2b54-4f41-82cd-04b01045ac9d",
        "id": "3ajg7Jtau89K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1924c477ac5488ad\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-1924c477ac5488ad/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 712.51it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForTokenClassification: {('predictions', 'dense', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'decoder', 'bias'), ('albert', 'pooler', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 13/13 [00:04<00:00,  2.75ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00,  3.20ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 6/6 [00:01<00:00,  3.17ba/s]\n",
            "INFO:__main__:Sample 9164 of the training set: {'input_ids': [2, 622, 356, 6306, 35, 21, 16474, 27, 10300, 30, 37, 134, 788, 21, 10, 3709, 25931, 21, 6366, 21, 10, 41, 89, 21, 10, 21, 12407, 21, 23, 54, 37, 22, 1250, 21, 7059, 21, 10, 21, 173, 30, 21, 10, 21, 12407, 4033, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, 1, 1, 2, 2, -100, 2, 2, -100, 2, 2, 2, 2, -100, 2, 2, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 2, 2, 2, 2, 2, -100, 2, -100, 2, -100, -100, 2, -100, 2, -100, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 10041 of the training set: {'input_ids': [2, 824, 21, 23, 5525, 21, 11434, 8762, 30, 24, 1831, 28, 513, 44, 443, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, -100, 2, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 825 of the training set: {'input_ids': [2, 26, 28, 918, 24, 42, 29, 1241, 6235, 21, 23, 21, 173, 1792, 100, 21, 23, 21, 10597, 13004, 202, 6369, 898, 536, 21, 23, 593, 21044, 871, 21, 23, 1360, 46, 3812, 21, 23, 2898, 21, 23, 68, 59, 300, 21, 20754, 784, 24, 21, 4042, 190, 25, 21, 4042, 205, 3349, 21, 23, 1068, 29, 8447, 182, 21, 23, 3368, 24, 29924, 30, 21, 23, 10190, 156, 24, 5191, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, 2, -100, -100, 2, 2, -100, 2, -100, -100, -100, 2, -100, 2, 2, -100, 2, -100, -100, 2, -100, 2, 2, 2, 2, -100, 2, 2, -100, 2, 2, 2, 2, -100, 2, 2, 0, -100, -100, 2, 0, -100, -100, 2, 2, -100, 2, 2, 2, 2, 2, -100, 2, 2, 2, -100, 2, -100, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (9 epochs) =====\n",
            "Step... (500/4707 | Training Loss: 0.0040063755586743355, Learning Rate: 6.257913628360257e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:30<00:00,  1.33it/s]\n",
            "INFO:__main__:Step... (523/4707 | Validation metrics: {'__precision': 0.7899022801302932, '__recall': 0.7922247631492976, '__f1': 0.7910618169955961, '__number': 3061, 'overall_precision': 0.7899022801302932, 'overall_recall': 0.7922247631492976, 'overall_f1': 0.7910618169955961, 'overall_accuracy': 0.976760226344979}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:34<00:00,  1.33it/s]\n",
            "Step... (1000/4707 | Training Loss: 0.003007482038810849, Learning Rate: 5.514340227819048e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (1000/4707 | Validation metrics: {'__precision': 0.7664777810915598, '__recall': 0.8395949036262659, '__f1': 0.8013719987527284, '__number': 3061, 'overall_precision': 0.7664777810915598, 'overall_recall': 0.8395949036262659, 'overall_f1': 0.8013719987527284, 'overall_accuracy': 0.9769713690492948}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (1046/4707 | Validation metrics: {'__precision': 0.7704221075007592, '__recall': 0.8288141130349559, '__f1': 0.7985520931696569, '__number': 3061, 'overall_precision': 0.7704221075007592, 'overall_recall': 0.8288141130349559, 'overall_f1': 0.7985520931696569, 'overall_accuracy': 0.9774358829987895}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:27<00:00,  1.35it/s]\n",
            "Step... (1500/4707 | Training Loss: 0.0015693254536017776, Learning Rate: 4.770766827277839e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (1569/4707 | Validation metrics: {'__precision': 0.7958865690246183, '__recall': 0.8343678536426005, '__f1': 0.8146730462519937, '__number': 3061, 'overall_precision': 0.7958865690246183, 'overall_recall': 0.8343678536426005, 'overall_f1': 0.8146730462519937, 'overall_accuracy': 0.9789842628304384}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:03<00:00,  1.44it/s]\n",
            "Step... (2000/4707 | Training Loss: 0.0009058315772563219, Learning Rate: 4.027193790534511e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (2000/4707 | Validation metrics: {'__precision': 0.7848418415283432, '__recall': 0.818686703691604, '__f1': 0.8014070994563479, '__number': 3061, 'overall_precision': 0.7848418415283432, 'overall_recall': 0.818686703691604, 'overall_f1': 0.8014070994563479, 'overall_accuracy': 0.9777877875059824}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.70it/s]\n",
            "INFO:__main__:Step... (2092/4707 | Validation metrics: {'__precision': 0.8171193935565382, '__recall': 0.8451486442339105, '__f1': 0.8308977035490606, '__number': 3061, 'overall_precision': 0.8171193935565382, 'overall_recall': 0.8451486442339105, 'overall_f1': 0.8308977035490606, 'overall_accuracy': 0.9789701866501506}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:27<00:00,  1.35it/s]\n",
            "Step... (2500/4707 | Training Loss: 2.389929250057321e-05, Learning Rate: 3.2836203899933025e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (2615/4707 | Validation metrics: {'__precision': 0.8195892575039494, '__recall': 0.8474354786017642, '__f1': 0.8332797944105365, '__number': 3061, 'overall_precision': 0.8195892575039494, 'overall_recall': 0.8474354786017642, 'overall_f1': 0.8332797944105365, 'overall_accuracy': 0.979448776779933}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:04<00:00,  1.44it/s]\n",
            "Step... (3000/4707 | Training Loss: 7.507057307520881e-05, Learning Rate: 2.540046989452094e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (3000/4707 | Validation metrics: {'__precision': 0.8070565797838525, '__recall': 0.8294674942829141, '__f1': 0.8181085870791042, '__number': 3061, 'overall_precision': 0.8070565797838525, 'overall_recall': 0.8294674942829141, 'overall_f1': 0.8181085870791042, 'overall_accuracy': 0.9784775203400805}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (3138/4707 | Validation metrics: {'__precision': 0.8109623170351915, '__recall': 0.850702384841555, '__f1': 0.8303571428571428, '__number': 3061, 'overall_precision': 0.8109623170351915, 'overall_recall': 0.850702384841555, 'overall_f1': 0.8303571428571428, 'overall_accuracy': 0.9795473100419471}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:27<00:00,  1.35it/s]\n",
            "Step... (3500/4707 | Training Loss: 0.00020117023086640984, Learning Rate: 1.7964734070119448e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (3661/4707 | Validation metrics: {'__precision': 0.7998749218261413, '__recall': 0.8356746161385168, '__f1': 0.8173829685253235, '__number': 3061, 'overall_precision': 0.7998749218261413, 'overall_recall': 0.8356746161385168, 'overall_f1': 0.8173829685253235, 'overall_accuracy': 0.9786323583232454}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:03<00:00,  1.44it/s]\n",
            "Step... (4000/4707 | Training Loss: 2.0745863366755657e-05, Learning Rate: 1.0529000064707361e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (4000/4707 | Validation metrics: {'__precision': 0.8147916013788781, '__recall': 0.8493956223456387, '__f1': 0.8317338451695457, '__number': 3061, 'overall_precision': 0.8147916013788781, 'overall_recall': 0.8493956223456387, 'overall_f1': 0.8317338451695457, 'overall_accuracy': 0.9795050815010838}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (4184/4707 | Validation metrics: {'__precision': 0.8137347130761995, '__recall': 0.8477621692257432, '__f1': 0.8304, '__number': 3061, 'overall_precision': 0.8137347130761995, 'overall_recall': 0.8477621692257432, 'overall_f1': 0.8304, 'overall_accuracy': 0.979744376565975}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:26<00:00,  1.35it/s]\n",
            "Step... (4500/4707 | Training Loss: 2.4161197416106006e-06, Learning Rate: 3.0932660592952743e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (4707/4707 | Validation metrics: {'__precision': 0.8170693442108566, '__recall': 0.850702384841555, '__f1': 0.8335467349551857, '__number': 3061, 'overall_precision': 0.8170693442108566, 'overall_recall': 0.850702384841555, 'overall_f1': 0.8335467349551857, 'overall_accuracy': 0.9797303003856873}\n",
            "Training...: 100%|████████████████████████████| 523/523 [06:03<00:00,  1.44it/s]\n",
            "Epoch ... 9/9: 100%|█████████████████████████████| 9/9 [56:38<00:00, 377.61s/it]\n",
            "Evaluating on Dev Set ...: 40it [00:22,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8170693442108566, '__recall': 0.850702384841555, '__f1': 0.8335467349551857, '__number': 3061, 'overall_precision': 0.8170693442108566, 'overall_recall': 0.850702384841555, 'overall_f1': 0.8335467349551857, 'overall_accuracy': 0.9797303003856873}\n",
            "Evaluating on Test Set...: 79it [00:45,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8070255474452555, '__recall': 0.8390513833992095, '__f1': 0.8227269203937679, '__number': 6325, 'overall_precision': 0.8070255474452555, 'overall_recall': 0.8390513833992095, 'overall_f1': 0.8227269203937679, 'overall_accuracy': 0.9792911163001429}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/tokcls/NCBI-disease_hf/train.json \\\n",
        "--validation_file data/tokcls/NCBI-disease_hf/dev.json \\\n",
        "--test_file data/tokcls/NCBI-disease_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--learning_rate 7e-5 \\\n",
        "--num_train_epochs 10 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task NCBI-disease \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c5d16e-fa90-484f-db76-f98a37148e06",
        "id": "H8ritWwLu89K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-4e811e02322508d1\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-4e811e02322508d1/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 753.56it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForTokenClassification: {('albert', 'embeddings', 'position_ids'), ('albert', 'pooler', 'kernel'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'bias'), ('albert', 'pooler', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'kernel'), ('predictions', 'LayerNorm', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 6/6 [00:01<00:00,  3.15ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  3.17ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  3.08ba/s]\n",
            "INFO:__main__:Sample 1123 of the training set: {'input_ids': [2, 22, 277, 2209, 193, 21, 23, 8645, 21, 23, 45, 17399, 27, 47, 5891, 6688, 1384, 1612, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 233 of the training set: {'input_ids': [2, 26, 22, 159, 53, 21, 23, 1382, 785, 26, 22, 160, 9489, 165, 21, 5, 160, 9489, 21, 6, 21, 23, 22, 165, 1261, 32, 160, 2451, 3943, 536, 21, 23, 59, 74, 110, 26, 1750, 24, 123, 21, 10, 21, 16845, 18145, 2626, 448, 29, 21, 19860, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 0, -100, 2, 2, -100, 2, -100, 2, -100, 2, -100, 2, 2, 2, 2, 0, -100, -100, 1, 2, -100, 2, 2, 2, 2, 2, 2, 0, 1, -100, 1, -100, 1, 1, 1, 2, 0, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 3030 of the training set: {'input_ids': [2, 50, 1270, 328, 21, 3742, 17434, 50, 2666, 25, 7187, 14458, 21, 23, 6903, 21, 23, 25, 1052, 785, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (10 epochs) =====\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:15<00:00,  1.05s/it]\n",
            "INFO:__main__:Step... (226/2260 | Validation metrics: {'__precision': 0.7902097902097902, '__recall': 0.8614993646759848, '__f1': 0.8243161094224923, '__number': 787, 'overall_precision': 0.7902097902097902, 'overall_recall': 0.8614993646759848, 'overall_f1': 0.8243161094224923, 'overall_accuracy': 0.9858150110559473}\n",
            "Training...: 100%|████████████████████████████| 226/226 [03:04<00:00,  1.23it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (452/2260 | Validation metrics: {'__precision': 0.8363417569193743, '__recall': 0.8831003811944091, '__f1': 0.8590852904820767, '__number': 787, 'overall_precision': 0.8363417569193743, 'overall_recall': 0.8831003811944091, 'overall_f1': 0.8590852904820767, 'overall_accuracy': 0.9870666277274813}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:35<00:00,  1.45it/s]\n",
            "Step... (500/2260 | Training Loss: 0.0010532749583944678, Learning Rate: 5.4544248996535316e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (678/2260 | Validation metrics: {'__precision': 0.8361650485436893, '__recall': 0.8754764930114358, '__f1': 0.8553693358162632, '__number': 787, 'overall_precision': 0.8361650485436893, 'overall_recall': 0.8754764930114358, 'overall_f1': 0.8553693358162632, 'overall_accuracy': 0.986273937168843}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:36<00:00,  1.44it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (904/2260 | Validation metrics: {'__precision': 0.8391521197007481, '__recall': 0.855146124523507, '__f1': 0.8470736312146004, '__number': 787, 'overall_precision': 0.8391521197007481, 'overall_recall': 0.855146124523507, 'overall_f1': 0.8470736312146004, 'overall_accuracy': 0.9860236138345363}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:36<00:00,  1.45it/s]\n",
            "Step... (1000/2260 | Training Loss: 2.0450086594792083e-05, Learning Rate: 3.905751873389818e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (1000/2260 | Validation metrics: {'__precision': 0.843403205918619, '__recall': 0.8691232528589581, '__f1': 0.8560700876095119, '__number': 787, 'overall_precision': 0.843403205918619, 'overall_recall': 0.8691232528589581, 'overall_f1': 0.8560700876095119, 'overall_accuracy': 0.9866494221703033}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (1130/2260 | Validation metrics: {'__precision': 0.8558786346396966, '__recall': 0.8602287166454892, '__f1': 0.8580481622306717, '__number': 787, 'overall_precision': 0.8558786346396966, 'overall_recall': 0.8602287166454892, 'overall_f1': 0.8580481622306717, 'overall_accuracy': 0.9861487755016897}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:45<00:00,  1.37it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (1356/2260 | Validation metrics: {'__precision': 0.8445273631840796, '__recall': 0.8627700127064803, '__f1': 0.8535512256442488, '__number': 787, 'overall_precision': 0.8445273631840796, 'overall_recall': 0.8627700127064803, 'overall_f1': 0.8535512256442488, 'overall_accuracy': 0.9866494221703033}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:35<00:00,  1.45it/s]\n",
            "Step... (1500/2260 | Training Loss: 3.466901398496702e-05, Learning Rate: 2.3570795747218654e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (1582/2260 | Validation metrics: {'__precision': 0.8555555555555555, '__recall': 0.8805590851334181, '__f1': 0.867877269881027, '__number': 787, 'overall_precision': 0.8555555555555555, 'overall_recall': 0.8805590851334181, 'overall_f1': 0.867877269881027, 'overall_accuracy': 0.9873586716175059}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:36<00:00,  1.45it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (1808/2260 | Validation metrics: {'__precision': 0.8558897243107769, '__recall': 0.8678526048284625, '__f1': 0.8618296529968454, '__number': 787, 'overall_precision': 0.8558897243107769, 'overall_recall': 0.8678526048284625, 'overall_f1': 0.8618296529968454, 'overall_accuracy': 0.9873586716175059}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:35<00:00,  1.45it/s]\n",
            "Step... (2000/2260 | Training Loss: 6.0502516134874895e-06, Learning Rate: 8.084071851044428e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/2260 | Validation metrics: {'__precision': 0.8634085213032582, '__recall': 0.8754764930114358, '__f1': 0.8694006309148264, '__number': 787, 'overall_precision': 0.8634085213032582, 'overall_recall': 0.8754764930114358, 'overall_f1': 0.8694006309148264, 'overall_accuracy': 0.9875255538403771}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (2034/2260 | Validation metrics: {'__precision': 0.86375, '__recall': 0.8780177890724269, '__f1': 0.870825456836799, '__number': 787, 'overall_precision': 0.86375, 'overall_recall': 0.8780177890724269, 'overall_f1': 0.870825456836799, 'overall_accuracy': 0.9876507155075306}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:44<00:00,  1.37it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:09<00:00,  1.64it/s]\n",
            "INFO:__main__:Step... (2260/2260 | Validation metrics: {'__precision': 0.8605230386052304, '__recall': 0.8780177890724269, '__f1': 0.8691823899371068, '__number': 787, 'overall_precision': 0.8605230386052304, 'overall_recall': 0.8780177890724269, 'overall_f1': 0.8691823899371068, 'overall_accuracy': 0.9876924360632484}\n",
            "Training...: 100%|████████████████████████████| 226/226 [02:36<00:00,  1.45it/s]\n",
            "Epoch ... 10/10: 100%|█████████████████████████| 10/10 [26:45<00:00, 160.60s/it]\n",
            "Evaluating on Dev Set ...: 15it [00:08,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8605230386052304, '__recall': 0.8780177890724269, '__f1': 0.8691823899371068, '__number': 787, 'overall_precision': 0.8605230386052304, 'overall_recall': 0.8780177890724269, 'overall_f1': 0.8691823899371068, 'overall_accuracy': 0.9876924360632484}\n",
            "Evaluating on Test Set...: 15it [00:08,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8651115618661258, '__recall': 0.8885416666666667, '__f1': 0.8766700924974307, '__number': 960, 'overall_precision': 0.8651115618661258, 'overall_recall': 0.8885416666666667, 'overall_f1': 0.8766700924974307, 'overall_accuracy': 0.9857533575539862}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/tokcls/JNLPBA_hf/train.json \\\n",
        "--validation_file data/tokcls/JNLPBA_hf/dev.json \\\n",
        "--test_file data/tokcls/JNLPBA_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task JNLPBA \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "id": "3cVQyac4u89K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cf2f61-5670-4dc8-a00b-3d0cb0f801b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1d788b965ff358e0\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-1d788b965ff358e0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 789.19it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForTokenClassification: {('albert', 'pooler', 'bias'), ('predictions', 'dense', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'dense', 'kernel'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'decoder', 'kernel'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'LayerNorm', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-1d788b965ff358e0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4bfacc51accaf978.arrow\n",
            "Running tokenizer on dataset: 100%|███████████████| 2/2 [00:00<00:00,  3.23ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-1d788b965ff358e0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6dbc2e8b76c2df4d.arrow\n",
            "INFO:__main__:Sample 16759 of the training set: {'input_ids': [2, 172, 1624, 160, 4369, 1783, 25, 3863, 9051, 2716, 343, 268, 33, 1964, 27, 22, 451, 1727, 21, 23, 28, 200, 10, 770, 185, 26, 862, 31, 148, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 0, -100, -100, 1, 1, -100, -100, 1, 1, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 7256 of the training set: {'input_ids': [2, 50, 9447, 1464, 27, 14755, 24, 22, 2651, 21, 89, 1034, 76, 5343, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 0, 1, -100, -100, 1, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 8922 of the training set: {'input_ids': [2, 1348, 21, 23, 594, 89, 10, 652, 31, 1263, 27, 3855, 343, 24, 21, 8219, 58, 27, 22, 92, 10, 5529, 594, 85, 819, 1730, 531, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, -100, 0, -100, -100, -100, 2, 2, 2, 2, 2, 2, 0, -100, -100, 2, 2, 2, -100, -100, 2, -100, -100, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (20 epochs) =====\n",
            "Step... (500/14000 | Training Loss: 0.004733615554869175, Learning Rate: 2.8930713597219437e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:23<00:00,  1.20it/s]\n",
            "INFO:__main__:Step... (700/14000 | Validation metrics: {'__precision': 0.7804828150572831, '__recall': 0.8382773016919358, '__f1': 0.8083483419853797, '__number': 4551, 'overall_precision': 0.7804828150572831, 'overall_recall': 0.8382773016919358, 'overall_f1': 0.8083483419853797, 'overall_accuracy': 0.9568698510355871}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:23<00:00,  1.39it/s]\n",
            "Step... (1000/14000 | Training Loss: 0.004574044607579708, Learning Rate: 2.7859285182785243e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (1000/14000 | Validation metrics: {'__precision': 0.8229974160206718, '__recall': 0.8398154251812788, '__f1': 0.831321370309951, '__number': 4551, 'overall_precision': 0.8229974160206718, 'overall_recall': 0.8398154251812788, 'overall_f1': 0.831321370309951, 'overall_accuracy': 0.960746718358231}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (1400/14000 | Validation metrics: {'__precision': 0.8095644748078565, '__recall': 0.8332234673698088, '__f1': 0.8212236058473199, '__number': 4551, 'overall_precision': 0.8095644748078565, 'overall_recall': 0.8332234673698088, 'overall_f1': 0.8212236058473199, 'overall_accuracy': 0.9582815364193759}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (1500/14000 | Training Loss: 0.0023702322505414486, Learning Rate: 2.6787856768351048e-05)\n",
            "Step... (2000/14000 | Training Loss: 0.0033070724457502365, Learning Rate: 2.5716428353916854e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/14000 | Validation metrics: {'__precision': 0.8149100257069408, '__recall': 0.8358602504943968, '__f1': 0.825252196550602, '__number': 4551, 'overall_precision': 0.8149100257069408, 'overall_recall': 0.8358602504943968, 'overall_f1': 0.825252196550602, 'overall_accuracy': 0.958997914076821}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (2100/14000 | Validation metrics: {'__precision': 0.8089643167972149, '__recall': 0.8169633047681828, '__f1': 0.8129441346889691, '__number': 4551, 'overall_precision': 0.8089643167972149, 'overall_recall': 0.8169633047681828, 'overall_f1': 0.8129441346889691, 'overall_accuracy': 0.9567223615178778}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (2500/14000 | Training Loss: 0.0020582445431500673, Learning Rate: 2.464499993948266e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (2800/14000 | Validation metrics: {'__precision': 0.8104389604717187, '__recall': 0.8154251812788398, '__f1': 0.8129244249726177, '__number': 4551, 'overall_precision': 0.8104389604717187, 'overall_recall': 0.8154251812788398, 'overall_f1': 0.8129244249726177, 'overall_accuracy': 0.957185900002107}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (3000/14000 | Training Loss: 0.0015870932256802917, Learning Rate: 2.3573571525048465e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (3000/14000 | Validation metrics: {'__precision': 0.8040669349714044, '__recall': 0.8341023950780049, '__f1': 0.818809318377912, '__number': 4551, 'overall_precision': 0.8040669349714044, 'overall_recall': 0.8341023950780049, 'overall_f1': 0.818809318377912, 'overall_accuracy': 0.9566380817934725}\n",
            "Step... (3500/14000 | Training Loss: 0.0017552312929183245, Learning Rate: 2.2502141291624866e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (3500/14000 | Validation metrics: {'__precision': 0.8181618776047379, '__recall': 0.8196000878927708, '__f1': 0.818880351262349, '__number': 4551, 'overall_precision': 0.8181618776047379, 'overall_recall': 0.8196000878927708, 'overall_f1': 0.818880351262349, 'overall_accuracy': 0.9573965993131203}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (4000/14000 | Training Loss: 0.002615608973428607, Learning Rate: 2.1430712877190672e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (4000/14000 | Validation metrics: {'__precision': 0.8068350668647846, '__recall': 0.8352010547132498, '__f1': 0.8207730511768516, '__number': 4551, 'overall_precision': 0.8068350668647846, 'overall_recall': 0.8352010547132498, 'overall_f1': 0.8207730511768516, 'overall_accuracy': 0.957185900002107}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (4200/14000 | Validation metrics: {'__precision': 0.7990206514796678, '__recall': 0.8246539222148979, '__f1': 0.8116349480968857, '__number': 4551, 'overall_precision': 0.7990206514796678, 'overall_recall': 0.8246539222148979, 'overall_f1': 0.8116349480968857, 'overall_accuracy': 0.955500305514001}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (4500/14000 | Training Loss: 0.0008506887243129313, Learning Rate: 2.035928628174588e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (4900/14000 | Validation metrics: {'__precision': 0.7965990843688685, '__recall': 0.8029004614370469, '__f1': 0.7997373604727511, '__number': 4551, 'overall_precision': 0.7965990843688685, 'overall_recall': 0.8029004614370469, 'overall_f1': 0.7997373604727511, 'overall_accuracy': 0.9547417879943533}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (5000/14000 | Training Loss: 0.00027688697446137667, Learning Rate: 1.9287857867311686e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (5000/14000 | Validation metrics: {'__precision': 0.8092497868712702, '__recall': 0.8343221270050538, '__f1': 0.8215947203288976, '__number': 4551, 'overall_precision': 0.8092497868712702, 'overall_recall': 0.8343221270050538, 'overall_f1': 0.8215947203288976, 'overall_accuracy': 0.9566380817934725}\n",
            "Step... (5500/14000 | Training Loss: 9.092019172385335e-05, Learning Rate: 1.8216429452877492e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (5600/14000 | Validation metrics: {'__precision': 0.8106597893831936, '__recall': 0.8288288288288288, '__f1': 0.8196436332029552, '__number': 4551, 'overall_precision': 0.8106597893831936, 'overall_recall': 0.8288288288288288, 'overall_f1': 0.8196436332029552, 'overall_accuracy': 0.9571016202777017}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (6000/14000 | Training Loss: 4.3565767555264756e-05, Learning Rate: 1.7144999219453894e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (6000/14000 | Validation metrics: {'__precision': 0.8143378407383559, '__recall': 0.8336629312239069, '__f1': 0.8238870792616722, '__number': 4551, 'overall_precision': 0.8143378407383559, 'overall_recall': 0.8336629312239069, 'overall_f1': 0.8238870792616722, 'overall_accuracy': 0.9583658161437812}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (6300/14000 | Validation metrics: {'__precision': 0.8196506550218341, '__recall': 0.8248736541419468, '__f1': 0.8222538604753039, '__number': 4551, 'overall_precision': 0.8196506550218341, 'overall_recall': 0.8248736541419468, 'overall_f1': 0.8222538604753039, 'overall_accuracy': 0.9582815364193759}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (6500/14000 | Training Loss: 0.0006548599922098219, Learning Rate: 1.6073568986030295e-05)\n",
            "Step... (7000/14000 | Training Loss: 0.0010451757116243243, Learning Rate: 1.5002141481090803e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (7000/14000 | Validation metrics: {'__precision': 0.8057754010695187, '__recall': 0.8277301691935838, '__f1': 0.8166052460437893, '__number': 4551, 'overall_precision': 0.8057754010695187, 'overall_recall': 0.8277301691935838, 'overall_f1': 0.8166052460437893, 'overall_accuracy': 0.9564905922757633}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:51<00:00,  1.48it/s]\n",
            "Step... (7500/14000 | Training Loss: 0.0006572651327587664, Learning Rate: 1.393071397615131e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (7700/14000 | Validation metrics: {'__precision': 0.8108225108225108, '__recall': 0.8231157987255548, '__f1': 0.8169229091702106, '__number': 4551, 'overall_precision': 0.8108225108225108, 'overall_recall': 0.8231157987255548, 'overall_f1': 0.8169229091702106, 'overall_accuracy': 0.9577337182107415}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (8000/14000 | Training Loss: 0.00024216242309194058, Learning Rate: 1.2859285561717115e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (8000/14000 | Validation metrics: {'__precision': 0.8211188204683435, '__recall': 0.8321248077345639, '__f1': 0.8265851795263561, '__number': 4551, 'overall_precision': 0.8211188204683435, 'overall_recall': 0.8321248077345639, 'overall_f1': 0.8265851795263561, 'overall_accuracy': 0.958934704283517}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (8400/14000 | Validation metrics: {'__precision': 0.8091735179575941, '__recall': 0.8217974071632608, '__f1': 0.8154366074348631, '__number': 4551, 'overall_precision': 0.8091735179575941, 'overall_recall': 0.8217974071632608, 'overall_f1': 0.8154366074348631, 'overall_accuracy': 0.9568698510355871}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (8500/14000 | Training Loss: 0.0002850846794899553, Learning Rate: 1.1787858056777623e-05)\n",
            "Step... (9000/14000 | Training Loss: 9.986581062548794e-06, Learning Rate: 1.0716427823354024e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (9000/14000 | Validation metrics: {'__precision': 0.817261118450022, '__recall': 0.8156449132058888, '__f1': 0.8164522159903221, '__number': 4551, 'overall_precision': 0.817261118450022, 'overall_recall': 0.8156449132058888, 'overall_f1': 0.8164522159903221, 'overall_accuracy': 0.957059480415499}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (9100/14000 | Validation metrics: {'__precision': 0.8186167899086559, '__recall': 0.8270709734124368, '__f1': 0.822822166356979, '__number': 4551, 'overall_precision': 0.8186167899086559, 'overall_recall': 0.8270709734124368, 'overall_f1': 0.822822166356979, 'overall_accuracy': 0.9579444175217547}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (9500/14000 | Training Loss: 1.0009478501160629e-05, Learning Rate: 9.64499940891983e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (9800/14000 | Validation metrics: {'__precision': 0.8122147359269724, '__recall': 0.8211382113821138, '__f1': 0.8166520979020979, '__number': 4551, 'overall_precision': 0.8122147359269724, 'overall_recall': 0.8211382113821138, 'overall_f1': 0.8166520979020979, 'overall_accuracy': 0.9570173405532963}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (10000/14000 | Training Loss: 3.935683253075695e-06, Learning Rate: 8.573571903980337e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (10000/14000 | Validation metrics: {'__precision': 0.8148710166919575, '__recall': 0.8259723137771918, '__f1': 0.8203841117415975, '__number': 4551, 'overall_precision': 0.8148710166919575, 'overall_recall': 0.8259723137771918, 'overall_f1': 0.8203841117415975, 'overall_accuracy': 0.9577337182107415}\n",
            "Step... (10500/14000 | Training Loss: 1.0218283932772465e-05, Learning Rate: 7.502143944293493e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (10500/14000 | Validation metrics: {'__precision': 0.8195047784535187, '__recall': 0.8290485607558779, '__f1': 0.8242490442381213, '__number': 4551, 'overall_precision': 0.8195047784535187, 'overall_recall': 0.8290485607558779, 'overall_f1': 0.8242490442381213, 'overall_accuracy': 0.958155116832768}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (11000/14000 | Training Loss: 7.136635758797638e-06, Learning Rate: 6.430714165617246e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (11000/14000 | Validation metrics: {'__precision': 0.8181818181818182, '__recall': 0.8345418589321029, '__f1': 0.8262808658762103, '__number': 4551, 'overall_precision': 0.8181818181818182, 'overall_recall': 0.8345418589321029, 'overall_f1': 0.8262808658762103, 'overall_accuracy': 0.9584500958681865}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (11200/14000 | Validation metrics: {'__precision': 0.8160869565217391, '__recall': 0.8248736541419468, '__f1': 0.8204567806797071, '__number': 4551, 'overall_precision': 0.8160869565217391, 'overall_recall': 0.8248736541419468, 'overall_f1': 0.8204567806797071, 'overall_accuracy': 0.9579233475906533}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (11500/14000 | Training Loss: 3.3290380088146776e-06, Learning Rate: 5.359285751183052e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (11900/14000 | Validation metrics: {'__precision': 0.8148788927335641, '__recall': 0.8279499011206328, '__f1': 0.8213623978201636, '__number': 4551, 'overall_precision': 0.8148788927335641, 'overall_recall': 0.8279499011206328, 'overall_f1': 0.8213623978201636, 'overall_accuracy': 0.958091907039464}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:51<00:00,  1.48it/s]\n",
            "Step... (12000/14000 | Training Loss: 2.0232696442690212e-06, Learning Rate: 4.287857791496208e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (12000/14000 | Validation metrics: {'__precision': 0.8160969277369104, '__recall': 0.8288288288288288, '__f1': 0.8224136051455359, '__number': 4551, 'overall_precision': 0.8160969277369104, 'overall_recall': 0.8288288288288288, 'overall_f1': 0.8224136051455359, 'overall_accuracy': 0.9580497671772613}\n",
            "Step... (12500/14000 | Training Loss: 1.449819365006988e-06, Learning Rate: 3.2164280128199607e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (12600/14000 | Validation metrics: {'__precision': 0.8178260869565217, '__recall': 0.8266315095583389, '__f1': 0.8222052234728444, '__number': 4551, 'overall_precision': 0.8178260869565217, 'overall_recall': 0.8266315095583389, 'overall_f1': 0.8222052234728444, 'overall_accuracy': 0.9580076273150587}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:07<00:00,  1.44it/s]\n",
            "Step... (13000/14000 | Training Loss: 1.0563825298959273e-06, Learning Rate: 2.145000053133117e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (13000/14000 | Validation metrics: {'__precision': 0.8174723607197052, '__recall': 0.8286090969017799, '__f1': 0.8230030554343082, '__number': 4551, 'overall_precision': 0.8174723607197052, 'overall_recall': 0.8286090969017799, 'overall_f1': 0.8230030554343082, 'overall_accuracy': 0.9580076273150587}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (13300/14000 | Validation metrics: {'__precision': 0.8183783783783783, '__recall': 0.8316853438804658, '__f1': 0.824978204010462, '__number': 4551, 'overall_precision': 0.8183783783783783, 'overall_recall': 0.8316853438804658, 'overall_f1': 0.824978204010462, 'overall_accuracy': 0.9581761867638693}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (13500/14000 | Training Loss: 1.1337558589730179e-06, Learning Rate: 1.0735719797594356e-06)\n",
            "Step... (14000/14000 | Training Loss: 7.640674084541388e-07, Learning Rate: 2.1439789943400456e-09)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (14000/14000 | Validation metrics: {'__precision': 0.8194414375405932, '__recall': 0.8316853438804658, '__f1': 0.8255179934569247, '__number': 4551, 'overall_precision': 0.8194414375405932, 'overall_recall': 0.8316853438804658, 'overall_f1': 0.8255179934569247, 'overall_accuracy': 0.9581129769705653}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Epoch ... 20/20: 100%|███████████████████████| 20/20 [2:41:14<00:00, 483.71s/it]\n",
            "Evaluating on Dev Set ...: 28it [00:16,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8194414375405932, '__recall': 0.8316853438804658, '__f1': 0.8255179934569247, '__number': 4551, 'overall_precision': 0.8194414375405932, 'overall_recall': 0.8316853438804658, 'overall_f1': 0.8255179934569247, 'overall_accuracy': 0.9581129769705653}\n",
            "Evaluating on Test Set...: 61it [00:35,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.7561706082825571, '__recall': 0.845301316093281, '__f1': 0.7982556554919596, '__number': 8662, 'overall_precision': 0.7561706082825571, 'overall_recall': 0.845301316093281, 'overall_f1': 0.7982556554919596, 'overall_accuracy': 0.9527410207939508}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge \\\n",
        "--train_file data/tokcls/ebmnlp_hf/train.json \\\n",
        "--validation_file data/tokcls/ebmnlp_hf/dev.json \\\n",
        "--test_file data/tokcls/ebmnlp_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 1 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task \"EBM PICO\" \\\n",
        "--max_seq_length 512 \\\n",
        "--return_macro_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9f5119-173f-43e5-fa67-61f456a7a3d3",
        "id": "V3z6vup0u89L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-66c19f981d7e86e9\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-66c19f981d7e86e9/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  5.82it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-INT\",\n",
            "    \"1\": \"B-OUT\",\n",
            "    \"2\": \"B-PAR\",\n",
            "    \"3\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B-INT\": 0,\n",
            "    \"B-OUT\": 1,\n",
            "    \"B-PAR\": 2,\n",
            "    \"O\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge/snapshots/c85649a7b3345b7de438e59c03f8f702e3aebc76/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge were not used when initializing FlaxAlbertForTokenClassification: {('sop_classifier', 'classifier', 'kernel'), ('predictions', 'decoder', 'bias'), ('albert', 'pooler', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'bias'), ('predictions', 'decoder', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('albert', 'pooler', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-66c19f981d7e86e9/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-18d4e807827f9bae.arrow\n",
            "Running tokenizer on dataset: 100%|█████████████| 11/11 [00:03<00:00,  2.81ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-66c19f981d7e86e9/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-de2942dcd646f6df.arrow\n",
            "INFO:__main__:Sample 26860 of the training set: {'input_ids': [2, 78, 33, 315, 40, 6917, 27, 2108, 105, 25, 956, 10, 5222, 31, 955, 41, 5476, 956, 329, 138, 266, 2490, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100, -100, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 6137 of the training set: {'input_ids': [2, 1291, 121, 413, 31, 148, 60, 42, 297, 29, 22728, 25, 177, 297, 29, 18966, 26, 1015, 24, 522, 24, 1549, 295, 21, 5, 604, 15, 56, 21, 7, 741, 685, 15, 67, 21, 7, 21, 6, 25, 2258, 17251, 932, 21, 5, 911, 15, 67, 21, 7, 741, 1255, 15, 117, 21, 7, 21, 6, 25, 113, 492, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 3, 3, 3, 3, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 2, -100, 2, -100, -100, 2, -100, 2, 2, -100, -100, 2, -100, 2, -100, 2, 1, -100, 1, 2, -100, 2, -100, -100, 2, -100, 2, 2, -100, -100, 2, -100, 2, -100, 2, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 6746 of the training set: {'input_ids': [2, 69, 78, 233, 36, 655, 10, 375, 10, 3713, 1241, 29825, 84, 1246, 22, 370, 7487, 20374, 24, 90, 1097, 25, 2210, 5023, 26, 22, 926, 900, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 3, 3, 0, -100, -100, -100, -100, 0, -100, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (1 epochs) =====\n",
            "Step... (500/2558 | Training Loss: 0.015418271534144878, Learning Rate: 1.6098514606710523e-05)\n",
            "Step... (1000/2558 | Training Loss: 0.016306480392813683, Learning Rate: 1.2189208973722998e-05)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [01:40<00:00,  1.62it/s]\n",
            "INFO:__main__:Step... (1000/2558 | Validation metrics: {'macro_precision': 0.7695895657977547, 'macro_recall': 0.6574050535082493, 'macro_f1': 0.7076946391738123}\n",
            "Step... (1500/2558 | Training Loss: 0.018208464607596397, Learning Rate: 8.279906069219578e-06)\n",
            "Step... (2000/2558 | Training Loss: 0.015892788767814636, Learning Rate: 4.370600890979404e-06)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [01:33<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/2558 | Validation metrics: {'macro_precision': 0.7558948659810943, 'macro_recall': 0.6892147275237664, 'macro_f1': 0.7202851451024057}\n",
            "Step... (2500/2558 | Training Loss: 0.029592927545309067, Learning Rate: 4.61297020137863e-07)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [01:33<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2558/2558 | Validation metrics: {'macro_precision': 0.7649577019631004, 'macro_recall': 0.6807679360157604, 'macro_f1': 0.7201931095615589}\n",
            "Training...: 100%|██████████████████████████| 2558/2558 [25:29<00:00,  1.67it/s]\n",
            "Epoch ... 1/1: 100%|████████████████████████████| 1/1 [25:29<00:00, 1529.69s/it]\n",
            "Evaluating on Dev Set ...: 163it [01:33,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'macro_precision': 0.7649577019631004, 'macro_recall': 0.6807679360157604, 'macro_f1': 0.7201931095615589}\n",
            "Evaluating on Test Set...: 33it [00:19,  1.72it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'macro_precision': 0.7536411665920224, 'macro_recall': 0.7400436281323629, 'macro_f1': 0.7350768124425245}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BioM-ALBERT-PMC**"
      ],
      "metadata": {
        "id": "L1kpBFervxMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/seqcls/chemprot_hf/train.json \\\n",
        "--validation_file data/seqcls/chemprot_hf/dev.json \\\n",
        "--test_file data/seqcls/chemprot_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 5 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task ChemProt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bf9d15-0d52-41c7-ad34-a6e4782dc081",
        "id": "4uhjEqFIvxM5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-7d7c32e2c7c8f019\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-7d7c32e2c7c8f019/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  4.24it/s]\n",
            "['0', 'CPR:3', 'CPR:4', 'CPR:5', 'CPR:6', 'CPR:9']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForSequenceClassification: {('sop_classifier', 'classifier', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'decoder', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  6.88ba/s]\n",
            "100%|███████████████████████████████████████████| 12/12 [00:01<00:00,  7.35ba/s]\n",
            "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  7.58ba/s]\n",
            "INFO:__main__:Sample 16977 of the training set: {'input_ids': [2, 21, 12024, 4625, 21747, 21, 8, 5, 67, 210, 6, 10, 14909, 23, 12481, 10, 12024, 10, 301, 10, 4625, 21747, 23, 306, 5, 56, 6, 4625, 21747, 9, 35, 136, 24, 400, 19476, 24, 21, 6388, 4899, 7056, 2702, 30, 21, 5, 6388, 3567, 7056, 6, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 7577 of the training set: {'input_ids': [2, 22, 126, 24, 12948, 21, 5, 1705, 385, 160, 1344, 774, 6, 23, 21, 6113, 3882, 14938, 1117, 21, 5, 85, 1344, 39, 774, 6, 23, 21, 15107, 2334, 21, 5, 85, 1344, 38, 774, 6, 25, 22, 21, 6388, 3567, 7056, 952, 21, 6388, 4899, 7056, 25, 21, 130, 778, 837, 33, 2840, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "INFO:__main__:Sample 7288 of the training set: {'input_ids': [2, 27, 336, 360, 306, 5, 38, 6, 30, 3122, 21, 13998, 989, 26, 14894, 57, 23, 22, 57, 33, 297, 29, 306, 5, 38, 6, 30, 1815, 310, 23, 1305, 4745, 27927, 21, 5, 6388, 4899, 7056, 6, 23, 21, 5, 214, 23, 552, 25, 654, 968, 1124, 6, 23, 36, 718, 26, 28, 111, 185, 26, 21, 6388, 3567, 7056, 298, 25, 106, 24, 160, 5973, 10, 131, 25, 21, 89, 1034, 39, 10, 210, 26, 14894, 57, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (5 epochs) =====\n",
            "Step... (500/11270 | Training Loss: 0.17572644352912903, Learning Rate: 1.9114462702418678e-05)\n",
            "Step... (1000/11270 | Training Loss: 0.2762899398803711, Learning Rate: 1.822715057642199e-05)\n",
            "Step... (1500/11270 | Training Loss: 0.5067679286003113, Learning Rate: 1.7339840269414708e-05)\n",
            "Step... (2000/11270 | Training Loss: 0.6063804030418396, Learning Rate: 1.645252814341802e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:53<00:00,  3.31it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.8163353500432152, 'eval_recall': 0.784468438538206, 'eval_F1': 0.8000847098686998}| Step... (2254/11270 | Eval metrics: {'accuracy': 0.9198615548455804}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [07:58<00:00,  4.71it/s]\n",
            "Step... (2500/11270 | Training Loss: 0.034615762531757355, Learning Rate: 1.5565216017421335e-05)\n",
            "Step... (3000/11270 | Training Loss: 0.06844218075275421, Learning Rate: 1.4677905710414052e-05)\n",
            "Step... (3500/11270 | Training Loss: 0.02864319086074829, Learning Rate: 1.3790593584417365e-05)\n",
            "Step... (4000/11270 | Training Loss: 0.14987994730472565, Learning Rate: 1.290328236791538e-05)\n",
            "Step... (4500/11270 | Training Loss: 0.04167671129107475, Learning Rate: 1.2015972060908098e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:46<00:00,  3.77it/s]\n",
            "INFO:__main__:{'eval_precision': 0.787842190016103, 'eval_recall': 0.8127076411960132, 'eval_F1': 0.8000817661488143}| Step... (4508/11270 | Eval metrics: {'accuracy': 0.9161341853035144}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [07:26<00:00,  5.05it/s]\n",
            "Step... (5000/11270 | Training Loss: 0.00237896665930748, Learning Rate: 1.112865902541671e-05)\n",
            "Step... (5500/11270 | Training Loss: 0.00306134601123631, Learning Rate: 1.0241348718409427e-05)\n",
            "Step... (6000/11270 | Training Loss: 0.10731575638055801, Learning Rate: 9.35403659241274e-06)\n",
            "Step... (6500/11270 | Training Loss: 0.10543332993984222, Learning Rate: 8.466725375910755e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:46<00:00,  3.78it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7972693421597021, 'eval_recall': 0.800249169435216, 'eval_F1': 0.7987564766839379}| Step... (6762/11270 | Eval metrics: {'accuracy': 0.9180866169684061}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [07:25<00:00,  5.06it/s]\n",
            "Step... (7000/11270 | Training Loss: 0.239152193069458, Learning Rate: 7.579413249914069e-06)\n",
            "Step... (7500/11270 | Training Loss: 0.005797129124403, Learning Rate: 6.692102942906786e-06)\n",
            "Step... (8000/11270 | Training Loss: 0.002355384873226285, Learning Rate: 5.80479127165745e-06)\n",
            "Step... (8500/11270 | Training Loss: 0.0027721093501895666, Learning Rate: 4.9174796004081145e-06)\n",
            "Step... (9000/11270 | Training Loss: 0.029162071645259857, Learning Rate: 4.030167929158779e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:46<00:00,  3.78it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7950260730044123, 'eval_recall': 0.8230897009966778, 'eval_F1': 0.8088145276474189}| Step... (9016/11270 | Eval metrics: {'accuracy': 0.9198615548455804}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [07:26<00:00,  5.05it/s]\n",
            "Step... (9500/11270 | Training Loss: 0.0005484981811605394, Learning Rate: 3.1428562579094432e-06)\n",
            "Step... (10000/11270 | Training Loss: 0.0003965520591009408, Learning Rate: 2.2555445866601076e-06)\n",
            "Step... (10500/11270 | Training Loss: 0.0011529745534062386, Learning Rate: 1.368234165965987e-06)\n",
            "Step... (11000/11270 | Training Loss: 0.0003515491262078285, Learning Rate: 4.809224378732324e-07)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:46<00:00,  3.78it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8153717627401837, 'eval_recall': 0.8106312292358804, 'eval_F1': 0.8129945855893378}| Step... (11270/11270 | Eval metrics: {'accuracy': 0.9232339368122116}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [07:26<00:00,  5.05it/s]\n",
            "Epoch ... 5/5: 100%|█████████████████████████████| 5/5 [37:43<00:00, 452.61s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████| 247/247 [01:05<00:00,  3.79it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8331242158092849, 'test_recall': 0.7743440233236152, 'test_F1': 0.8026594137201571}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC  \\\n",
        "--train_file data/seqcls/DDI_hf/train.json \\\n",
        "--validation_file data/seqcls/DDI_hf/dev.json \\\n",
        "--test_file data/seqcls/DDI_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 5 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task DDI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcd497f-2e25-4560-c61a-b08f8c33e726",
        "id": "ymvlAmdOvxM6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-0ecf918e6c735628\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "\r  0%|                                                     | 0/3 [00:00<?, ?it/s]\r100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 738.09it/s]\r\n",
            "['0', 'DDI-advise', 'DDI-effect', 'DDI-int', 'DDI-mechanism']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForSequenceClassification: {('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'decoder', 'bias'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'decoder', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4831f2fdbb75e282.arrow\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  9.09ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f4ca70d38eb2b24d.arrow\n",
            "INFO:__main__:Sample 7525 of the training set: {'input_ids': [2, 2760, 14606, 7397, 9446, 79, 3105, 29, 22, 340, 2959, 20, 14380, 21, 5, 14025, 10144, 6, 23, 2323, 162, 41, 13300, 25, 8990, 23, 13528, 30, 21, 5, 8789, 1495, 202, 23, 16301, 23, 21, 6388, 4928, 7056, 23, 2084, 17897, 197, 23, 27, 3116, 2163, 23, 21, 21046, 2594, 15347, 6, 23, 6393, 30, 21, 5, 190, 5275, 10, 85, 4132, 23, 21, 89, 7865, 5918, 23, 21, 21418, 89, 3016, 6, 23, 16491, 21, 5, 5018, 1417, 228, 6, 23, 21, 4779, 18781, 21, 5, 131, 3142, 9486, 10, 30, 6, 23, 10697, 21, 5, 2858, 5918, 23, 8020, 8775, 6, 23, 306, 3442, 616, 1876, 41, 3472, 952, 21, 5, 1376, 3388, 1310, 23, 21, 11143, 7265, 8830, 23, 32, 375, 620, 1310, 23, 26, 6585, 1310, 23, 21, 9662, 130, 3676, 23, 1823, 6585, 23, 11447, 131, 8146, 6, 23, 3896, 21, 5, 131, 9652, 5596, 23, 21, 4777, 17163, 23, 21, 6388, 4928, 7056, 6, 23, 9917, 2908, 24653, 23, 13268, 21, 5, 3028, 15250, 6, 23, 21, 19606, 2193, 202, 21, 5, 19606, 4824, 202, 6, 23, 21, 1327, 15, 21, 3356, 23722, 30, 21, 26572, 23, 21, 2319, 205, 14308, 23, 12948, 21, 5, 1080, 290, 10, 18724, 6, 23, 25, 1573, 788, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 23859 of the training set: {'input_ids': [2, 21, 6388, 4928, 7056, 20, 41, 29, 291, 116, 25427, 23, 4246, 602, 24, 9605, 29, 21, 6388, 4928, 7056, 79, 848, 27, 939, 316, 304, 24, 12948, 25, 10474, 24, 134, 3699, 1243, 10, 2480, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 15264 of the training set: {'input_ids': [2, 79, 3105, 29, 22, 340, 20, 21, 6388, 4928, 7056, 21, 5, 18374, 8248, 79, 1683, 18398, 922, 3500, 25, 1338, 22, 14573, 21, 17972, 30, 43, 2007, 8495, 6, 25, 2353, 290, 5642, 2305, 23, 24516, 23, 21, 14432, 26806, 30, 23, 25, 21, 6388, 4928, 7056, 21, 5, 18374, 8248, 79, 185, 22, 135, 24, 506, 979, 6, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (5 epochs) =====\n",
            "Step... (500/5270 | Training Loss: 0.27188387513160706, Learning Rate: 3.621252108132467e-05)\n",
            "Step... (1000/5270 | Training Loss: 0.03134925663471222, Learning Rate: 3.241745798732154e-05)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:16<00:00,  2.31it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.9004424778761062, 'eval_recall': 0.8678038379530917, 'eval_F1': 0.8838219326818676}| Step... (1054/5270 | Eval metrics: {'accuracy': 0.9603365384615384}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [06:55<00:00,  2.54it/s]\n",
            "Step... (1500/5270 | Training Loss: 0.045417506247758865, Learning Rate: 2.8622391255339608e-05)\n",
            "Step... (2000/5270 | Training Loss: 0.01188949029892683, Learning Rate: 2.482732270436827e-05)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:10<00:00,  3.68it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8316831683168316, 'eval_recall': 0.8955223880597015, 'eval_F1': 0.8624229979466119}| Step... (2108/5270 | Eval metrics: {'accuracy': 0.9515224358974359}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [06:22<00:00,  2.76it/s]\n",
            "Step... (2500/5270 | Training Loss: 0.008598790504038334, Learning Rate: 2.1032255972386338e-05)\n",
            "Step... (3000/5270 | Training Loss: 0.0023807394318282604, Learning Rate: 1.7237191059393808e-05)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:10<00:00,  3.68it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8464646464646465, 'eval_recall': 0.8933901918976546, 'eval_F1': 0.8692946058091285}| Step... (3162/5270 | Eval metrics: {'accuracy': 0.9555288461538461}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [06:23<00:00,  2.75it/s]\n",
            "Step... (3500/5270 | Training Loss: 0.016418583691120148, Learning Rate: 1.3442125236906577e-05)\n",
            "Step... (4000/5270 | Training Loss: 0.0007623096462339163, Learning Rate: 9.647059414419346e-06)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:10<00:00,  3.66it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8658280922431866, 'eval_recall': 0.8805970149253731, 'eval_F1': 0.8731501057082452}| Step... (4216/5270 | Eval metrics: {'accuracy': 0.9571314102564102}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [06:22<00:00,  2.75it/s]\n",
            "Step... (4500/5270 | Training Loss: 0.03092886507511139, Learning Rate: 5.851990863448009e-06)\n",
            "Step... (5000/5270 | Training Loss: 0.0003668995632324368, Learning Rate: 2.056925268334453e-06)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:10<00:00,  3.67it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8609958506224067, 'eval_recall': 0.8848614072494669, 'eval_F1': 0.8727655099894847}| Step... (5270/5270 | Eval metrics: {'accuracy': 0.9583333333333334}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [06:22<00:00,  2.76it/s]\n",
            "Epoch ... 5/5: 100%|█████████████████████████████| 5/5 [32:27<00:00, 389.41s/it]\n",
            "Evaluating on Test Data ...: 100%|██████████████| 90/90 [00:23<00:00,  3.78it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8437821171634121, 'test_recall': 0.8386108273748724, 'test_F1': 0.8411885245901638}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/seqcls/GAD_hf/train.json \\\n",
        "--validation_file data/seqcls/GAD_hf/dev.json \\\n",
        "--test_file data/seqcls/GAD_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 8 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task GAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70ac181-6bc0-4546-d22a-b7c56b713f85",
        "id": "Ywh5plRsvxM6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-5058072c4f44a661\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-5058072c4f44a661/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 48.40it/s]\n",
            "['0', '1']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'LayerNorm', 'bias'), ('predictions', 'decoder', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'dense', 'kernel'), ('sop_classifier', 'classifier', 'bias'), ('albert', 'embeddings', 'position_ids'), ('sop_classifier', 'classifier', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00,  9.01ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.19ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.60ba/s]\n",
            "INFO:__main__:Sample 3972 of the training set: {'input_ids': [2, 50, 53, 761, 36, 22, 21, 6388, 3567, 7056, 165, 890, 54, 836, 28, 341, 181, 41, 28, 21, 6388, 14970, 7056, 1741, 165, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "INFO:__main__:Sample 2918 of the training set: {'input_ids': [2, 68, 1428, 36, 22, 21, 6388, 3567, 7056, 10, 38, 3716, 4023, 2282, 637, 45, 103, 29, 947, 7242, 21, 6388, 14970, 7056, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 3406 of the training set: {'input_ids': [2, 26, 3569, 23, 26, 50, 338, 17628, 577, 24, 662, 1519, 634, 23, 466, 1016, 26, 22, 21, 6388, 3567, 7056, 31, 54, 103, 29, 21, 6388, 14970, 7056, 390, 46, 29, 4011, 23, 1455, 23, 25, 1231, 598, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (8 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:08<00:00,  1.00it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.6738095238095239, 'eval_recall': 0.9658703071672355, 'eval_F1': 0.7938288920056101}| Step... (177/1416 | Eval metrics: {'accuracy': 0.7252336448598131}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:34<00:00,  1.88it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7441176470588236, 'eval_recall': 0.863481228668942, 'eval_F1': 0.7993680884676145}| Step... (354/1416 | Eval metrics: {'accuracy': 0.7626168224299066}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.72it/s]\n",
            "Step... (500/1416 | Training Loss: 0.3721851706504822, Learning Rate: 2.5903953428496607e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8203389830508474, 'eval_recall': 0.825938566552901, 'eval_F1': 0.8231292517006802}| Step... (531/1416 | Eval metrics: {'accuracy': 0.805607476635514}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.72it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8135048231511254, 'eval_recall': 0.863481228668942, 'eval_F1': 0.837748344370861}| Step... (708/1416 | Eval metrics: {'accuracy': 0.8168224299065421}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8445229681978799, 'eval_recall': 0.8156996587030717, 'eval_F1': 0.829861111111111}| Step... (885/1416 | Eval metrics: {'accuracy': 0.8168224299065421}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.71it/s]\n",
            "Step... (1000/1416 | Training Loss: 0.014742712490260601, Learning Rate: 1.1779660781030543e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8311258278145696, 'eval_recall': 0.856655290102389, 'eval_F1': 0.8436974789915966}| Step... (1062/1416 | Eval metrics: {'accuracy': 0.8261682242990654}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.72it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.37it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8056426332288401, 'eval_recall': 0.8771331058020477, 'eval_F1': 0.8398692810457515}| Step... (1239/1416 | Eval metrics: {'accuracy': 0.8168224299065421}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:04<00:00,  2.72it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8178807947019867, 'eval_recall': 0.8430034129692833, 'eval_F1': 0.8302521008403362}| Step... (1416/1416 | Eval metrics: {'accuracy': 0.811214953271028}) \n",
            "Training...: 100%|████████████████████████████| 177/177 [01:05<00:00,  2.72it/s]\n",
            "Epoch ... 8/8: 100%|██████████████████████████████| 8/8 [09:10<00:00, 68.79s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 9/9 [00:02<00:00,  3.81it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8233333333333334, 'test_recall': 0.8790035587188612, 'test_F1': 0.8502581755593802}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC  \\\n",
        "--train_file data/seqcls/bioasq_hf/train.json \\\n",
        "--validation_file data/seqcls/bioasq_hf/dev.json \\\n",
        "--test_file data/seqcls/bioasq_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--blurb_task BioASQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42897d0-55ed-4f29-b910-4268470db1f4",
        "id": "vF3EMjzQvxM6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-25f663af987ecd04\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-25f663af987ecd04/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 25.30it/s]\n",
            "['no', 'yes']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'LayerNorm', 'kernel'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'decoder', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.23ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.82ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.07ba/s]\n",
            "INFO:__main__:Sample 198 of the training set: {'input_ids': [2, 35, 12274, 9726, 407, 26, 76, 3672, 3023, 3, 26, 50, 53, 23, 68, 110, 36, 7721, 451, 12274, 9726, 26, 1096, 25, 226, 57, 23, 48, 304, 36, 728, 76, 3672, 75, 54, 751, 21, 5, 214, 5, 10, 95, 6, 27, 152, 5, 10, 117, 6, 968, 23, 32, 200, 901, 6, 15, 982, 24, 12274, 9726, 608, 29, 514, 24, 3672, 75, 54, 751, 315, 40, 28, 852, 24, 508, 25, 2154, 2651, 5826, 5968, 21, 5, 26245, 25, 21, 1602, 89, 819, 1017, 6, 25, 226, 76, 889, 1087, 25, 2072, 27, 7721, 21, 5, 39, 131, 117, 25, 80, 131, 117, 160, 15284, 1090, 6, 15, 68, 110, 36, 2483, 24, 22, 12274, 13225, 21, 4634, 6227, 38, 102, 76, 3672, 25, 253, 76, 176, 27, 22, 283, 10, 24922, 104, 24, 7721, 15, 22834, 23, 22, 21, 4634, 6227, 38, 774, 6348, 8996, 202, 253, 76, 3672, 25, 9676, 22, 283, 10, 24922, 104, 24, 7721, 15, 22, 1267, 129, 31, 328, 9676, 37, 22, 286, 24, 22, 306, 22131, 67, 774, 14491, 5089, 28, 15, 76, 3672, 25, 1744, 2462, 1539, 6997, 21, 5, 10557, 6, 9726, 14, 1344, 24361, 23, 28, 359, 2314, 37, 1211, 41, 306, 22131, 67, 23, 28, 341, 2868, 2082, 10, 12931, 13225, 15, 903, 7952, 38, 25, 306, 22131, 67, 25746, 26, 22, 21, 16865, 12161, 85, 22195, 24, 21, 14419, 57, 23, 1084, 27, 724, 12274, 26215, 25, 764, 3672, 15, 50, 332, 3947, 22, 2881, 1444, 24, 12274, 9726, 26, 432, 658, 835, 23, 2416, 43, 76, 3672, 23, 76, 977, 850, 46, 76, 859, 27, 1150, 6656, 25, 3776, 15, 227, 78, 476, 36, 1678, 85, 5449, 14, 85, 551, 3557, 27, 306, 22131, 67, 21, 5, 22453, 13225, 235, 6, 23, 47, 503, 1261, 32, 12274, 26215, 15, 16335, 23, 68, 307, 36, 22, 732, 624, 24, 69, 88, 268, 718, 26, 22, 514, 24, 22, 13225, 104, 24, 306, 22131, 67, 15, 5849, 23, 68, 307, 36, 23, 4578, 27, 116, 306, 22131, 67, 952, 23, 1678, 85, 5449, 14, 85, 551, 3015, 22, 6997, 1343, 37, 2800, 22, 218, 2175, 24, 22, 6997, 1672, 7994, 25, 86, 2284, 76, 3672, 41, 307, 37, 127, 21, 14932, 3911, 747, 15, 21, 12, 6935, 12708, 23, 12, 61, 2846, 498, 10, 12931, 26215, 26, 2294, 57, 15, 68, 348, 270, 36, 1174, 21, 1783, 3918, 13225, 235, 21, 5, 388, 22131, 67, 6, 35, 22, 1150, 533, 24, 21, 6935, 12708, 15, 21, 6935, 12708, 70, 231, 54, 842, 22, 1125, 24, 8875, 75, 231, 477, 76, 3672, 15, 317, 86, 233, 36, 364, 854, 36, 3519, 1710, 306, 22131, 67, 10, 699, 498, 10, 12931, 26215, 23, 28, 179, 2139, 24, 61, 35, 21, 6935, 12708, 23, 637, 59, 516, 1220, 41, 283, 11465, 25, 19774, 808, 15, 14514, 23, 2483, 24, 306, 22131, 67, 3788, 11927, 76, 1813, 23, 3206, 22, 6156, 36, 306, 22131, 67, 10, 699, 26215, 3654, 6997, 10, 352, 76, 3672, 15, 306, 22131, 67, 35, 28, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "INFO:__main__:Sample 150 of the training set: {'input_ids': [2, 35, 22, 165, 14114, 697, 56, 4266, 888, 2314, 26, 3746, 57, 3023, 3, 246, 44, 400, 267, 40, 1609, 200, 1329, 1058, 456, 215, 14114, 697, 56, 213, 21, 128, 56, 1337, 56, 6189, 15, 862, 24, 69, 267, 35, 1587, 12561, 37, 4266, 3058, 24, 50, 3824, 456, 25, 86, 1173, 24, 2010, 10859, 30, 27, 22, 3099, 3153, 15, 708, 122, 348, 17274, 270, 36, 28, 5944, 3695, 7445, 165, 26, 22, 21, 128, 56, 1337, 56, 3099, 23, 14114, 697, 56, 23, 35, 603, 26, 22, 153, 9765, 25, 760, 4266, 888, 21, 17748, 26, 3746, 670, 15, 22, 153, 1156, 10, 13994, 290, 5217, 5944, 3567, 14114, 697, 56, 35, 603, 26, 22, 5784, 25, 4266, 888, 12561, 26, 3746, 670, 15, 21, 11129, 2930, 30, 5804, 8610, 14352, 8919, 21, 5, 799, 3335, 128, 6, 35, 28, 2013, 5199, 8919, 557, 37, 264, 4266, 7650, 24, 22, 21, 128, 56, 1337, 56, 3952, 20996, 6189, 25, 5628, 106, 24, 14114, 697, 56, 23, 28, 5944, 3567, 2168, 28, 9765, 862, 238, 4667, 26, 240, 3099, 15, 69, 230, 16117, 204, 4266, 25, 14114, 697, 56, 106, 7446, 758, 26, 42, 25, 444, 23, 170, 23, 215, 92, 14114, 697, 56, 106, 107, 26, 22, 9765, 23, 21, 5, 202, 11966, 6, 4266, 7650, 26, 3746, 252, 23, 25, 2320, 3335, 128, 10, 385, 21, 8293, 130, 7039, 14114, 697, 56, 106, 26, 6108, 447, 2825, 103, 29, 21, 128, 56, 1337, 56, 3446, 2821, 15, 14114, 697, 56, 23, 28, 5944, 3567, 1782, 26, 22, 21, 128, 56, 1337, 56, 6189, 23, 35, 4218, 4266, 888, 21, 17748, 26, 3746, 57, 15, 26, 475, 27, 144, 2133, 447, 25, 147, 116, 3746, 670, 23, 1208, 10, 4186, 14114, 697, 56, 4513, 25, 109, 35, 603, 48, 1063, 3599, 107, 26, 153, 5784, 23, 147, 591, 26, 22, 3771, 10, 1217, 57, 15, 451, 11041, 21, 5, 4150, 30, 6, 57, 86, 2890, 1208, 10, 4186, 14114, 697, 56, 25, 859, 24, 144, 21, 4150, 30, 57, 27, 3085, 1602, 3181, 8543, 106, 24, 1208, 10, 4186, 14114, 697, 56, 23, 418, 106, 24, 1208, 10, 4186, 14114, 697, 56, 14540, 26, 3114, 2320, 3335, 128, 21, 4150, 30, 57, 15, 1068, 23, 69, 248, 476, 36, 1208, 10, 4186, 14114, 697, 56, 35, 4218, 603, 48, 313, 1870, 1297, 25, 35, 2197, 26, 147, 3746, 670, 15, 708, 122, 348, 17274, 270, 36, 28, 5944, 3695, 7445, 165, 26, 22, 21, 128, 56, 1337, 56, 3099, 23, 14114, 697, 56, 23, 35, 603, 26, 22, 153, 9765, 25, 760, 4266, 888, 21, 17748, 26, 3746, 670, 15, 14114, 697, 56, 23, 28, 5944, 3567, 1782, 26, 22, 21, 128, 56, 1337, 56, 6189, 23, 35, 4218, 4266, 888, 21, 17748, 26, 3746, 57, 15, 14114, 697, 56, 23, 28, 5944, 3567, 1782, 26, 22, 21, 128, 56, 1337, 56, 6189, 23, 35, 4218, 4266, 888, 21, 17748, 26, 3746, 57, 15, 708, 122, 348, 17274, 270, 36, 28, 5944, 3695, 7445, 165, 26, 22, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "INFO:__main__:Sample 339 of the training set: {'input_ids': [2, 44, 2112, 13670, 1558, 103, 29, 22, 1591, 24, 1098, 410, 5571, 3023, 3, 1416, 23, 68, 1596, 22, 270, 36, 788, 5968, 44, 690, 24, 22, 1578, 165, 1370, 2162, 21, 5, 7379, 4736, 6, 36, 19089, 1108, 1098, 410, 5571, 15, 22, 341, 1098, 151, 13687, 1049, 58, 438, 3430, 132, 6974, 15, 68, 1773, 36, 22, 5068, 10, 8796, 6395, 220, 37, 788, 5968, 13687, 96, 22, 21, 12, 898, 10, 7405, 3873, 12, 24, 1370, 719, 36, 698, 96, 293, 1098, 1591, 15, 6592, 23, 125, 1098, 151, 23, 29, 125, 1578, 21, 7379, 4736, 23, 2053, 1108, 2707, 24, 788, 5968, 15, 281, 27, 22, 1089, 1125, 24, 1098, 410, 5571, 23, 69, 1578, 1370, 1013, 59, 74, 9180, 26, 2038, 254, 994, 27351, 1191, 26, 125, 1098, 151, 15, 2112, 13670, 1558, 25, 22, 1591, 24, 1098, 410, 5571, 15, 2112, 13670, 1558, 25, 22, 1591, 24, 1098, 410, 5571, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (20 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:08<00:00,  4.09s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_accuracy': 0.8266666531562805}| Step... (83/1660 | Eval metrics: {'accuracy': 0.8266666666666667}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:52<00:00,  1.58it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9466666579246521}| Step... (166/1660 | Eval metrics: {'accuracy': 0.9466666666666667}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (249/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8933333158493042}| Step... (332/1660 | Eval metrics: {'accuracy': 0.8933333333333333}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.65it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9333333373069763}| Step... (415/1660 | Eval metrics: {'accuracy': 0.9333333333333333}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (498/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n",
            "Step... (500/1660 | Training Loss: 0.006755476351827383, Learning Rate: 1.3987951206217986e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9066666960716248}| Step... (581/1660 | Eval metrics: {'accuracy': 0.9066666666666666}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9066666960716248}| Step... (664/1660 | Eval metrics: {'accuracy': 0.9066666666666666}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.65it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9066666960716248}| Step... (747/1660 | Eval metrics: {'accuracy': 0.9066666666666666}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (830/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (913/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (996/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Step... (1000/1660 | Training Loss: 0.0014597238041460514, Learning Rate: 7.963854841364082e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1079/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1162/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1245/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1328/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1411/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1494/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n",
            "Step... (1500/1660 | Training Loss: 0.001648022560402751, Learning Rate: 1.9397591586312046e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1577/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.9200000166893005}| Step... (1660/1660 | Eval metrics: {'accuracy': 0.92}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n",
            "Epoch ... 20/20: 100%|██████████████████████████| 20/20 [08:01<00:00, 24.07s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 3/3 [00:01<00:00,  1.85it/s]\n",
            "INFO:__main__: test results : {'test_accuracy': 0.949999988079071}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/seqcls/pubmedqa_hf/train.json \\\n",
        "--validation_file data/seqcls/pubmedqa_hf/dev.json \\\n",
        "--test_file data/seqcls/pubmedqa_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 8 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--blurb_task PubMedQA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8661f0b-a60a-41ff-9e1f-f6ece67b3fae",
        "id": "pPoc9xcFvxM6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-d26b3028a530c846\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-d26b3028a530c846/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 35.55it/s]\n",
            "['maybe', 'no', 'yes']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'decoder', 'kernel'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'dense', 'kernel'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'dense', 'bias'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.87ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.43ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.91ba/s]\n",
            "INFO:__main__:Sample 358 of the training set: {'input_ids': [2, 3140, 24, 22, 3531, 1678, 7713, 3304, 26, 28, 338, 844, 10, 20218, 905, 24, 42, 297, 32, 555, 76, 756, 20, 44, 328, 2766, 949, 3023, 3, 28, 203, 19101, 24, 22, 1678, 7713, 31, 1075, 2637, 36, 2461, 3058, 32, 22, 4946, 180, 24, 995, 1732, 15, 1652, 23, 1678, 38, 1732, 33, 687, 17648, 139, 1678, 38, 131, 25, 1678, 38, 251, 21, 5, 1536, 228, 21, 9732, 1110, 331, 1049, 214, 1110, 6, 23, 600, 29, 555, 2105, 1189, 46, 1994, 26957, 1231, 1189, 33, 1876, 41, 1678, 49, 131, 1732, 23, 25, 177, 29, 3646, 1189, 33, 1876, 41, 1678, 56, 1732, 15, 227, 526, 31, 27, 4587, 22, 1075, 2637, 19101, 24, 22, 1678, 7713, 4946, 180, 32, 277, 226, 1637, 26, 995, 138, 15, 227, 5699, 1369, 53, 2217, 24, 4144, 1443, 42, 297, 26, 562, 3595, 21, 12564, 2309, 2673, 15, 42, 629, 366, 1963, 46, 1360, 9128, 15, 28767, 25, 5553, 21, 18860, 692, 423, 3611, 138, 10, 385, 308, 21, 5, 217, 30, 30, 6, 66, 311, 15, 26, 22, 53, 23, 558, 1731, 42, 21, 5, 1058, 15, 58, 7, 6, 33, 1876, 41, 21, 5973, 39, 131, 23, 542, 819, 21, 5, 981, 7, 6, 41, 21, 5973, 39, 251, 23, 2482, 93, 21, 5, 95, 7, 6, 41, 21, 5973, 38, 131, 23, 21, 11135, 21, 5, 49, 7, 6, 41, 21, 5973, 38, 251, 23, 152, 1623, 21, 5, 487, 7, 6, 41, 21, 5973, 49, 131, 23, 12461, 21, 5, 38, 7, 6, 41, 21, 5973, 49, 251, 23, 1394, 21, 5, 65, 15, 58, 7, 6, 41, 21, 5973, 49, 217, 23, 25, 21, 7740, 21, 5, 56, 7, 6, 41, 21, 5973, 56, 15, 48, 28, 916, 396, 10, 243, 24, 2051, 5570, 23, 3949, 67, 21, 5, 496, 7, 6, 87, 2097, 24, 100, 15, 26, 28767, 105, 23, 42, 29, 21, 5973, 38, 251, 25, 21, 5973, 49, 131, 600, 87, 265, 788, 30, 30, 23, 41, 231, 42, 29, 21, 5973, 49, 217, 25, 21, 5973, 56, 600, 15, 16335, 23, 90, 21, 5973, 49, 131, 25, 21, 5973, 49, 251, 1297, 314, 42, 29, 2961, 430, 15, 26, 5553, 105, 23, 22, 433, 1637, 24, 22, 277, 226, 31, 28, 3703, 806, 2723, 24, 788, 30, 30, 21, 5, 85, 32, 2205, 572, 15, 1365, 6, 15, 494, 23, 22, 687, 9652, 347, 4176, 24, 21, 5973, 39, 600, 231, 54, 6457, 47, 806, 1398, 181, 15, 22, 341, 2493, 24, 22, 53, 44, 1369, 805, 23, 1082, 24, 743, 4167, 332, 23, 25, 22, 364, 208, 24, 42, 314, 26, 291, 687, 1937, 30, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "INFO:__main__:Sample 175 of the training set: {'input_ids': [2, 573, 24, 22, 21, 18843, 375, 1296, 5432, 27788, 26, 22, 2406, 2619, 43, 22, 5445, 6878, 24, 21, 5867, 13004, 20, 35, 773, 28, 5443, 238, 3023, 3, 22, 2130, 295, 37, 28242, 81, 21, 251, 35, 136, 24, 22, 767, 1061, 24, 2130, 14753, 3169, 10, 21086, 156, 15, 32, 50, 3821, 28, 792, 35, 705, 27, 240, 2406, 2619, 26, 540, 27, 1328, 134, 257, 23, 25, 594, 137, 31, 22, 266, 23, 27, 4937, 47, 1986, 70, 96, 9170, 15, 22, 526, 24, 50, 53, 31, 27, 6250, 22, 573, 24, 50, 28242, 26, 22, 2406, 232, 43, 21, 5867, 13004, 23, 41, 143, 41, 22, 261, 834, 27, 773, 25, 145, 15, 28, 4733, 653, 10, 1608, 53, 1425, 26, 22, 626, 587, 5583, 2129, 43, 21, 5867, 13004, 15, 22, 577, 35, 1345, 43, 1133, 65, 232, 20, 821, 56, 44, 43, 21, 5419, 7814, 773, 21, 5, 1622, 15, 49, 7, 6, 23, 3491, 44, 43, 21, 2058, 13953, 2309, 773, 21, 5, 840, 15, 49, 7, 6, 25, 558, 232, 43, 1190, 1375, 21, 5, 67, 15, 56, 7, 6, 15, 86, 137, 35, 406, 36, 3949, 24, 781, 44, 659, 132, 1949, 46, 247, 21, 5, 981, 15, 1485, 7, 6, 23, 21, 10873, 44, 60, 1394, 25, 2095, 132, 1949, 21, 5, 1568, 15, 1307, 7, 6, 25, 2464, 44, 1317, 132, 1949, 46, 91, 21, 5, 496, 15, 1917, 7, 6, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "INFO:__main__:Sample 269 of the training set: {'input_ids': [2, 890, 2811, 28650, 311, 632, 403, 24, 471, 26, 42, 1793, 7341, 10, 19103, 5014, 100, 35, 143, 992, 29, 461, 206, 3023, 3, 90, 461, 206, 25, 2811, 28650, 311, 59, 74, 384, 27, 632, 403, 24, 471, 26, 7341, 10, 19103, 5014, 100, 15, 882, 42, 29, 951, 2248, 144, 46, 884, 126, 40, 461, 206, 637, 45, 1418, 27, 59, 530, 403, 24, 471, 66, 311, 23, 227, 526, 31, 27, 336, 23, 32, 22, 179, 127, 23, 360, 42, 1793, 383, 44, 143, 992, 40, 461, 206, 75, 167, 16310, 27, 2995, 311, 21, 5, 5188, 3782, 6, 703, 961, 530, 403, 24, 471, 15, 1369, 105, 24, 227, 161, 1958, 21, 5, 17905, 10, 16205, 23, 21, 89, 5402, 567, 6, 220, 833, 42, 167, 629, 2811, 28650, 311, 32, 22, 4363, 24, 161, 3782, 15, 88, 7808, 403, 10, 1198, 10, 2480, 3536, 21, 5, 14103, 537, 1585, 25, 2150, 579, 143, 10, 3938, 635, 6, 25, 28, 21, 2200, 2248, 1410, 21, 5, 2200, 2248, 6948, 1272, 6, 33, 1314, 6530, 23, 255, 40, 461, 206, 23, 25, 235, 225, 66, 311, 15, 8626, 10, 3254, 42, 1314, 112, 175, 3536, 48, 90, 127, 2025, 20, 1695, 1135, 23, 1382, 1136, 19, 258, 145, 2051, 21, 5, 496, 10, 1680, 6, 132, 15, 1805, 762, 255, 40, 461, 206, 33, 99, 530, 66, 311, 20, 726, 537, 1585, 916, 631, 2580, 762, 2680, 15, 65, 25, 2968, 15, 65, 21, 5, 85, 545, 15, 14661, 6, 25, 1232, 2580, 762, 2972, 15, 65, 25, 2845, 15, 65, 21, 5, 85, 545, 15, 12918, 6, 19, 2150, 579, 143, 10, 3938, 916, 189, 762, 3949, 15, 65, 25, 1218, 15, 65, 21, 5, 85, 545, 15, 1365, 6, 19, 21, 2200, 2248, 6948, 1272, 916, 189, 762, 119, 15, 567, 25, 80, 15, 1521, 21, 5, 85, 545, 15, 2836, 93, 6, 25, 5014, 762, 119, 15, 438, 25, 80, 15, 2023, 21, 5, 85, 572, 15, 1365, 6, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (8 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:07<00:00,  7.40s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_accuracy': 0.5600000023841858}| Step... (56/448 | Eval metrics: {'accuracy': 0.56}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:45<00:00,  1.24it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.5400000214576721}| Step... (112/448 | Eval metrics: {'accuracy': 0.54}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:14<00:00,  3.74it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.5600000023841858}| Step... (168/448 | Eval metrics: {'accuracy': 0.56}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.72it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.7400000095367432}| Step... (224/448 | Eval metrics: {'accuracy': 0.74}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.71it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.800000011920929}| Step... (280/448 | Eval metrics: {'accuracy': 0.8}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.64it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8399999737739563}| Step... (336/448 | Eval metrics: {'accuracy': 0.84}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.69it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8199999928474426}| Step... (392/448 | Eval metrics: {'accuracy': 0.82}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.70it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8199999928474426}| Step... (448/448 | Eval metrics: {'accuracy': 0.82}) \n",
            "Training...: 100%|██████████████████████████████| 56/56 [00:15<00:00,  3.69it/s]\n",
            "Epoch ... 8/8: 100%|██████████████████████████████| 8/8 [02:31<00:00, 18.89s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 8/8 [00:04<00:00,  1.84it/s]\n",
            "INFO:__main__: test results : {'test_accuracy': 0.7540000081062317}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/seqcls/BIOSSES_hf/train.json \\\n",
        "--validation_file data/seqcls/BIOSSES_hf/dev.json \\\n",
        "--test_file data/seqcls/BIOSSES_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 60 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name pearsonr \\\n",
        "--blurb_task BIOSSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWDy7q_gAouM",
        "outputId": "a45c230c-377b-41e7-b52b-c53d5f36fac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-6281a72ec163028f\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 670.30it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForSequenceClassification: {('predictions', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-391ec4ac1ec4bae0.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-41d4d4de5bc3f8b4.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7fc1e75abc818c42.arrow\n",
            "INFO:__main__:Sample 59 of the training set: {'input_ids': [2, 175, 1667, 23, 21, 6681, 7870, 23, 21, 10058, 1352, 131, 23, 25, 533, 9355, 21, 23, 33, 83, 27, 1466, 22, 1524, 24, 21, 10058, 10, 837, 15, 3, 22, 267, 36, 264, 119, 10, 770, 46, 91, 33, 328, 3050, 32, 443, 21, 10058, 10, 1326, 38, 14, 49, 533, 561, 77, 28, 724, 3304, 24, 22, 533, 9355, 1942, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2.4}.\n",
            "INFO:__main__:Sample 20 of the training set: {'input_ids': [2, 317, 220, 36, 291, 267, 407, 26, 21, 8193, 10, 333, 674, 838, 33, 11804, 37, 903, 3908, 38, 26, 3588, 4645, 1129, 75, 54, 1083, 10, 565, 226, 57, 15, 3, 22, 3431, 725, 523, 952, 1304, 32, 88, 24, 22, 838, 2314, 37, 903, 3908, 38, 27, 2108, 230, 29, 3588, 4645, 10, 4229, 21, 4736, 4753, 217, 30, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2.0}.\n",
            "INFO:__main__:Sample 44 of the training set: {'input_ids': [2, 26, 6456, 24, 69, 20177, 1151, 30, 23, 22, 78, 24, 327, 1517, 28, 502, 204, 6288, 26, 22, 319, 10, 8635, 20367, 27, 2104, 6600, 210, 490, 138, 3, 68, 292, 2647, 22, 98, 24, 327, 41, 47, 13726, 75, 293, 1523, 26, 22, 319, 359, 24, 310, 2900, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0.8}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (60 epochs) =====\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:07<00:00,  7.73s/it]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5539812616135348}| Step... (8/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:30<00:00,  3.87s/it]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.47505781609865866}| Step... (16/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5273304685946982}| Step... (24/480 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5097932094901001}| Step... (32/480 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5057668709472212}| Step... (40/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5215447991719907}| Step... (48/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.85it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.49063777555528737}| Step... (56/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.84it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5530940172191314}| Step... (64/480 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6170276846602756}| Step... (72/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6413878987983725}| Step... (80/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7073439519484581}| Step... (88/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7636514191409975}| Step... (96/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7984601370872415}| Step... (104/480 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8125485694310421}| Step... (112/480 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8109970800787976}| Step... (120/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8675436651356679}| Step... (128/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8376215414830311}| Step... (136/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8121924430201906}| Step... (144/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8602840092768061}| Step... (152/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8490215653877464}| Step... (160/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8528754102977423}| Step... (168/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8124981618410901}| Step... (176/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8071944753006695}| Step... (184/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.84it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8192101736261932}| Step... (192/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8363406190788499}| Step... (200/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8243726313133406}| Step... (208/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8406616674599003}| Step... (216/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8295210955766095}| Step... (224/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8022517124606277}| Step... (232/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.807787456903556}| Step... (240/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7811191965877339}| Step... (248/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7895718483557772}| Step... (256/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7805655803249804}| Step... (264/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.808354675311058}| Step... (272/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.85it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8270854414785078}| Step... (280/480 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8620119240723888}| Step... (288/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.85it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8541850607870819}| Step... (296/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8335611360090041}| Step... (304/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.87it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8549839557636617}| Step... (312/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8459729497151517}| Step... (320/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8376604328634254}| Step... (328/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8380529336339229}| Step... (336/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8506169603745137}| Step... (344/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.849298290693644}| Step... (352/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8423632640823632}| Step... (360/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8386780389492908}| Step... (368/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8296281501962504}| Step... (376/480 | Eval metrics: {'accuracy': 0.3125}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8204291106928826}| Step... (384/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8007622060014927}| Step... (392/480 | Eval metrics: {'accuracy': 0.375}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7980429485970222}| Step... (400/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8061743277887274}| Step... (408/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.8045183574177281}| Step... (416/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7983204990685162}| Step... (424/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7859550345983011}| Step... (432/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7893212219237371}| Step... (440/480 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7954753327032299}| Step... (448/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7945652683792364}| Step... (456/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.83it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7951930545502469}| Step... (464/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7887380361101226}| Step... (472/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7914717089419686}| Step... (480/480 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 8/8 [00:02<00:00,  2.89it/s]\n",
            "Epoch ... 60/60: 100%|██████████████████████████| 60/60 [03:14<00:00,  3.25s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 1/1 [00:00<00:00,  1.93it/s]\n",
            "INFO:__main__: test results : {'test_pearsonr': 0.9444129653363287}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/tokcls/BC5CDR-chem_hf/train.json \\\n",
        "--validation_file data/tokcls/BC5CDR-chem_hf/dev.json \\\n",
        "--test_file data/tokcls/BC5CDR-chem_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 6 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC5CDR-chem \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 10000000"
      ],
      "metadata": {
        "id": "Dhc4oZbUCwCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f17b901-20d2-4b47-dbd6-8b2201355476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-a8f8745a9e3dc8a7\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-a8f8745a9e3dc8a7/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 13.07it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForTokenClassification: {('sop_classifier', 'classifier', 'bias'), ('albert', 'pooler', 'bias'), ('predictions', 'dense', 'bias'), ('predictions', 'bias'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'decoder', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'decoder', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'dense', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  3.06ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.68ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.81ba/s]\n",
            "INFO:__main__:Sample 3913 of the training set: {'input_ids': [2, 86, 1018, 33, 136, 266, 24, 379, 24, 22, 2105, 21, 23, 61, 499, 70, 21, 23, 25, 88, 157, 24, 6683, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 4113 of the training set: {'input_ids': [2, 78, 824, 1833, 233, 121, 320, 21, 10, 176, 129, 32, 7721, 980, 1242, 2288, 241, 21, 14, 968, 38, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 0, 2, 2, 2, 2, 2, -100, 2, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 1594 of the training set: {'input_ids': [2, 22, 409, 24, 22133, 21, 10, 103, 7435, 44, 10694, 25, 79, 45, 281, 27, 47, 185, 26, 4247, 3697, 390, 340, 16806, 774, 3868, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 0, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (6 epochs) =====\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:48<00:00,  1.48it/s]\n",
            "INFO:__main__:Step... (190/1140 | Validation metrics: {'__precision': 0.9108483483483484, '__recall': 0.9076117449036843, '__f1': 0.9092271662763467, '__number': 5347, 'overall_precision': 0.9108483483483484, 'overall_recall': 0.9076117449036843, 'overall_f1': 0.9092271662763467, 'overall_accuracy': 0.989961942223698}\n",
            "Training...: 100%|████████████████████████████| 190/190 [03:17<00:00,  1.04s/it]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (380/1140 | Validation metrics: {'__precision': 0.8956924172303311, '__recall': 0.9410884608191509, '__f1': 0.917829457364341, '__number': 5347, 'overall_precision': 0.8956924172303311, 'overall_recall': 0.9410884608191509, 'overall_f1': 0.917829457364341, 'overall_accuracy': 0.9900641107506832}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Step... (500/1140 | Training Loss: 0.0009216428734362125, Learning Rate: 2.8114032829762436e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:42<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (570/1140 | Validation metrics: {'__precision': 0.894307501330023, '__recall': 0.943145689171498, '__f1': 0.9180775532495903, '__number': 5347, 'overall_precision': 0.894307501330023, 'overall_recall': 0.943145689171498, 'overall_f1': 0.9180775532495903, 'overall_accuracy': 0.9908984870543962}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (760/1140 | Validation metrics: {'__precision': 0.922082717872969, '__recall': 0.9339816719655882, '__f1': 0.9279940537024993, '__number': 5347, 'overall_precision': 0.922082717872969, 'overall_recall': 0.9339816719655882, 'overall_f1': 0.9279940537024993, 'overall_accuracy': 0.9916051526993777}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:42<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (950/1140 | Validation metrics: {'__precision': 0.9223869668680212, '__recall': 0.9423976061342809, '__f1': 0.9322849213691027, '__number': 5347, 'overall_precision': 0.9223869668680212, 'overall_recall': 0.9423976061342809, 'overall_f1': 0.9322849213691027, 'overall_accuracy': 0.9918350318850945}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Step... (1000/1140 | Training Loss: 8.830522710923105e-05, Learning Rate: 6.184208359627519e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.71it/s]\n",
            "INFO:__main__:Step... (1140/1140 | Validation metrics: {'__precision': 0.9233455882352941, '__recall': 0.9394052739854124, '__f1': 0.9313062019097061, '__number': 5347, 'overall_precision': 0.9233455882352941, 'overall_recall': 0.9394052739854124, 'overall_f1': 0.9313062019097061, 'overall_accuracy': 0.9917243493141937}\n",
            "Training...: 100%|████████████████████████████| 190/190 [02:46<00:00,  1.14it/s]\n",
            "Epoch ... 6/6: 100%|█████████████████████████████| 6/6 [17:10<00:00, 171.69s/it]\n",
            "Evaluating on Dev Set ...: 72it [00:41,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.9233455882352941, '__recall': 0.9394052739854124, '__f1': 0.9313062019097061, '__number': 5347, 'overall_precision': 0.9233455882352941, 'overall_recall': 0.9394052739854124, 'overall_f1': 0.9313062019097061, 'overall_accuracy': 0.9917243493141937}\n",
            "Evaluating on Test Set...: 75it [00:43,  1.73it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.9159032612678637, '__recall': 0.9283194057567317, '__f1': 0.9220695379507516, '__number': 5385, 'overall_precision': 0.9159032612678637, 'overall_recall': 0.9283194057567317, 'overall_f1': 0.9220695379507516, 'overall_accuracy': 0.9915270541082164}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/tokcls/BC5CDR-disease_hf/train.json \\\n",
        "--validation_file data/tokcls/BC5CDR-disease_hf/dev.json \\\n",
        "--test_file data/tokcls/BC5CDR-disease_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 8e-5 \\\n",
        "--num_train_epochs 5 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC5-disease \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "id": "6_bdiIL5CwCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4fc087d-4056-4692-f477-1595b08a5a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-741663b27c9f6158\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-741663b27c9f6158/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 24.10it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForTokenClassification: {('predictions', 'decoder', 'kernel'), ('predictions', 'decoder', 'bias'), ('predictions', 'dense', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'bias'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('albert', 'pooler', 'bias'), ('predictions', 'dense', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  3.09ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.72ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.78ba/s]\n",
            "INFO:__main__:Sample 4055 of the training set: {'input_ids': [2, 21, 620, 89, 251, 31, 1070, 4579, 41, 28, 659, 21, 10, 241, 21, 14, 968, 38, 726, 1895, 21, 5, 21, 196, 21, 15, 21, 620, 21, 15, 21, 6, 1628, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, -100, 2, 2, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 2, -100, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 1621 of the training set: {'input_ids': [2, 319, 21, 10, 21, 357, 396, 21, 10, 511, 24, 25506, 555, 1146, 26, 224, 297, 32, 1347, 4809, 600, 21, 20, 47, 3496, 6810, 24, 1740, 7583, 395, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, -100, 2, -100, 2, 2, -100, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 1, 1, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 1304 of the training set: {'input_ids': [2, 22, 78, 460, 22, 204, 198, 24, 903, 14366, 1946, 388, 38, 41, 28, 516, 1157, 368, 4154, 12496, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, 2, 2, 2, 2, 2, 0, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (5 epochs) =====\n",
            "Step... (500/2850 | Training Loss: 0.0018616762245073915, Learning Rate: 6.599298649234697e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:48<00:00,  1.47it/s]\n",
            "INFO:__main__:Step... (570/2850 | Validation metrics: {'__precision': 0.7538719129342821, '__recall': 0.8483278379651437, '__f1': 0.7983156028368794, '__number': 4246, 'overall_precision': 0.7538719129342821, 'overall_recall': 0.8483278379651437, 'overall_f1': 0.7983156028368794, 'overall_accuracy': 0.9800515951061276}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:39<00:00,  2.60it/s]\n",
            "Step... (1000/2850 | Training Loss: 0.002552902325987816, Learning Rate: 5.195789344725199e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (1000/2850 | Validation metrics: {'__precision': 0.86417273412001, '__recall': 0.8106453132359868, '__f1': 0.8365536517195286, '__number': 4246, 'overall_precision': 0.86417273412001, 'overall_recall': 0.8106453132359868, 'overall_f1': 0.8365536517195286, 'overall_accuracy': 0.984138336185538}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (1140/2850 | Validation metrics: {'__precision': 0.814663951120163, '__recall': 0.8478568064060292, '__f1': 0.830929024812464, '__number': 4246, 'overall_precision': 0.814663951120163, 'overall_recall': 0.8478568064060292, 'overall_f1': 0.830929024812464, 'overall_accuracy': 0.9847428333035342}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:50<00:00,  2.47it/s]\n",
            "Step... (1500/2850 | Training Loss: 0.0005757310427725315, Learning Rate: 3.7922804040135816e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (1710/2850 | Validation metrics: {'__precision': 0.8243777622702955, '__recall': 0.8346679227508244, '__f1': 0.8294909303686367, '__number': 4246, 'overall_precision': 0.8243777622702955, 'overall_recall': 0.8346679227508244, 'overall_f1': 0.8294909303686367, 'overall_accuracy': 0.98359343737495}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:07<00:00,  3.03it/s]\n",
            "Step... (2000/2850 | Training Loss: 4.45404184574727e-05, Learning Rate: 2.388771463301964e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/2850 | Validation metrics: {'__precision': 0.8326812428078251, '__recall': 0.8520960904380593, '__f1': 0.84227680130369, '__number': 4246, 'overall_precision': 0.8326812428078251, 'overall_recall': 0.8520960904380593, 'overall_f1': 0.84227680130369, 'overall_accuracy': 0.9847513473474496}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (2280/2850 | Validation metrics: {'__precision': 0.8461357763683345, '__recall': 0.8483278379651437, '__f1': 0.8472303892743737, '__number': 4246, 'overall_precision': 0.8461357763683345, 'overall_recall': 0.8483278379651437, 'overall_f1': 0.8472303892743737, 'overall_accuracy': 0.9851855635871369}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:50<00:00,  2.47it/s]\n",
            "Step... (2500/2850 | Training Loss: 1.37488023028709e-05, Learning Rate: 9.852628863882273e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:41<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (2850/2850 | Validation metrics: {'__precision': 0.8461538461538461, '__recall': 0.8497409326424871, '__f1': 0.8479435957696827, '__number': 4246, 'overall_precision': 0.8461538461538461, 'overall_recall': 0.8497409326424871, 'overall_f1': 0.8479435957696827, 'overall_accuracy': 0.9851770495432215}\n",
            "Training...: 100%|████████████████████████████| 570/570 [03:06<00:00,  3.05it/s]\n",
            "Epoch ... 5/5: 100%|█████████████████████████████| 5/5 [17:35<00:00, 211.11s/it]\n",
            "Evaluating on Dev Set ...: 72it [00:41,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8461538461538461, '__recall': 0.8497409326424871, '__f1': 0.8479435957696827, '__number': 4246, 'overall_precision': 0.8461538461538461, 'overall_recall': 0.8497409326424871, 'overall_f1': 0.8479435957696827, 'overall_accuracy': 0.9851770495432215}\n",
            "Evaluating on Test Set...: 75it [00:43,  1.73it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8288700195950359, '__recall': 0.8605334538878843, '__f1': 0.8444050127536875, '__number': 4424, 'overall_precision': 0.8288700195950359, 'overall_recall': 0.8605334538878843, 'overall_f1': 0.8444050127536875, 'overall_accuracy': 0.9850501002004008}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/tokcls/BC2GM_hf/train.json \\\n",
        "--validation_file data/tokcls/BC2GM_hf/dev.json \\\n",
        "--test_file data/tokcls/BC2GM_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 6 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC2GM \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "id": "HtyzIj_8CwCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bcdc39a-5e1e-4f0d-d1e2-fb9b0662bef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1924c477ac5488ad\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-1924c477ac5488ad/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  8.14it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForTokenClassification: {('predictions', 'LayerNorm', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('albert', 'pooler', 'kernel'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'bias'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'decoder', 'kernel'), ('predictions', 'dense', 'bias'), ('predictions', 'dense', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 13/13 [00:04<00:00,  2.72ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00,  3.17ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 6/6 [00:01<00:00,  3.07ba/s]\n",
            "INFO:__main__:Sample 11694 of the training set: {'input_ids': [2, 28, 477, 24, 22, 10046, 104, 14977, 29, 22, 113, 843, 24, 22, 964, 7073, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 9998 of the training set: {'input_ids': [2, 21, 130, 38, 799, 104, 35, 2314, 26, 690, 37, 22, 16188, 554, 24, 226, 3965, 268, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 10954 of the training set: {'input_ids': [2, 22, 8664, 801, 201, 571, 1001, 4244, 21, 7, 3280, 27, 22, 153, 658, 21, 22755, 571, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, -100, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (6 epochs) =====\n",
            "Step... (500/4710 | Training Loss: 0.002334491116926074, Learning Rate: 4.470275962376036e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:29<00:00,  1.34it/s]\n",
            "INFO:__main__:Step... (785/4710 | Validation metrics: {'__precision': 0.7797638046600702, '__recall': 0.7981051943809213, '__f1': 0.7888278979657734, '__number': 3061, 'overall_precision': 0.7797638046600702, 'overall_recall': 0.7981051943809213, 'overall_f1': 0.7888278979657734, 'overall_accuracy': 0.9766476169026773}\n",
            "Training...: 100%|████████████████████████████| 785/785 [06:57<00:00,  1.88it/s]\n",
            "Step... (1000/4710 | Training Loss: 0.0011759366607293487, Learning Rate: 3.9394901250489056e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (1000/4710 | Validation metrics: {'__precision': 0.7702539298669892, '__recall': 0.8324077098987259, '__f1': 0.800125608415764, '__number': 3061, 'overall_precision': 0.7702539298669892, 'overall_recall': 0.8324077098987259, 'overall_f1': 0.800125608415764, 'overall_accuracy': 0.9771684355733228}\n",
            "Step... (1500/4710 | Training Loss: 0.003230378497391939, Learning Rate: 3.408705015317537e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (1570/4710 | Validation metrics: {'__precision': 0.767580452920143, '__recall': 0.8415550473701405, '__f1': 0.8028673835125447, '__number': 3061, 'overall_precision': 0.767580452920143, 'overall_recall': 0.8415550473701405, 'overall_f1': 0.8028673835125447, 'overall_accuracy': 0.9784493679795051}\n",
            "Training...: 100%|████████████████████████████| 785/785 [06:50<00:00,  1.91it/s]\n",
            "Step... (2000/4710 | Training Loss: 0.0008388151181861758, Learning Rate: 2.877919359889347e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (2000/4710 | Validation metrics: {'__precision': 0.7983920841063699, '__recall': 0.8435151911140151, '__f1': 0.8203335980937253, '__number': 3061, 'overall_precision': 0.7983920841063699, 'overall_recall': 0.8435151911140151, 'overall_f1': 0.8203335980937253, 'overall_accuracy': 0.9798710621885645}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (2355/4710 | Validation metrics: {'__precision': 0.7962903741605373, '__recall': 0.8134596537079386, '__f1': 0.8047834518422754, '__number': 3061, 'overall_precision': 0.7962903741605373, 'overall_recall': 0.8134596537079386, 'overall_f1': 0.8047834518422754, 'overall_accuracy': 0.9786886630443963}\n",
            "Training...: 100%|████████████████████████████| 785/785 [06:50<00:00,  1.91it/s]\n",
            "Step... (2500/4710 | Training Loss: 0.001234716852195561, Learning Rate: 2.3471337044611573e-05)\n",
            "Step... (3000/4710 | Training Loss: 0.000278618885204196, Learning Rate: 1.8163484128308482e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (3000/4710 | Validation metrics: {'__precision': 0.8046526449968132, '__recall': 0.8248938255472068, '__f1': 0.8146475237941603, '__number': 3061, 'overall_precision': 0.8046526449968132, 'overall_recall': 0.8248938255472068, 'overall_f1': 0.8146475237941603, 'overall_accuracy': 0.9790264913713015}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (3140/4710 | Validation metrics: {'__precision': 0.8151016456921588, '__recall': 0.8252205161711859, '__f1': 0.82012987012987, '__number': 3061, 'overall_precision': 0.8151016456921588, 'overall_recall': 0.8252205161711859, 'overall_f1': 0.82012987012987, 'overall_accuracy': 0.9798710621885645}\n",
            "Training...: 100%|████████████████████████████| 785/785 [06:49<00:00,  1.92it/s]\n",
            "Step... (3500/4710 | Training Loss: 5.7903343986254185e-05, Learning Rate: 1.2855627574026585e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (3925/4710 | Validation metrics: {'__precision': 0.8008698353525939, '__recall': 0.8422084286180986, '__f1': 0.8210191082802548, '__number': 3061, 'overall_precision': 0.8008698353525939, 'overall_recall': 0.8422084286180986, 'overall_f1': 0.8210191082802548, 'overall_accuracy': 0.9792657864361927}\n",
            "Training...: 100%|████████████████████████████| 785/785 [06:26<00:00,  2.03it/s]\n",
            "Step... (4000/4710 | Training Loss: 2.4402283088420518e-05, Learning Rate: 7.5477719292393886e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:22<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (4000/4710 | Validation metrics: {'__precision': 0.8022457891453525, '__recall': 0.8402482848742241, '__f1': 0.8208074038614968, '__number': 3061, 'overall_precision': 0.8022457891453525, 'overall_recall': 0.8402482848742241, 'overall_f1': 0.8208074038614968, 'overall_accuracy': 0.9794910053207961}\n",
            "Step... (4500/4710 | Training Loss: 1.951012382050976e-05, Learning Rate: 2.239915602331166e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:23<00:00,  1.72it/s]\n",
            "INFO:__main__:Step... (4710/4710 | Validation metrics: {'__precision': 0.7990683229813664, '__recall': 0.8405749754982031, '__f1': 0.8192962903996178, '__number': 3061, 'overall_precision': 0.7990683229813664, 'overall_recall': 0.8405749754982031, 'overall_f1': 0.8192962903996178, 'overall_accuracy': 0.979645843303961}\n",
            "Training...: 100%|████████████████████████████| 785/785 [06:50<00:00,  1.91it/s]\n",
            "Epoch ... 6/6: 100%|█████████████████████████████| 6/6 [40:43<00:00, 407.32s/it]\n",
            "Evaluating on Dev Set ...: 40it [00:22,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.7990683229813664, '__recall': 0.8405749754982031, '__f1': 0.8192962903996178, '__number': 3061, 'overall_precision': 0.7990683229813664, 'overall_recall': 0.8405749754982031, 'overall_f1': 0.8192962903996178, 'overall_accuracy': 0.979645843303961}\n",
            "Evaluating on Test Set...: 79it [00:45,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8037946092455955, '__recall': 0.8439525691699604, '__f1': 0.8233842356933518, '__number': 6325, 'overall_precision': 0.8037946092455955, 'overall_recall': 0.8439525691699604, 'overall_f1': 0.8233842356933518, 'overall_accuracy': 0.9797860105252152}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/tokcls/NCBI-disease_hf/train.json \\\n",
        "--validation_file data/tokcls/NCBI-disease_hf/dev.json \\\n",
        "--test_file data/tokcls/NCBI-disease_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--learning_rate 7e-5 \\\n",
        "--num_train_epochs 5 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task NCBI-disease \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "id": "wi5BGS54CwCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9392c20-e098-410e-9c8c-0a218c6344ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-4e811e02322508d1\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-4e811e02322508d1/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 26.69it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForTokenClassification: {('predictions', 'decoder', 'bias'), ('predictions', 'decoder', 'kernel'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'LayerNorm', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('albert', 'pooler', 'bias'), ('predictions', 'bias'), ('predictions', 'LayerNorm', 'kernel'), ('albert', 'embeddings', 'position_ids'), ('predictions', 'dense', 'bias'), ('predictions', 'dense', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 6/6 [00:01<00:00,  3.13ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  3.10ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  2.97ba/s]\n",
            "INFO:__main__:Sample 2195 of the training set: {'input_ids': [2, 50, 421, 8401, 47, 293, 6475, 5872, 235, 4576, 3101, 43, 22, 2692, 531, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 3037 of the training set: {'input_ids': [2, 137, 120, 21, 23, 14514, 21, 23, 36, 3358, 261, 60, 21, 14215, 25, 28, 17120, 44, 54, 8220, 22, 3522, 24, 9373, 2666, 281, 27, 125, 785, 213, 22, 377, 165, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, -100, 2, 2, -100, 2, 2, 2, 2, 0, -100, 2, 0, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 549 of the training set: {'input_ids': [2, 26, 286, 21, 23, 68, 110, 36, 22, 903, 2058, 39, 968, 210, 2334, 31, 7306, 43, 150, 136, 2226, 21, 23, 1032, 36, 22, 116, 2226, 31, 26, 47, 4850, 636, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, -100, 2, 2, 2, 2, 2, -100, -100, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (5 epochs) =====\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:15<00:00,  1.04s/it]\n",
            "INFO:__main__:Step... (339/1695 | Validation metrics: {'__precision': 0.7761363636363636, '__recall': 0.8678526048284625, '__f1': 0.8194361127774444, '__number': 787, 'overall_precision': 0.7761363636363636, 'overall_recall': 0.8678526048284625, 'overall_f1': 0.8194361127774444, 'overall_accuracy': 0.9837707038257749}\n",
            "Training...: 100%|████████████████████████████| 339/339 [03:14<00:00,  1.74it/s]\n",
            "Step... (500/1695 | Training Loss: 0.0017143283039331436, Learning Rate: 4.939233258483e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (678/1695 | Validation metrics: {'__precision': 0.6912899669239251, '__recall': 0.7966963151207116, '__f1': 0.7402597402597402, '__number': 787, 'overall_precision': 0.6912899669239251, 'overall_recall': 0.7966963151207116, 'overall_f1': 0.7402597402597402, 'overall_accuracy': 0.9809337060369644}\n",
            "Training...: 100%|████████████████████████████| 339/339 [02:46<00:00,  2.04it/s]\n",
            "Step... (1000/1695 | Training Loss: 0.002279196633026004, Learning Rate: 2.874336496461183e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (1000/1695 | Validation metrics: {'__precision': 0.8503740648379052, '__recall': 0.866581956797967, '__f1': 0.8584015103838892, '__number': 787, 'overall_precision': 0.8503740648379052, 'overall_recall': 0.866581956797967, 'overall_f1': 0.8584015103838892, 'overall_accuracy': 0.9868163043931745}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (1017/1695 | Validation metrics: {'__precision': 0.8482587064676617, '__recall': 0.866581956797967, '__f1': 0.8573224387177875, '__number': 787, 'overall_precision': 0.8482587064676617, 'overall_recall': 0.866581956797967, 'overall_f1': 0.8573224387177875, 'overall_accuracy': 0.9867745838374568}\n",
            "Training...: 100%|████████████████████████████| 339/339 [02:54<00:00,  1.94it/s]\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (1356/1695 | Validation metrics: {'__precision': 0.8381294964028777, '__recall': 0.8881829733163914, '__f1': 0.8624305983960519, '__number': 787, 'overall_precision': 0.8381294964028777, 'overall_recall': 0.8881829733163914, 'overall_f1': 0.8624305983960519, 'overall_accuracy': 0.9872335099503525}\n",
            "Training...: 100%|████████████████████████████| 339/339 [02:45<00:00,  2.04it/s]\n",
            "Step... (1500/1695 | Training Loss: 3.195085082552396e-05, Learning Rate: 8.09439461590955e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:08<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (1695/1695 | Validation metrics: {'__precision': 0.8537794299876085, '__recall': 0.8754764930114358, '__f1': 0.8644918444165621, '__number': 787, 'overall_precision': 0.8537794299876085, 'overall_recall': 0.8754764930114358, 'overall_f1': 0.8644918444165621, 'overall_accuracy': 0.9876924360632484}\n",
            "Training...: 100%|████████████████████████████| 339/339 [02:45<00:00,  2.05it/s]\n",
            "Epoch ... 5/5: 100%|█████████████████████████████| 5/5 [14:26<00:00, 173.37s/it]\n",
            "Evaluating on Dev Set ...: 15it [00:08,  1.76it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8537794299876085, '__recall': 0.8754764930114358, '__f1': 0.8644918444165621, '__number': 787, 'overall_precision': 0.8537794299876085, 'overall_recall': 0.8754764930114358, 'overall_f1': 0.8644918444165621, 'overall_accuracy': 0.9876924360632484}\n",
            "Evaluating on Test Set...: 15it [00:08,  1.75it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8696098562628337, '__recall': 0.8822916666666667, '__f1': 0.8759048603929681, '__number': 960, 'overall_precision': 0.8696098562628337, 'overall_recall': 0.8822916666666667, 'overall_f1': 0.8759048603929681, 'overall_accuracy': 0.9867738906804915}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/tokcls/JNLPBA_hf/train.json \\\n",
        "--validation_file data/tokcls/JNLPBA_hf/dev.json \\\n",
        "--test_file data/tokcls/JNLPBA_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task JNLPBA \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "id": "IgMC8YgYCwCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c77c564-7c59-4d32-92ff-5932747da82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1d788b965ff358e0\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-1d788b965ff358e0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 711.99it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForTokenClassification: {('predictions', 'decoder', 'kernel'), ('predictions', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('albert', 'embeddings', 'position_ids'), ('sop_classifier', 'classifier', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'dense', 'kernel'), ('predictions', 'LayerNorm', 'kernel'), ('predictions', 'decoder', 'bias'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'dense', 'bias'), ('albert', 'pooler', 'bias')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 17/17 [00:06<00:00,  2.65ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 2/2 [00:00<00:00,  2.53ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 4/4 [00:01<00:00,  2.68ba/s]\n",
            "INFO:__main__:Sample 12449 of the training set: {'input_ids': [2, 21, 8219, 49, 1726, 35, 1121, 43, 21, 8219, 49, 652, 37, 755, 9803, 25, 10797, 22, 8344, 10, 1312, 3475, 24, 21, 8219, 49, 652, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, -100, -100, -100, 2, 2, 2, 0, -100, -100, -100, 2, 2, 2, 2, 2, 2, 0, -100, -100, 1, 2, 0, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 16334 of the training set: {'input_ids': [2, 136, 24, 69, 3344, 21, 23, 160, 1337, 620, 388, 539, 21, 23, 1316, 22, 306, 3442, 1373, 3305, 811, 28, 1011, 26, 22, 21, 18647, 6208, 21, 251, 531, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, -100, 0, -100, -100, -100, -100, 2, -100, 2, 2, 0, -100, 1, 1, 2, 2, 2, 2, 2, 0, -100, 1, 1, -100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 446 of the training set: {'input_ids': [2, 37, 69, 370, 831, 21, 23, 68, 5195, 68, 59, 220, 88, 5892, 1044, 21, 23, 433, 952, 24, 21, 89, 4505, 21, 10, 39, 10, 699, 862, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 0, -100, -100, 2, -100, -100, -100, -100, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (20 epochs) =====\n",
            "Step... (500/14000 | Training Loss: 0.004547875840216875, Learning Rate: 4.8217854782706127e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:23<00:00,  1.21it/s]\n",
            "INFO:__main__:Step... (700/14000 | Validation metrics: {'__precision': 0.7977980097395723, '__recall': 0.8279499011206328, '__f1': 0.8125943497951261, '__number': 4551, 'overall_precision': 0.7977980097395723, 'overall_recall': 0.8279499011206328, 'overall_f1': 0.8125943497951261, 'overall_accuracy': 0.9580076273150587}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:23<00:00,  1.39it/s]\n",
            "Step... (1000/14000 | Training Loss: 0.004764489829540253, Learning Rate: 4.643214197130874e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (1000/14000 | Validation metrics: {'__precision': 0.8279171210468921, '__recall': 0.8341023950780049, '__f1': 0.8309982486865148, '__number': 4551, 'overall_precision': 0.8279171210468921, 'overall_recall': 0.8341023950780049, 'overall_f1': 0.8309982486865148, 'overall_accuracy': 0.9596300120098608}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (1400/14000 | Validation metrics: {'__precision': 0.8046001266089893, '__recall': 0.8378378378378378, '__f1': 0.8208826695371367, '__number': 4551, 'overall_precision': 0.8046001266089893, 'overall_recall': 0.8378378378378378, 'overall_f1': 0.8208826695371367, 'overall_accuracy': 0.9583658161437812}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:07<00:00,  1.43it/s]\n",
            "Step... (1500/14000 | Training Loss: 0.001957716653123498, Learning Rate: 4.464642915991135e-05)\n",
            "Step... (2000/14000 | Training Loss: 0.003958927001804113, Learning Rate: 4.2860712710535154e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/14000 | Validation metrics: {'__precision': 0.810471657291216, '__recall': 0.8231157987255548, '__f1': 0.8167447945056143, '__number': 4551, 'overall_precision': 0.810471657291216, 'overall_recall': 0.8231157987255548, 'overall_f1': 0.8167447945056143, 'overall_accuracy': 0.9573965993131203}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (2100/14000 | Validation metrics: {'__precision': 0.8109833971902938, '__recall': 0.8371786420566908, '__f1': 0.82387285111904, '__number': 4551, 'overall_precision': 0.8109833971902938, 'overall_recall': 0.8371786420566908, 'overall_f1': 0.82387285111904, 'overall_accuracy': 0.9585765154547945}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (2500/14000 | Training Loss: 0.0033706920221447945, Learning Rate: 4.1074999899137765e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (2800/14000 | Validation metrics: {'__precision': 0.8176211453744493, '__recall': 0.8156449132058888, '__f1': 0.8166318336816631, '__number': 4551, 'overall_precision': 0.8176211453744493, 'overall_recall': 0.8156449132058888, 'overall_f1': 0.8166318336816631, 'overall_accuracy': 0.9582604664882746}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (3000/14000 | Training Loss: 0.0025334146339446306, Learning Rate: 3.9289287087740377e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (3000/14000 | Validation metrics: {'__precision': 0.8078481552569844, '__recall': 0.8323445396616128, '__f1': 0.8199134199134198, '__number': 4551, 'overall_precision': 0.8078481552569844, 'overall_recall': 0.8323445396616128, 'overall_f1': 0.8199134199134198, 'overall_accuracy': 0.9583658161437812}\n",
            "Step... (3500/14000 | Training Loss: 0.002394268289208412, Learning Rate: 3.750357063836418e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (3500/14000 | Validation metrics: {'__precision': 0.8141612687067232, '__recall': 0.8009228740936059, '__f1': 0.8074878156845371, '__number': 4551, 'overall_precision': 0.8141612687067232, 'overall_recall': 0.8009228740936059, 'overall_f1': 0.8074878156845371, 'overall_accuracy': 0.954594298476644}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (4000/14000 | Training Loss: 0.0020700530149042606, Learning Rate: 3.5717854188987985e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (4000/14000 | Validation metrics: {'__precision': 0.8095341750108838, '__recall': 0.8171830366952318, '__f1': 0.8133406232914162, '__number': 4551, 'overall_precision': 0.8095341750108838, 'overall_recall': 0.8171830366952318, 'overall_f1': 0.8133406232914162, 'overall_accuracy': 0.957249109795411}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (4200/14000 | Validation metrics: {'__precision': 0.8113661202185792, '__recall': 0.8156449132058888, '__f1': 0.8134998904229673, '__number': 4551, 'overall_precision': 0.8113661202185792, 'overall_recall': 0.8156449132058888, 'overall_f1': 0.8134998904229673, 'overall_accuracy': 0.9576283685552348}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (4500/14000 | Training Loss: 0.0005145347095094621, Learning Rate: 3.39321413775906e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (4900/14000 | Validation metrics: {'__precision': 0.8111475409836065, '__recall': 0.8154251812788398, '__f1': 0.8132807363576594, '__number': 4551, 'overall_precision': 0.8111475409836065, 'overall_recall': 0.8154251812788398, 'overall_f1': 0.8132807363576594, 'overall_accuracy': 0.9565116622068646}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:51<00:00,  1.48it/s]\n",
            "Step... (5000/14000 | Training Loss: 0.00015040615107864141, Learning Rate: 3.214642856619321e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (5000/14000 | Validation metrics: {'__precision': 0.8093910806174958, '__recall': 0.8294880246099758, '__f1': 0.8193163320672816, '__number': 4551, 'overall_precision': 0.8093910806174958, 'overall_recall': 0.8294880246099758, 'overall_f1': 0.8193163320672816, 'overall_accuracy': 0.9573965993131203}\n",
            "Step... (5500/14000 | Training Loss: 0.00038189999759197235, Learning Rate: 3.036071575479582e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (5600/14000 | Validation metrics: {'__precision': 0.8046628859483301, '__recall': 0.8417930125247198, '__f1': 0.8228092783505154, '__number': 4551, 'overall_precision': 0.8046628859483301, 'overall_recall': 0.8417930125247198, 'overall_f1': 0.8228092783505154, 'overall_accuracy': 0.9581129769705653}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (6000/14000 | Training Loss: 0.0002035191864706576, Learning Rate: 2.8574999305419624e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (6000/14000 | Validation metrics: {'__precision': 0.8060263653483992, '__recall': 0.8464073829927489, '__f1': 0.8257234726688103, '__number': 4551, 'overall_precision': 0.8060263653483992, 'overall_recall': 0.8464073829927489, 'overall_f1': 0.8257234726688103, 'overall_accuracy': 0.9594614525610501}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (6300/14000 | Validation metrics: {'__precision': 0.8135667396061269, '__recall': 0.8169633047681828, '__f1': 0.8152614844863502, '__number': 4551, 'overall_precision': 0.8135667396061269, 'overall_recall': 0.8169633047681828, 'overall_f1': 0.8152614844863502, 'overall_accuracy': 0.9571016202777017}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (6500/14000 | Training Loss: 0.00010874042345676571, Learning Rate: 2.6789282856043428e-05)\n",
            "Step... (7000/14000 | Training Loss: 0.00014763603394385427, Learning Rate: 2.500357004464604e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (7000/14000 | Validation metrics: {'__precision': 0.8214992306001319, '__recall': 0.8211382113821138, '__f1': 0.8213186813186814, '__number': 4551, 'overall_precision': 0.8214992306001319, 'overall_recall': 0.8211382113821138, 'overall_f1': 0.8213186813186814, 'overall_accuracy': 0.9582815364193759}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (7500/14000 | Training Loss: 8.289877769129816e-06, Learning Rate: 2.3217855414259247e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (7700/14000 | Validation metrics: {'__precision': 0.8137573004542504, '__recall': 0.8266315095583389, '__f1': 0.8201438848920864, '__number': 4551, 'overall_precision': 0.8137573004542504, 'overall_recall': 0.8266315095583389, 'overall_f1': 0.8201438848920864, 'overall_accuracy': 0.9585133056614905}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:51<00:00,  1.48it/s]\n",
            "Step... (8000/14000 | Training Loss: 9.857898476184346e-06, Learning Rate: 2.143214260286186e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (8000/14000 | Validation metrics: {'__precision': 0.8090232150678931, '__recall': 0.8116897385190068, '__f1': 0.8103542832071955, '__number': 4551, 'overall_precision': 0.8090232150678931, 'overall_recall': 0.8116897385190068, 'overall_f1': 0.8103542832071955, 'overall_accuracy': 0.9567012915867765}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (8400/14000 | Validation metrics: {'__precision': 0.8179627601314349, '__recall': 0.8204790156009668, '__f1': 0.8192189556823168, '__number': 4551, 'overall_precision': 0.8179627601314349, 'overall_recall': 0.8204790156009668, 'overall_f1': 0.8192189556823168, 'overall_accuracy': 0.958218326626072}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:10<00:00,  1.43it/s]\n",
            "Step... (8500/14000 | Training Loss: 8.512231943313964e-06, Learning Rate: 1.964642979146447e-05)\n",
            "Step... (9000/14000 | Training Loss: 3.2900439691729844e-05, Learning Rate: 1.7860713342088275e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (9000/14000 | Validation metrics: {'__precision': 0.8138974134151687, '__recall': 0.8158646451329378, '__f1': 0.8148798419839789, '__number': 4551, 'overall_precision': 0.8138974134151687, 'overall_recall': 0.8158646451329378, 'overall_f1': 0.8148798419839789, 'overall_accuracy': 0.9577969280040455}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.68it/s]\n",
            "INFO:__main__:Step... (9100/14000 | Validation metrics: {'__precision': 0.8205407762756215, '__recall': 0.8268512414853878, '__f1': 0.8236839225128598, '__number': 4551, 'overall_precision': 0.8205407762756215, 'overall_recall': 0.8268512414853878, 'overall_f1': 0.8236839225128598, 'overall_accuracy': 0.9589768441457196}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (9500/14000 | Training Loss: 1.822210651880596e-05, Learning Rate: 1.6075000530690886e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (9800/14000 | Validation metrics: {'__precision': 0.8237221494102228, '__recall': 0.8286090969017799, '__f1': 0.8261583963194217, '__number': 4551, 'overall_precision': 0.8237221494102228, 'overall_recall': 0.8286090969017799, 'overall_f1': 0.8261583963194217, 'overall_accuracy': 0.9588293546280103}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:50<00:00,  1.49it/s]\n",
            "Step... (10000/14000 | Training Loss: 3.016501068486832e-06, Learning Rate: 1.4289286809798796e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (10000/14000 | Validation metrics: {'__precision': 0.8209635416666666, '__recall': 0.8312458800263678, '__f1': 0.8260727153619392, '__number': 4551, 'overall_precision': 0.8209635416666666, 'overall_recall': 0.8312458800263678, 'overall_f1': 0.8260727153619392, 'overall_accuracy': 0.958934704283517}\n",
            "Step... (10500/14000 | Training Loss: 3.6016804187966045e-06, Learning Rate: 1.2503573088906705e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (10500/14000 | Validation metrics: {'__precision': 0.8239773716275022, '__recall': 0.8321248077345639, '__f1': 0.8280310484311796, '__number': 4551, 'overall_precision': 0.8239773716275022, 'overall_recall': 0.8321248077345639, 'overall_f1': 0.8280310484311796, 'overall_accuracy': 0.9594614525610501}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (11000/14000 | Training Loss: 2.1002153516747057e-05, Learning Rate: 1.071785663953051e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (11000/14000 | Validation metrics: {'__precision': 0.8198024054982818, '__recall': 0.8387167655460338, '__f1': 0.8291517323775388, '__number': 4551, 'overall_precision': 0.8198024054982818, 'overall_recall': 0.8387167655460338, 'overall_f1': 0.8291517323775388, 'overall_accuracy': 0.9592928931122395}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (11200/14000 | Validation metrics: {'__precision': 0.8252448313384113, '__recall': 0.8332234673698088, '__f1': 0.8292149573584081, '__number': 4551, 'overall_precision': 0.8252448313384113, 'overall_recall': 0.8332234673698088, 'overall_f1': 0.8292149573584081, 'overall_accuracy': 0.9595878721476581}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:08<00:00,  1.43it/s]\n",
            "Step... (11500/14000 | Training Loss: 3.0253877412178554e-06, Learning Rate: 8.93214291863842e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (11900/14000 | Validation metrics: {'__precision': 0.8180242634315424, '__recall': 0.8297077565370248, '__f1': 0.8238245881967927, '__number': 4551, 'overall_precision': 0.8180242634315424, 'overall_recall': 0.8297077565370248, 'overall_f1': 0.8238245881967927, 'overall_accuracy': 0.9591454035945303}\n",
            "Training...: 100%|████████████████████████████| 700/700 [07:52<00:00,  1.48it/s]\n",
            "Step... (12000/14000 | Training Loss: 3.1045353807712672e-06, Learning Rate: 7.14642965249368e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.75it/s]\n",
            "INFO:__main__:Step... (12000/14000 | Validation metrics: {'__precision': 0.8192614985964155, '__recall': 0.8336629312239069, '__f1': 0.8263994772380745, '__number': 4551, 'overall_precision': 0.8192614985964155, 'overall_recall': 0.8336629312239069, 'overall_f1': 0.8263994772380745, 'overall_accuracy': 0.9592296833189355}\n",
            "Step... (12500/14000 | Training Loss: 2.950627504105796e-06, Learning Rate: 5.360713657864835e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.69it/s]\n",
            "INFO:__main__:Step... (12600/14000 | Validation metrics: {'__precision': 0.8198002605297439, '__recall': 0.8297077565370248, '__f1': 0.8247242546685596, '__number': 4551, 'overall_precision': 0.8198002605297439, 'overall_recall': 0.8297077565370248, 'overall_f1': 0.8247242546685596, 'overall_accuracy': 0.9592928931122395}\n",
            "Training...: 100%|████████████████████████████| 700/700 [08:09<00:00,  1.43it/s]\n",
            "Step... (13000/14000 | Training Loss: 1.5111775155673968e-06, Learning Rate: 3.57500016434642e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:16<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (13000/14000 | Validation metrics: {'__precision': 0.8207016024252923, '__recall': 0.8327840035157108, '__f1': 0.826698658523285, '__number': 4551, 'overall_precision': 0.8207016024252923, 'overall_recall': 0.8327840035157108, 'overall_f1': 0.826698658523285, 'overall_accuracy': 0.9596721518720633}\n",
            "Training...:  76%|█████████████████████▍      | 535/700 [06:04<01:47,  1.54it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ALBERT-xxlarge-PMC \\\n",
        "--train_file data/tokcls/ebmnlp_hf/train.json \\\n",
        "--validation_file data/tokcls/ebmnlp_hf/dev.json \\\n",
        "--test_file data/tokcls/ebmnlp_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 1 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task \"EBM PICO\" \\\n",
        "--max_seq_length 512 \\\n",
        "--return_macro_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "id": "ysAy1x9rCwCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25dbe1ab-8374-43c3-8762-0a1cc148d0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-66c19f981d7e86e9\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-66c19f981d7e86e9/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 636.63it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-INT\",\n",
            "    \"1\": \"B-OUT\",\n",
            "    \"2\": \"B-PAR\",\n",
            "    \"3\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"label2id\": {\n",
            "    \"B-INT\": 0,\n",
            "    \"B-OUT\": 1,\n",
            "    \"B-PAR\": 2,\n",
            "    \"O\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-PMC\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-PMC/snapshots/047499f199be4e57c5dd131a355914131d9c9669/pytorch_model.bin\n",
            "PyTorch checkpoint contains 227,028,962 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC were not used when initializing FlaxAlbertForTokenClassification: {('albert', 'embeddings', 'position_ids'), ('sop_classifier', 'classifier', 'kernel'), ('predictions', 'dense', 'bias'), ('albert', 'pooler', 'bias'), ('predictions', 'bias'), ('predictions', 'dense', 'kernel'), ('albert', 'pooler', 'kernel'), ('predictions', 'decoder', 'bias'), ('predictions', 'LayerNorm', 'bias'), ('predictions', 'decoder', 'kernel'), ('sop_classifier', 'classifier', 'bias'), ('predictions', 'LayerNorm', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxAlbertForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ALBERT-xxlarge-PMC and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 41/41 [00:16<00:00,  2.51ba/s]\n",
            "Running tokenizer on dataset: 100%|█████████████| 11/11 [00:04<00:00,  2.65ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00,  3.66ba/s]\n",
            "INFO:__main__:Sample 3743 of the training set: {'input_ids': [2, 78, 33, 54, 1182, 173, 728, 37, 2910, 32, 21, 12943, 1217, 353, 46, 194, 26, 178, 390, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 3, 3, -100, 3, 3, 3, 3, 3, -100, -100, 3, 3, 3, 3, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 28530 of the training set: {'input_ids': [2, 22, 41, 128, 81, 11946, 28, 265, 208, 24, 27, 10, 4164, 10, 2365, 29857, 4981, 41, 22, 1678, 128, 81, 21, 23, 75, 99, 2864, 27, 10, 4164, 10, 301, 4415, 6929, 4981, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 2, -100, 2, 3, 3, 3, 3, 3, 1, -100, -100, -100, -100, -100, 1, 3, 3, 3, -100, 3, 3, -100, 3, 3, 3, 1, -100, -100, -100, -100, -100, -100, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 32232 of the training set: {'input_ids': [2, 2087, 10, 7758, 21, 11940, 11129, 1117, 35, 4953, 32, 224, 29, 1268, 4406, 11181, 863, 75, 54, 143, 303, 26, 41, 128, 21, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, -100, -100, 3, -100, -100, -100, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (1 epochs) =====\n",
            "Step... (500/2558 | Training Loss: 0.015506650321185589, Learning Rate: 3.219702921342105e-05)\n",
            "Step... (1000/2558 | Training Loss: 0.015806159004569054, Learning Rate: 2.4378417947445996e-05)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [01:40<00:00,  1.62it/s]\n",
            "INFO:__main__:Step... (1000/2558 | Validation metrics: {'macro_precision': 0.7565722081271865, 'macro_recall': 0.6782014799825363, 'macro_f1': 0.7136872743587697}\n",
            "Step... (1500/2558 | Training Loss: 0.017638348042964935, Learning Rate: 1.6559812138439156e-05)\n",
            "Step... (2000/2558 | Training Loss: 0.016081780195236206, Learning Rate: 8.741201781958807e-06)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [01:33<00:00,  1.74it/s]\n",
            "INFO:__main__:Step... (2000/2558 | Validation metrics: {'macro_precision': 0.7576261951437996, 'macro_recall': 0.6885458910406307, 'macro_f1': 0.7207550080359505}\n",
            "Step... (2500/2558 | Training Loss: 0.02958046644926071, Learning Rate: 9.22594040275726e-07)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [01:34<00:00,  1.73it/s]\n",
            "INFO:__main__:Step... (2558/2558 | Validation metrics: {'macro_precision': 0.7664058605937719, 'macro_recall': 0.679343975190339, 'macro_f1': 0.719937098281207}\n",
            "Training...: 100%|██████████████████████████| 2558/2558 [25:11<00:00,  1.69it/s]\n",
            "Epoch ... 1/1: 100%|████████████████████████████| 1/1 [25:11<00:00, 1511.47s/it]\n",
            "Evaluating on Dev Set ...: 163it [01:33,  1.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'macro_precision': 0.7664058605937719, 'macro_recall': 0.679343975190339, 'macro_f1': 0.719937098281207}\n",
            "Evaluating on Test Set...: 33it [00:19,  1.73it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'macro_precision': 0.7541105201958992, 'macro_recall': 0.7416323932427652, 'macro_f1': 0.7348466297557875}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BioM-ELECTRA-Base**"
      ],
      "metadata": {
        "id": "xnquHEZAyriJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/seqcls/chemprot_hf/train.json \\\n",
        "--validation_file data/seqcls/chemprot_hf/dev.json \\\n",
        "--test_file data/seqcls/chemprot_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 4 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task ChemProt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca232c3-1309-46b4-fab3-6f88623dac9b",
        "id": "TIl3MNxNyriO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-7d7c32e2c7c8f019\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-7d7c32e2c7c8f019/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "\r  0%|                                                     | 0/3 [00:00<?, ?it/s]\r100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 668.49it/s]\r\n",
            "['0', 'CPR:3', 'CPR:4', 'CPR:5', 'CPR:6', 'CPR:9']\n",
            "Downloading: 100%|██████████████████████████████| 665/665 [00:00<00:00, 662kB/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Downloading: 100%|███████████████████████████| 225k/225k [00:00<00:00, 6.40MB/s]\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Downloading: 100%|███████████████████████████| 433M/433M [00:06<00:00, 67.6MB/s]\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForSequenceClassification: {('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense', 'kernel'), ('electra', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'dense', 'kernel'), ('classifier', 'out_proj', 'kernel'), ('classifier', 'dense', 'bias'), ('classifier', 'out_proj', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  7.64ba/s]\n",
            "100%|███████████████████████████████████████████| 12/12 [00:01<00:00,  7.43ba/s]\n",
            "100%|███████████████████████████████████████████| 16/16 [00:01<00:00,  8.23ba/s]\n",
            "INFO:__main__:Sample 13690 of the training set: {'input_ids': [2, 3179, 29, 1701, 4461, 2842, 1699, 1035, 1787, 11, 1889, 1035, 12, 15, 1683, 11770, 2445, 1715, 20364, 3151, 1950, 7617, 18269, 8182, 11, 17012, 12, 2027, 2082, 26043, 1026, 2385, 2826, 2182, 1685, 1680, 3455, 4077, 34, 16, 19176, 2292, 11, 6794, 12, 1690, 9950, 11, 9626, 12, 15, 1715, 11462, 20533, 13549, 11, 11462, 12, 15, 1683, 20169, 1950, 26043, 1026, 2826, 6794, 1772, 3311, 1685, 6794, 19272, 11, 6794, 16, 61, 12, 15, 1715, 4566, 1701, 2076, 6735, 1755, 2826, 2182, 1685, 6794, 1690, 35, 3829, 7, 1690, 1755, 3599, 1685, 35, 2397, 7, 15, 6794, 16, 61, 1690, 9626, 19272, 11, 9626, 16, 61, 12, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 4182 of the training set: {'input_ids': [2, 1683, 5640, 10659, 11394, 15, 22, 16, 16313, 1734, 2389, 5338, 1772, 8877, 1680, 14928, 1922, 1685, 1680, 15543, 1690, 25552, 35, 3829, 7, 5171, 1715, 2306, 2222, 18839, 1690, 2033, 7729, 3797, 5846, 10659, 11394, 15, 22, 16, 16313, 16, 6312, 2727, 6306, 11, 35, 2397, 7, 12, 1690, 10659, 11394, 15, 22, 16, 16313, 16, 3784, 26445, 2573, 11374, 1016, 2678, 17676, 11, 11660, 1021, 12, 1748, 3378, 4678, 1772, 3308, 16, 9374, 1680, 5640, 1715, 11842, 1781, 11660, 1021, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 14123 of the training set: {'input_ids': [2, 35, 2397, 7, 1690, 35, 3829, 7, 21634, 1999, 1732, 1680, 2504, 5115, 6790, 1725, 3314, 1701, 19393, 16, 11149, 1682, 42, 3877, 2156, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (4 epochs) =====\n",
            "Step... (500/9016 | Training Loss: 0.19228622317314148, Learning Rate: 1.8893078959081322e-05)\n",
            "Step... (1000/9016 | Training Loss: 0.3290571868419647, Learning Rate: 1.7783939256332815e-05)\n",
            "Step... (1500/9016 | Training Loss: 0.6125084757804871, Learning Rate: 1.6674799553584307e-05)\n",
            "Step... (2000/9016 | Training Loss: 0.5326133370399475, Learning Rate: 1.5565661669825204e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:12<00:00, 14.54it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.737625604763677, 'eval_recall': 0.8230897009966778, 'eval_F1': 0.77801766437684}| Step... (2254/9016 | Eval metrics: {'accuracy': 0.905040823571175}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [03:03<00:00, 12.26it/s]\n",
            "Step... (2500/9016 | Training Loss: 0.058979060500860214, Learning Rate: 1.4456521967076696e-05)\n",
            "Step... (3000/9016 | Training Loss: 0.37700292468070984, Learning Rate: 1.334738226432819e-05)\n",
            "Step... (3500/9016 | Training Loss: 0.02214912325143814, Learning Rate: 1.2238242561579682e-05)\n",
            "Step... (4000/9016 | Training Loss: 0.03302440047264099, Learning Rate: 1.1129102858831175e-05)\n",
            "Step... (4500/9016 | Training Loss: 0.02901993878185749, Learning Rate: 1.001996406557737e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:07<00:00, 24.86it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7193229901269393, 'eval_recall': 0.8471760797342193, 'eval_F1': 0.7780320366132724}| Step... (4508/9016 | Eval metrics: {'accuracy': 0.901313454029109}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [01:52<00:00, 20.05it/s]\n",
            "Step... (5000/9016 | Training Loss: 0.004436066374182701, Learning Rate: 8.910825272323564e-06)\n",
            "Step... (5500/9016 | Training Loss: 0.009392636828124523, Learning Rate: 7.801685569575056e-06)\n",
            "Step... (6000/9016 | Training Loss: 0.48194363713264465, Learning Rate: 6.6925463215739e-06)\n",
            "Step... (6500/9016 | Training Loss: 0.2997462749481201, Learning Rate: 5.583406618825393e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:07<00:00, 24.52it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7285358033487709, 'eval_recall': 0.8492524916943521, 'eval_F1': 0.7842761265580057}| Step... (6762/9016 | Eval metrics: {'accuracy': 0.9046858359957402}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [01:52<00:00, 20.03it/s]\n",
            "Step... (7000/9016 | Training Loss: 0.3082713484764099, Learning Rate: 4.474267825571587e-06)\n",
            "Step... (7500/9016 | Training Loss: 0.5516279339790344, Learning Rate: 3.3651278954494046e-06)\n",
            "Step... (8000/9016 | Training Loss: 0.003566027618944645, Learning Rate: 2.2559893295692746e-06)\n",
            "Step... (8500/9016 | Training Loss: 0.0021402796264737844, Learning Rate: 1.1468493994470919e-06)\n",
            "Step... (9000/9016 | Training Loss: 0.013544723391532898, Learning Rate: 3.7710666589418906e-08)\n",
            "Evaluating on Dev Data ...: 100%|█████████████| 177/177 [00:07<00:00, 24.57it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7706065318818041, 'eval_recall': 0.8230897009966778, 'eval_F1': 0.7959839357429719}| Step... (9016/9016 | Eval metrics: {'accuracy': 0.9140930067447639}) \n",
            "Training...: 100%|██████████████████████████| 2254/2254 [01:51<00:00, 20.14it/s]\n",
            "Epoch ... 4/4: 100%|█████████████████████████████| 4/4 [08:40<00:00, 130.20s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████| 247/247 [00:10<00:00, 24.30it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.772341628959276, 'test_recall': 0.7962099125364431, 'test_F1': 0.7840941716910709}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator  \\\n",
        "--train_file data/seqcls/DDI_hf/train.json \\\n",
        "--validation_file data/seqcls/DDI_hf/dev.json \\\n",
        "--test_file data/seqcls/DDI_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 3 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task DDI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fb2417-bd7e-489d-bcd7-657e02350202",
        "id": "FImHMZQxyriP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-0ecf918e6c735628\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-0ecf918e6c735628/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 688.83it/s]\n",
            "['0', 'DDI-advise', 'DDI-effect', 'DDI-int', 'DDI-mechanism']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForSequenceClassification: {('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'bias'), ('electra', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'dense', 'bias'), ('classifier', 'out_proj', 'bias'), ('classifier', 'dense', 'kernel'), ('classifier', 'out_proj', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|███████████████████████████████████████████| 26/26 [00:03<00:00,  7.96ba/s]\n",
            "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 10.30ba/s]\n",
            "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00,  7.09ba/s]\n",
            "INFO:__main__:Sample 2185 of the training set: {'input_ids': [2, 4992, 9349, 29, 1680, 2846, 3914, 2056, 3816, 4992, 4609, 1781, 10050, 2182, 15, 5026, 1772, 2228, 1755, 4992, 4609, 3548, 15, 4528, 15, 3272, 15, 4141, 15, 4609, 3872, 15, 1781, 8010, 15, 1781, 5161, 10050, 4528, 29, 3455, 13705, 2200, 13860, 15, 57, 16, 3455, 6758, 3707, 10784, 2292, 15, 20002, 15, 18445, 1690, 2374, 20662, 8008, 15, 2586, 13443, 11, 13695, 10136, 23567, 15, 28856, 15, 7284, 2403, 5106, 1828, 12, 15, 27440, 4007, 3524, 15, 43, 16, 9108, 8732, 3914, 15, 19420, 15, 35, 2445, 7, 15, 15802, 15, 6364, 1690, 6364, 8208, 15, 24666, 12419, 14549, 3380, 15, 14848, 15, 35, 2445, 7, 15, 4603, 3280, 21390, 15, 3433, 15, 27888, 14575, 3270, 3914, 15, 10842, 16, 3368, 3558, 15, 19502, 15, 21074, 9311, 15, 10442, 15, 25, 16, 23581, 1014, 5944, 15, 28093, 15, 3279, 7702, 2602, 15, 20025, 15, 19242, 15, 19834, 15, 1745, 1684, 2524, 1705, 15, 24596, 15, 12725, 11331, 15, 20024, 16121, 15, 22174, 12787, 1026, 15, 16448, 1969, 19724, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 10244 of the training set: {'input_ids': [2, 2223, 1682, 3323, 6978, 1953, 3025, 1760, 35, 2445, 7, 2029, 1982, 2224, 1755, 2974, 1680, 10308, 1781, 25522, 1685, 17877, 15, 35, 2445, 7, 15, 12423, 15, 1781, 25049, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 24518 of the training set: {'input_ids': [2, 4183, 13425, 1026, 1755, 35, 2445, 7, 1810, 3367, 1725, 6724, 23810, 15, 1690, 1898, 1953, 2030, 14960, 1682, 1680, 2418, 1685, 42, 3189, 9356, 5233, 1725, 35, 2445, 7, 2019, 6724, 16, 19664, 4987, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (3 epochs) =====\n",
            "Step... (500/3162 | Training Loss: 0.2847119867801666, Learning Rate: 1.684376911725849e-05)\n",
            "Step... (1000/3162 | Training Loss: 0.03986614942550659, Learning Rate: 1.3681213204108644e-05)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:06<00:00,  6.12it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.8747203579418344, 'eval_recall': 0.8336886993603412, 'eval_F1': 0.8537117903930133}| Step... (1054/3162 | Eval metrics: {'accuracy': 0.9527243589743589}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [02:32<00:00,  6.89it/s]\n",
            "Step... (1500/3162 | Training Loss: 0.1528664231300354, Learning Rate: 1.0518659109948203e-05)\n",
            "Step... (2000/3162 | Training Loss: 0.06652370095252991, Learning Rate: 7.356103651545709e-06)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:01<00:00, 24.12it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8282208588957055, 'eval_recall': 0.8635394456289979, 'eval_F1': 0.8455114822546973}| Step... (2108/3162 | Eval metrics: {'accuracy': 0.9499198717948718}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [01:23<00:00, 12.57it/s]\n",
            "Step... (2500/3162 | Training Loss: 0.006336505990475416, Learning Rate: 4.193549102637917e-06)\n",
            "Step... (3000/3162 | Training Loss: 0.016880420967936516, Learning Rate: 1.0309934168617474e-06)\n",
            "Evaluating on Dev Data ...: 100%|███████████████| 39/39 [00:01<00:00, 24.66it/s]\n",
            "INFO:__main__:{'eval_precision': 0.8490566037735849, 'eval_recall': 0.8635394456289979, 'eval_F1': 0.8562367864693446}| Step... (3162/3162 | Eval metrics: {'accuracy': 0.953926282051282}) \n",
            "Training...: 100%|██████████████████████████| 1054/1054 [01:24<00:00, 12.46it/s]\n",
            "Epoch ... 3/3: 100%|█████████████████████████████| 3/3 [05:21<00:00, 107.11s/it]\n",
            "Evaluating on Test Data ...: 100%|██████████████| 90/90 [00:03<00:00, 24.68it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.8309278350515464, 'test_recall': 0.8232890704800817, 'test_F1': 0.8270908158029758}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/seqcls/GAD_hf/train.json \\\n",
        "--validation_file data/seqcls/GAD_hf/dev.json \\\n",
        "--test_file data/seqcls/GAD_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 1e-5 \\\n",
        "--num_train_epochs 4 \\\n",
        "--max_seq_length 256 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name PRF1 \\\n",
        "--blurb_task GAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a434e95-872f-483d-e0b7-ca8175ca2122",
        "id": "Lx8m9gGLyriP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-5058072c4f44a661\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-5058072c4f44a661/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 851.23it/s]\n",
            "['0', '1']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForSequenceClassification: {('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'out_proj', 'bias'), ('classifier', 'out_proj', 'kernel'), ('classifier', 'dense', 'kernel'), ('classifier', 'dense', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00,  9.88ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.30ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.19ba/s]\n",
            "INFO:__main__:Sample 531 of the training set: {'input_ids': [2, 1682, 1805, 1901, 1685, 35, 2174, 7, 1808, 15, 12596, 1734, 1888, 2374, 1701, 6902, 1682, 1680, 35, 2397, 7, 2397, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "INFO:__main__:Sample 3829 of the training set: {'input_ids': [2, 3805, 1685, 3098, 1685, 35, 2397, 7, 6902, 1781, 12972, 26079, 1026, 42, 2683, 2467, 1685, 1805, 2397, 3173, 15831, 1701, 1898, 3430, 35, 2174, 7, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "INFO:__main__:Sample 2225 of the training set: {'input_ids': [2, 1898, 2606, 2393, 1760, 2641, 5519, 1685, 5769, 2310, 2056, 1765, 8940, 1772, 35, 2397, 7, 1690, 28792, 6902, 15, 1690, 1760, 1680, 3471, 1685, 3056, 2446, 3125, 1682, 35, 2174, 7, 2398, 1715, 4308, 19776, 2056, 3568, 1683, 2497, 2467, 1682, 5769, 10855, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (4 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_precision': 0.7635327635327636, 'eval_recall': 0.9146757679180887, 'eval_F1': 0.8322981366459627}| Step... (266/1064 | Eval metrics: {'accuracy': 0.7981308411214953}) \n",
            "Training...: 100%|████████████████████████████| 266/266 [01:19<00:00,  3.37it/s]\n",
            "Step... (500/1064 | Training Loss: 0.4225180149078369, Learning Rate: 5.310150299919769e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:00<00:00, 25.43it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7771260997067448, 'eval_recall': 0.9044368600682594, 'eval_F1': 0.83596214511041}| Step... (532/1064 | Eval metrics: {'accuracy': 0.805607476635514}) \n",
            "Training...: 100%|████████████████████████████| 266/266 [00:16<00:00, 15.90it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:00<00:00, 25.45it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7828571428571428, 'eval_recall': 0.9351535836177475, 'eval_F1': 0.8522550544323484}| Step... (798/1064 | Eval metrics: {'accuracy': 0.822429906542056}) \n",
            "Training...: 100%|████████████████████████████| 266/266 [00:16<00:00, 15.86it/s]\n",
            "Step... (1000/1064 | Training Loss: 0.23475965857505798, Learning Rate: 6.109023047429218e-07)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 9/9 [00:00<00:00, 24.96it/s]\n",
            "INFO:__main__:{'eval_precision': 0.7694444444444445, 'eval_recall': 0.9453924914675768, 'eval_F1': 0.8483920367534457}| Step... (1064/1064 | Eval metrics: {'accuracy': 0.8149532710280374}) \n",
            "Training...: 100%|████████████████████████████| 266/266 [00:16<00:00, 15.98it/s]\n",
            "Epoch ... 4/4: 100%|██████████████████████████████| 4/4 [02:09<00:00, 32.31s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 9/9 [00:00<00:00, 25.48it/s]\n",
            "INFO:__main__: test results : {'test_precision': 0.7833827893175074, 'test_recall': 0.9395017793594306, 'test_F1': 0.8543689320388348}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path  sultan/BioM-ELECTRA-Base-Discriminator  \\\n",
        "--train_file data/seqcls/bioasq_hf/train.json \\\n",
        "--validation_file data/seqcls/bioasq_hf/dev.json \\\n",
        "--test_file data/seqcls/bioasq_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--blurb_task BioASQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6866d58-78c1-4a29-8fab-2e738a733132",
        "id": "mWha4d--yriP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-25f663af987ecd04\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-25f663af987ecd04/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 843.13it/s]\n",
            "['no', 'yes']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForSequenceClassification: {('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('electra', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'dense', 'kernel'), ('classifier', 'out_proj', 'bias'), ('classifier', 'out_proj', 'kernel'), ('classifier', 'dense', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-25f663af987ecd04/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-55b26d98faa2a5b1.arrow\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 24.58ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-25f663af987ecd04/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bcfb56c5c36f2c3c.arrow\n",
            "INFO:__main__:Sample 422 of the training set: {'input_ids': [2, 4042, 9594, 1784, 26176, 6129, 2310, 34, 3, 2323, 20752, 1685, 9383, 5099, 1013, 15, 9594, 1784, 15, 1690, 13977, 1033, 1810, 2310, 16, 2138, 2828, 17, 10489, 6899, 1685, 1680, 1816, 13419, 13436, 9594, 1784, 10743, 1682, 2323, 2310, 17, 1802, 1958, 1760, 9594, 1784, 1744, 6725, 22467, 12964, 1682, 5038, 2582, 1816, 4120, 1690, 8737, 23238, 2310, 2348, 4039, 2170, 1715, 5078, 2488, 3625, 1682, 3265, 17, 1898, 2123, 13435, 42, 3309, 19943, 22822, 1685, 9594, 1784, 1725, 10489, 2582, 1816, 8506, 1690, 6086, 15, 3009, 4945, 8831, 1701, 2174, 4200, 1682, 5096, 17, 13475, 1685, 3863, 16, 24978, 4436, 10489, 1716, 2165, 1690, 6418, 5368, 5932, 2045, 9594, 1784, 1682, 2323, 3549, 15, 4080, 1682, 20545, 3077, 1816, 5258, 1690, 2582, 1816, 19185, 17, 9594, 1784, 2182, 3629, 2582, 7465, 1682, 9143, 3812, 1808, 17, 9594, 1784, 18143, 19694, 1026, 1701, 3162, 2310, 3099, 1685, 9594, 1784, 1690, 4051, 15576, 21350, 10287, 1759, 1682, 3265, 15, 4080, 1682, 2051, 7400, 3162, 3549, 15, 53, 9594, 1784, 10743, 15, 42, 4863, 1685, 42, 2300, 16, 3038, 2582, 9281, 2397, 2435, 17697, 16, 2864, 26176, 1685, 1680, 2582, 9281, 9594, 1784, 10743, 1682, 2323, 4592, 1690, 3418, 2310, 1933, 17, 9594, 1784, 10743, 11, 9594, 1784, 12, 4676, 12, 1744, 26303, 1716, 2374, 1701, 4706, 4745, 1685, 5291, 4200, 1690, 19185, 17, 9594, 1784, 2181, 1744, 28425, 1682, 2323, 4947, 2310, 15, 9594, 1784, 18143, 7737, 4947, 9364, 1680, 2121, 4802, 1685, 1680, 2626, 1682, 2638, 1734, 10664, 1772, 2280, 5762, 1760, 9594, 1784, 26176, 4671, 3538, 1715, 3464, 2909, 1682, 2323, 4947, 2310, 17, 1802, 3581, 1760, 9594, 1784, 10743, 7617, 3418, 2310, 3067, 1690, 1760, 26176, 1685, 13419, 4208, 9477, 20777, 1690, 9711, 2581, 1682, 9672, 1772, 19327, 15341, 1690, 8037, 1816, 3227, 17, 26176, 1685, 9594, 1784, 10743, 9477, 9109, 13506, 1690, 8325, 42, 2467, 1725, 1816, 13419, 1682, 3812, 17, 3099, 1685, 9594, 1784, 10743, 9477, 6086, 1685, 1933, 2448, 4915, 4452, 1682, 1683, 2691, 6481, 3723, 2114, 17, 9594, 1784, 10743, 2181, 1744, 2762, 1682, 3013, 4874, 2323, 5711, 17, 3099, 1685, 2323, 9594, 1784, 10743, 10287, 2003, 1715, 49, 16, 7812, 1701, 6129, 1816, 6086, 2448, 26176, 1685, 9873, 9065, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "INFO:__main__:Sample 417 of the training set: {'input_ids': [2, 1810, 1877, 16, 3066, 6560, 21232, 1007, 2916, 1725, 2015, 1685, 8097, 4554, 34, 3, 1680, 2167, 1685, 1877, 16, 3066, 6560, 21232, 1007, 1744, 1683, 6999, 9931, 4508, 1701, 6560, 4482, 2595, 6262, 2985, 11, 17998, 12, 1725, 9824, 4264, 3011, 1685, 3266, 8097, 4554, 11, 7727, 2293, 15, 31, 3644, 3127, 12, 1970, 22485, 17, 1680, 2167, 1685, 1877, 16, 3066, 6560, 21232, 1007, 1734, 7850, 28466, 1701, 1680, 2167, 1685, 6560, 17998, 15, 1715, 2015, 3624, 5953, 1682, 4564, 1685, 16403, 4554, 11, 3930, 17, 21, 8, 12, 1682, 1680, 6560, 16, 21232, 1007, 1922, 1690, 1682, 4294, 1685, 16986, 4554, 11, 2646, 17, 27, 8, 12, 1682, 1680, 17998, 1922, 11, 2209, 3140, 15, 27, 17, 23, 4739, 4985, 30, 2544, 8, 4414, 4048, 15, 16, 20, 17, 28, 1701, 2651, 17, 26, 12, 17, 2775, 1680, 3253, 1725, 1680, 2770, 3259, 1734, 5570, 1701, 1680, 9443, 1685, 7850, 28466, 1752, 15, 1680, 3540, 1685, 1877, 16, 3066, 6560, 21232, 1007, 1734, 2576, 1701, 1760, 1685, 17998, 1732, 4264, 3011, 1725, 3266, 8097, 4554, 1970, 22485, 17, 3937, 1877, 3066, 6560, 21232, 11, 5808, 18013, 12, 1744, 7246, 1732, 1683, 4820, 15, 3387, 8451, 2144, 1685, 21780, 15, 7679, 3387, 4121, 1701, 1680, 15020, 10, 60, 6193, 15, 1950, 2056, 2814, 5440, 15, 5677, 1690, 9543, 17, 1682, 2157, 15, 2100, 1744, 42, 5754, 2739, 1685, 1680, 7651, 1685, 5808, 18013, 1732, 1683, 4508, 5347, 1685, 21780, 17, 10102, 28800, 18013, 1690, 7046, 3921, 1014, 3473, 2576, 2956, 1685, 22485, 3624, 17, 1680, 2167, 1685, 5808, 18013, 1732, 42, 4264, 3011, 9276, 1744, 3173, 1682, 1680, 6827, 15, 5468, 15, 1690, 3531, 4012, 1732, 1683, 4508, 1701, 2175, 16, 4874, 2595, 2985, 7370, 17, 3040, 2739, 3795, 1760, 5808, 18013, 1744, 2300, 8451, 1690, 2056, 1765, 8021, 1682, 42, 7931, 1685, 1808, 2386, 4169, 15092, 3011, 1715, 2175, 16, 4874, 7370, 17, 18017, 15, 23841, 2709, 15, 1877, 16, 3066, 6560, 21232, 3712, 2543, 11, 10102, 28800, 18013, 12, 2029, 2030, 2093, 1701, 2814, 7370, 1682, 8097, 4554, 17, 3173, 3066, 2956, 1685, 10102, 28800, 18013, 2543, 1810, 2138, 1715, 4105, 3815, 1682, 5540, 8749, 1682, 23174, 1808, 17, 1683, 4508, 1701, 1680, 2167, 1685, 6560, 4482, 2595, 6262, 2985, 11, 7046, 3921, 1014, 12, 1732, 42, 2175, 16, 4874, 9276, 1701, 3011, 4264, 7382, 1682, 8403, 4554, 2029, 2030, 1680, 3254, 4798, 1685, 1877, 3066, 6560, 21232, 11, 5808, 18013, 12, 5991, 1682, 3013, 6005, 5299, 17, 2100, 2029, 2030, 2165, 2167, 1685, 5808, 18013, 11072, 3058, 1685, 26934, 9515, 4629, 1690, 4211, 1760, 2021, 1744, 8518, 1701, 2167, 15, 1690, 2300, 8451, 1772, 1680, 4554, 15, 2555, 12502, 2762, 6560, 13828, 8884, 16119, 1675, 17, 1877, 16, 3066, 6560, 21232, 1007, 11, 5808, 18013, 12, 1810, 17822, 1682, 20487, 1732, 42, 2144, 1685, 2175, 16, 4874, 4264, 3011, 1725, 8097, 4554, 1682, 6005, 6060, 2434, 5299, 5184, 1680, 4517, 17, 5808, 18013, 2056, 1765, 1732, 2916, 1732, 7046, 3921, 1014, 1798, 6124, 4264, 3257, 2330, 1732, 14742, 3494, 1690, 2626, 1685, 9438, 1682, 8097, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "INFO:__main__:Sample 608 of the training set: {'input_ids': [2, 1883, 3028, 11450, 1808, 2054, 18944, 20735, 16119, 2677, 3328, 34, 3, 3028, 11450, 11, 2560, 12, 1744, 1683, 4577, 7562, 1952, 7939, 1685, 1680, 2323, 2837, 1933, 6184, 3565, 3751, 11, 2082, 7025, 1701, 1732, 15692, 6275, 1026, 12, 4596, 1701, 4244, 4159, 1690, 18944, 20735, 16119, 2677, 3328, 17, 1805, 3506, 2430, 2056, 1765, 2336, 1682, 1808, 1715, 1680, 2846, 7250, 2944, 15, 2330, 1732, 2770, 10368, 4552, 3103, 18944, 20735, 16119, 2677, 11, 10368, 4552, 2851, 15, 11122, 2851, 12, 2570, 1715, 27089, 15, 2770, 2837, 18944, 20735, 16119, 2677, 11, 3028, 11450, 15, 18526, 16464, 2851, 15, 18526, 23647, 2125, 9667, 10423, 8376, 15, 3246, 23647, 2125, 9667, 10423, 8376, 15, 1690, 11555, 2649, 3316, 12, 15, 2770, 10097, 2707, 16452, 1693, 18944, 20735, 16119, 2677, 11, 11850, 8085, 7162, 15, 17491, 15, 1690, 15607, 1816, 2174, 12, 17, 42, 5048, 16, 2069, 16, 3186, 6764, 1715, 3038, 2560, 2386, 3484, 4290, 2015, 1725, 2874, 2281, 15, 3202, 3884, 1701, 3246, 21916, 2090, 1012, 1690, 1798, 2389, 1734, 6419, 1701, 1953, 18944, 20735, 16119, 2677, 3328, 17, 1970, 42, 6089, 3517, 19361, 2137, 2672, 4436, 1685, 2959, 3296, 3624, 15, 1680, 2348, 3714, 4064, 1715, 42, 2951, 4111, 15, 5280, 2031, 7095, 11, 2177, 1792, 12, 15, 2727, 2031, 7095, 11, 4401, 1014, 12, 15, 1690, 5698, 27415, 1725, 6926, 2837, 1816, 3246, 2011, 5690, 1690, 3484, 28183, 1725, 18944, 20735, 16119, 2677, 3328, 1690, 19593, 15, 1950, 3745, 1682, 3397, 3296, 2108, 17, 2784, 1890, 2229, 6114, 16419, 4813, 15, 4073, 2727, 2964, 3478, 7194, 15, 1690, 5633, 1739, 27526, 2031, 1682, 1680, 5280, 1715, 42, 2193, 16, 2177, 2951, 5385, 4989, 4372, 2837, 1816, 3246, 2011, 5690, 17, 1680, 2348, 3484, 42, 2782, 1685, 3028, 11450, 11, 2560, 12, 1690, 1734, 8857, 1755, 4365, 1690, 12435, 17, 28183, 11, 2654, 12, 1744, 5464, 1725, 1808, 1715, 18944, 20735, 16119, 2677, 3328, 1781, 12283, 11160, 3202, 1715, 13088, 15, 1950, 2056, 3938, 1701, 2461, 1680, 7390, 16, 5611, 2276, 17, 3028, 11450, 1744, 42, 9711, 2837, 16, 1816, 3870, 4080, 1814, 4754, 2837, 1933, 1682, 1680, 2951, 5385, 17, 2021, 1883, 3434, 42, 18944, 20735, 16119, 2677, 3328, 3884, 1701, 1680, 15692, 6275, 6895, 2138, 1715, 1680, 2174, 17, 1680, 2165, 18944, 20735, 16119, 2677, 1883, 2840, 1701, 5898, 6064, 21902, 1690, 2187, 7554, 4159, 1760, 2056, 5261, 7221, 17, 17994, 1810, 2996, 2027, 6862, 18944, 20735, 16119, 2677, 1744, 2051, 2672, 1682, 23604, 4919, 1911, 1776, 10, 60, 26255, 2851, 1690, 3028, 11450, 17, 4450, 8338, 28183, 1682, 42, 4840, 1715, 3028, 11450, 1690, 18944, 20735, 16119, 2677, 3328, 17, 42, 2265, 2069, 3186, 15, 4078, 2899, 15, 3619, 16, 14434, 15, 5630, 3301, 4840, 3202, 1715, 42, 2206, 2748, 3834, 1685, 2121, 5403, 4052, 1715, 18944, 20735, 16119, 2677, 3328, 3884, 1701, 3028, 11450, 17, 1680, 2143, 1901, 2571, 1725, 1680, 2389, 2194, 1680, 2167, 1685, 4450, 8338, 28183, 1701, 4083, 2121, 5403, 1685, 18944, 20735, 16119, 2677, 3328, 1682, 42, 4840, 1715, 3028, 11450, 17, 1683, 9849, 3323, 3612, 2117, 6231, 1715, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (20 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:05<00:00,  2.97s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_accuracy': 0.8266666531562805}| Step... (83/1660 | Eval metrics: {'accuracy': 0.8266666666666667}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [01:16<00:00,  1.08it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 15.92it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8399999737739563}| Step... (166/1660 | Eval metrics: {'accuracy': 0.84}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.29it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.28it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8399999737739563}| Step... (249/1660 | Eval metrics: {'accuracy': 0.84}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.57it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.13it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8533333539962769}| Step... (332/1660 | Eval metrics: {'accuracy': 0.8533333333333334}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.26it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8399999737739563}| Step... (415/1660 | Eval metrics: {'accuracy': 0.84}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.62it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.27it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8933333158493042}| Step... (498/1660 | Eval metrics: {'accuracy': 0.8933333333333333}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.40it/s]\n",
            "Step... (500/1660 | Training Loss: 0.01258282084017992, Learning Rate: 2.0981926354579628e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.04it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8933333158493042}| Step... (581/1660 | Eval metrics: {'accuracy': 0.8933333333333333}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.05it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.07it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (664/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.46it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.37it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (747/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.61it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 15.96it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (830/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.56it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.18it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (913/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.18it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.34it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (996/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.91it/s]\n",
            "Step... (1000/1660 | Training Loss: 0.0003150438133161515, Learning Rate: 1.1945782716793474e-05)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.00it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1079/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.80it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.20it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1162/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.75it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.17it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1245/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.81it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.24it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1328/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.20it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.43it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1411/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.76it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.01it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1494/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:04<00:00, 16.74it/s]\n",
            "Step... (1500/1660 | Training Loss: 0.00029213406378403306, Learning Rate: 2.9096388516336447e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.20it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1577/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.09it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 2/2 [00:00<00:00, 16.21it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.8799999952316284}| Step... (1660/1660 | Eval metrics: {'accuracy': 0.88}) \n",
            "Training...: 100%|██████████████████████████████| 83/83 [00:05<00:00, 16.54it/s]\n",
            "Epoch ... 20/20: 100%|██████████████████████████| 20/20 [02:52<00:00,  8.62s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 3/3 [00:00<00:00, 15.11it/s]\n",
            "INFO:__main__: test results : {'test_accuracy': 0.8714285492897034}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/seqcls/pubmedqa_hf/train.json \\\n",
        "--validation_file data/seqcls/pubmedqa_hf/dev.json \\\n",
        "--test_file data/seqcls/pubmedqa_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 3 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--blurb_task PubMedQA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc4b5f2-6616-4341-82c3-fe3e0728cf10",
        "id": "edkmJ6M9yriP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-d26b3028a530c846\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-d26b3028a530c846/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 749.21it/s]\n",
            "['maybe', 'no', 'yes']\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForSequenceClassification: {('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'out_proj', 'bias'), ('classifier', 'dense', 'kernel'), ('classifier', 'out_proj', 'kernel'), ('classifier', 'dense', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.73ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 56.09ba/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.50ba/s]\n",
            "INFO:__main__:Sample 430 of the training set: {'input_ids': [2, 4042, 5664, 17569, 4169, 7243, 2976, 3292, 5597, 34, 3, 5664, 17569, 11, 5836, 12, 1744, 7737, 2448, 2155, 1781, 2874, 3274, 24133, 1685, 5597, 1690, 7243, 1950, 24851, 42, 5612, 5597, 17, 1701, 2313, 2955, 42, 7243, 4637, 24851, 1680, 5612, 3292, 5597, 15, 42, 4039, 2615, 7243, 11, 3231, 5311, 16452, 1693, 17569, 29, 2517, 1014, 12, 1690, 42, 4039, 1715, 12610, 4092, 1685, 5597, 15, 50, 17, 46, 17, 23626, 5597, 11, 11811, 12, 15, 1950, 1744, 6447, 2374, 1701, 1680, 2418, 1685, 24824, 1702, 15, 1734, 2170, 1701, 4193, 5836, 11, 11003, 12, 17, 3658, 1748, 2525, 1683, 3615, 2368, 16, 11115, 6982, 9675, 11, 5722, 26021, 1796, 10794, 12, 1760, 1748, 4384, 2295, 2155, 1685, 2634, 4039, 29, 11, 20, 12, 2039, 11, 55, 32, 26, 12, 29, 3187, 2256, 2488, 3066, 16, 3187, 2256, 2052, 3066, 11, 2073, 8, 12, 5597, 16, 3187, 2256, 7243, 17, 11, 21, 12, 11003, 11, 55, 32, 26, 12, 29, 23, 3674, 24, 2256, 9345, 3066, 1715, 2073, 2256, 7243, 2562, 16, 3187, 2256, 2052, 3066, 11, 2073, 8, 12, 16, 5597, 3187, 2256, 7243, 17, 11, 22, 12, 2517, 1014, 11, 55, 32, 26, 12, 29, 2774, 2256, 2488, 3066, 16, 2073, 2256, 1982, 3066, 16, 3187, 7419, 2052, 3066, 11, 2073, 8, 12, 5597, 16, 23, 3187, 7419, 7243, 17, 11, 23, 12, 11811, 11, 55, 32, 25, 12, 29, 12610, 3000, 1701, 2073, 8, 3066, 2098, 3187, 7419, 16, 3187, 7419, 2052, 3066, 11, 2073, 8, 12, 5597, 16, 3187, 7419, 7243, 17, 1798, 1680, 2188, 1685, 2562, 5233, 15, 1680, 24728, 3222, 1734, 3158, 17, 1680, 11007, 3222, 1682, 2039, 9675, 1734, 25, 17, 26, 14, 18, 16, 20, 17, 23, 8, 11, 4483, 14, 18, 16, 4034, 12, 1685, 5770, 2453, 3222, 15, 1682, 11003, 9675, 21, 17, 25, 14, 18, 16, 19, 17, 27, 8, 15, 1682, 2517, 1014, 9675, 22, 17, 20, 14, 18, 16, 19, 17, 24, 8, 15, 1690, 1682, 11811, 9675, 22, 17, 19, 14, 18, 16, 19, 17, 22, 8, 11, 1948, 57, 31, 19, 17, 2766, 2873, 17, 2039, 12, 17, 1680, 2706, 1955, 1680, 2306, 5501, 7490, 1748, 4224, 1888, 1932, 15, 1690, 1982, 5527, 5233, 2716, 2373, 16, 5664, 4507, 4389, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}.\n",
            "INFO:__main__:Sample 423 of the training set: {'input_ids': [2, 20082, 5463, 6607, 7064, 2868, 1682, 21674, 10, 60, 2174, 29, 1883, 1802, 3629, 3259, 34, 3, 1701, 2271, 1680, 1890, 1685, 20082, 5463, 6607, 7064, 2868, 1682, 1808, 1715, 21674, 10, 60, 2174, 2150, 42, 2578, 1685, 2651, 2281, 15, 1690, 1701, 2958, 2844, 2100, 1810, 2446, 1950, 2982, 3629, 1680, 3259, 17, 8536, 16, 4746, 8272, 1808, 2570, 10667, 1772, 42, 2742, 9516, 1682, 2333, 28872, 4188, 5307, 2602, 1955, 10826, 1690, 9650, 1748, 2671, 1690, 2123, 1814, 5166, 1685, 1898, 1953, 2030, 5539, 17, 4855, 2147, 1685, 3259, 3633, 17, 1808, 1748, 5130, 2295, 2306, 2345, 11, 7509, 15, 3624, 1690, 7465, 12, 6473, 1755, 1680, 4395, 3259, 1685, 2076, 2015, 1732, 2942, 1798, 1680, 2194, 1685, 2147, 15, 50, 17, 46, 17, 5316, 2660, 11, 3721, 12, 2281, 1970, 2868, 17, 7509, 1744, 3905, 1732, 5355, 11146, 1685, 2542, 15563, 7690, 2109, 1715, 1726, 16, 9608, 1685, 15925, 9772, 9031, 15, 4615, 1685, 2121, 3529, 1690, 6210, 6082, 1755, 2052, 16, 2728, 9644, 4064, 17, 3624, 1744, 3905, 1732, 1680, 4091, 1685, 3105, 1685, 1898, 3529, 17, 7465, 1744, 3905, 1732, 1680, 1726, 16, 9608, 1685, 21674, 10, 60, 2174, 2051, 1981, 2155, 2069, 1970, 4933, 17, 2121, 3529, 2330, 1732, 2911, 15, 3124, 15, 4310, 15, 2138, 8446, 3648, 1690, 4711, 15, 5355, 2223, 2574, 5489, 1690, 3973, 2727, 7941, 15, 5280, 2964, 7941, 15, 2727, 10492, 15, 8614, 15, 4990, 1690, 3216, 2606, 1748, 3158, 1682, 4996, 1701, 1898, 2306, 2345, 1701, 2958, 2955, 3105, 2446, 2509, 12335, 3629, 3624, 1781, 7465, 1970, 2015, 17, 1685, 1680, 5166, 1808, 2990, 1682, 1805, 1901, 15, 3735, 11, 5127, 17, 26, 8, 12, 4115, 3582, 7509, 1690, 2604, 11, 3068, 17, 22, 8, 12, 5949, 2015, 17, 4126, 1808, 10250, 4336, 7465, 11, 2824, 3184, 16, 16080, 2660, 12, 1682, 2076, 2430, 1970, 5513, 7509, 15, 14291, 42, 5441, 1922, 1685, 4464, 1808, 11, 4642, 17, 21, 8, 12, 1682, 1680, 7509, 1922, 17, 5291, 1734, 2671, 1798, 2868, 1682, 4564, 1808, 15, 1685, 6314, 4078, 4115, 7509, 17, 1682, 3801, 15, 2298, 22, 1685, 28, 1808, 1682, 6314, 1982, 5291, 1734, 2671, 4115, 7509, 17, 1805, 3140, 1734, 1932, 11, 57, 32, 19, 17, 20980, 12, 17, 2278, 2060, 8614, 1690, 4990, 2606, 1748, 2595, 15, 1680, 7771, 1685, 9655, 7509, 1734, 2171, 2301, 1981, 2844, 2060, 8661, 1748, 3055, 11, 57, 32, 19, 17, 20866, 12, 17, 2100, 1748, 1932, 2706, 1955, 7509, 1690, 3624, 2345, 2278, 21, 16, 1690, 25, 16, 2621, 3973, 2727, 7941, 2182, 11, 57, 32, 19, 17, 7516, 1690, 19, 17, 2983, 15, 2427, 12, 1690, 25, 16, 2621, 3973, 5280, 2964, 7941, 2182, 11, 57, 32, 19, 17, 19124, 12, 1748, 2170, 17, 1805, 6169, 4348, 1685, 1808, 2386, 5949, 3216, 2015, 1682, 1680, 2743, 3973, 2578, 17, 3498, 1685, 2868, 2990, 14127, 1963, 1921, 1682, 2604, 15, 14127, 1963, 5613, 7820, 1682, 27, 1690, 14127, 1963, 6560, 6101, 1690, 17404, 21921, 1856, 1682, 22, 17, 9116, 1685, 4464, 1808, 3424, 2585, 2144, 1685, 9439, 5941, 2543, 3955, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.\n",
            "INFO:__main__:Sample 175 of the training set: {'input_ids': [2, 3477, 1685, 1680, 6287, 19306, 18621, 13071, 4810, 3110, 8014, 1682, 1680, 6157, 6764, 1814, 1680, 10211, 9006, 1685, 3890, 12559, 29, 1744, 3723, 42, 10705, 2611, 34, 3, 1680, 6005, 2527, 1772, 10954, 1922, 43, 1744, 2155, 1685, 1680, 2504, 4436, 1685, 6005, 7599, 7849, 16, 3144, 2366, 17, 1725, 1805, 6182, 42, 3847, 1744, 3621, 1701, 2562, 6157, 6764, 1682, 3121, 1701, 3927, 2233, 2697, 15, 1690, 2844, 2021, 1734, 1680, 2632, 15, 1701, 9982, 1683, 4497, 2015, 2098, 14089, 17, 1680, 2587, 1685, 1805, 1901, 1734, 1701, 3492, 1680, 3477, 1685, 1805, 10954, 1682, 1680, 6157, 2592, 1814, 3890, 12559, 15, 1732, 2300, 1732, 1680, 2706, 3544, 1701, 3723, 1690, 2293, 17, 42, 9095, 3308, 16, 5605, 1901, 5047, 1682, 1680, 2895, 16722, 1027, 5932, 1814, 3890, 12559, 17, 1680, 3392, 1744, 4339, 1814, 16294, 2592, 29, 20041, 1810, 1814, 2496, 6380, 3723, 11, 5215, 17, 22, 8, 12, 15, 4980, 1810, 1814, 11514, 1909, 2994, 3723, 11, 2721, 17, 22, 8, 12, 1690, 2651, 2592, 1814, 4818, 4819, 11, 25, 17, 23, 8, 12, 17, 2082, 2021, 1744, 3038, 1760, 5127, 1685, 3500, 1810, 2646, 2281, 3186, 1781, 2552, 11, 3615, 17, 4799, 8, 12, 15, 16080, 1810, 1955, 3525, 1690, 3930, 2281, 3186, 11, 4651, 17, 4065, 8, 12, 1690, 4361, 1810, 3460, 2281, 3186, 1781, 2051, 11, 2441, 17, 5318, 8, 12, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (30 epochs) =====\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:05<00:00,  5.61s/it]\n",
            "run_flax_seq2.py:543: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "INFO:__main__:{'eval_accuracy': 0.5400000214576721}| Step... (18/540 | Eval metrics: {'accuracy': 0.54}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [01:08<00:00,  3.82s/it]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.12it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.5400000214576721}| Step... (36/540 | Eval metrics: {'accuracy': 0.54}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.42it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.17it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.5600000023841858}| Step... (54/540 | Eval metrics: {'accuracy': 0.56}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.01it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.24it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.5799999833106995}| Step... (72/540 | Eval metrics: {'accuracy': 0.58}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.42it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.31it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6399999856948853}| Step... (90/540 | Eval metrics: {'accuracy': 0.64}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.51it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.25it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6200000047683716}| Step... (108/540 | Eval metrics: {'accuracy': 0.62}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.26it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.24it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6200000047683716}| Step... (126/540 | Eval metrics: {'accuracy': 0.62}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  7.99it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.34it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6399999856948853}| Step... (144/540 | Eval metrics: {'accuracy': 0.64}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.10it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.33it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (162/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.27it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.47it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6200000047683716}| Step... (180/540 | Eval metrics: {'accuracy': 0.62}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.41it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.42it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6200000047683716}| Step... (198/540 | Eval metrics: {'accuracy': 0.62}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.51it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.37it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (216/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.39it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.37it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (234/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.55it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.44it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (252/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.41it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.37it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (270/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.35it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.39it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (288/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.03it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.32it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (306/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.41it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.40it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6399999856948853}| Step... (324/540 | Eval metrics: {'accuracy': 0.64}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  7.82it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.77it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (342/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.57it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.36it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (360/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.48it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (378/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.51it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.49it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (396/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.32it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.43it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (414/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.04it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.56it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (432/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.31it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.19it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (450/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  7.78it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.41it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (468/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.46it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (486/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.32it/s]\n",
            "Step... (500/540 | Training Loss: 0.0011268246453255415, Learning Rate: 2.277778321513324e-06)\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.32it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (504/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.40it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.35it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (522/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.32it/s]\n",
            "Evaluating on Dev Data ...: 100%|█████████████████| 1/1 [00:00<00:00, 12.45it/s]\n",
            "INFO:__main__:{'eval_accuracy': 0.6000000238418579}| Step... (540/540 | Eval metrics: {'accuracy': 0.6}) \n",
            "Training...: 100%|██████████████████████████████| 18/18 [00:02<00:00,  8.06it/s]\n",
            "Epoch ... 30/30: 100%|██████████████████████████| 30/30 [02:11<00:00,  4.39s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 8/8 [00:00<00:00, 13.24it/s]\n",
            "INFO:__main__: test results : {'test_accuracy': 0.6959999799728394}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_seq.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/seqcls/BIOSSES_hf/train.json \\\n",
        "--validation_file data/seqcls/BIOSSES_hf/dev.json \\\n",
        "--test_file data/seqcls/BIOSSES_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 4 \\\n",
        "--per_device_eval_batch_size 8 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 30 \\\n",
        "--max_seq_length 512 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--eval_steps 100000 \\\n",
        "--metric_name pearsonr \\\n",
        "--blurb_task BIOSSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8mIMcnDCJZX",
        "outputId": "041f3cf2-1a6d-4337-cddb-ec09c1cfff59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-6281a72ec163028f\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 900.84it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForSequenceClassification: {('discriminator_predictions', 'dense_prediction', 'bias'), ('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'dense', 'bias'), ('classifier', 'dense', 'kernel'), ('classifier', 'out_proj', 'bias'), ('classifier', 'out_proj', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6691b1f382fcbeb6.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cbc66a41669ee30f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/big35manf/.cache/huggingface/datasets/json/default-6281a72ec163028f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-60adef871b406c5b.arrow\n",
            "INFO:__main__:Sample 39 of the training set: {'input_ids': [2, 13506, 1744, 42, 28366, 2377, 1760, 6781, 3028, 1816, 3390, 17, 3, 2661, 2223, 1953, 3758, 42, 2534, 1685, 17387, 4040, 1760, 2664, 1701, 1765, 6057, 1725, 42, 2310, 1701, 6942, 15, 16412, 7825, 5499, 1680, 7955, 1682, 2582, 4200, 1682, 42, 4850, 1685, 21697, 15, 6993, 5003, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1.0}.\n",
            "INFO:__main__:Sample 23 of the training set: {'input_ids': [2, 1682, 2401, 15, 3056, 1690, 2913, 2223, 2393, 1760, 9748, 12916, 15883, 14296, 1015, 2101, 1021, 4295, 4485, 1701, 9483, 6579, 15, 1798, 3617, 1682, 3100, 15, 1772, 8037, 9994, 7458, 2108, 17, 3, 1682, 2401, 15, 2177, 16, 3675, 1685, 3863, 16, 2721, 2029, 2082, 2030, 2336, 1682, 4514, 1690, 5069, 5711, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0.2}.\n",
            "INFO:__main__:Sample 45 of the training set: {'input_ids': [2, 1680, 6467, 1971, 1014, 15, 6873, 24549, 1010, 11, 2225, 1013, 16, 2138, 2611, 12, 15, 1690, 228, 16, 5594, 1748, 6594, 2189, 1732, 3958, 1685, 1680, 6467, 3293, 1023, 18, 26810, 16, 3223, 6873, 1032, 8557, 16, 8553, 2586, 17, 3, 1682, 2401, 15, 1971, 18355, 16, 2374, 6873, 24549, 1010, 1690, 228, 16, 5594, 1810, 3958, 1685, 1680, 2323, 3293, 1023, 18, 26810, 2586, 1690, 2509, 3568, 42, 2467, 1682, 2233, 3033, 16, 4655, 2724, 1701, 1680, 8557, 18, 4253, 4452, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 3.4}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (30 epochs) =====\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:05<00:00,  5.49s/it]\n",
            "INFO:__main__:{'eval_pearsonr': -0.3750147682204343}| Step... (2/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:58<00:00, 29.25s/it]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.64it/s]\n",
            "INFO:__main__:{'eval_pearsonr': -0.24263143828199263}| Step... (4/60 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.82it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.64it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.2580326069789202}| Step... (6/60 | Eval metrics: {'accuracy': 0.125}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.78it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.68it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.5951140537181692}| Step... (8/60 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.69it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6695844489243072}| Step... (10/60 | Eval metrics: {'accuracy': 0.1875}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.85it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.66it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.6990661768849551}| Step... (12/60 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.96it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.75it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7060606069476574}| Step... (14/60 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.69it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7147659116711338}| Step... (16/60 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.83it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.76it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7294169329405529}| Step... (18/60 | Eval metrics: {'accuracy': 0.25}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.95it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.74it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7436876618760784}| Step... (20/60 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.83it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.71it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7535326223606816}| Step... (22/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.85it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.74it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7566276207262954}| Step... (24/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.74it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.758550930233962}| Step... (26/60 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.82it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.73it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7730140765175206}| Step... (28/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.76it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.72it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7825916345317175}| Step... (30/60 | Eval metrics: {'accuracy': 0.625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.72it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7808998630489213}| Step... (32/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.90it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.84it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7720953909999082}| Step... (34/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.63it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 11.12it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.781272859158022}| Step... (36/60 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.96it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.76it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7919324309731819}| Step... (38/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.78it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.75it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.798281763265226}| Step... (40/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.89it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.77it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7981231568239807}| Step... (42/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.94it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.78it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7905981314992301}| Step... (44/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  4.01it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.75it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7858486754858294}| Step... (46/60 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.77it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.75it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.780851856106572}| Step... (48/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.86it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.79it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7796619863307972}| Step... (50/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.98it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.73it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7795783378702836}| Step... (52/60 | Eval metrics: {'accuracy': 0.4375}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.88it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.74it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.781646989695766}| Step... (54/60 | Eval metrics: {'accuracy': 0.5}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.94it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.78it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7825186307649399}| Step... (56/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.91it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 10.78it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7815922780048179}| Step... (58/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.79it/s]\n",
            "Evaluating ...: 100%|█████████████████████████████| 1/1 [00:00<00:00, 11.39it/s]\n",
            "INFO:__main__:{'eval_pearsonr': 0.7818495204153236}| Step... (60/60 | Eval metrics: {'accuracy': 0.5625}) \n",
            "Training...: 100%|████████████████████████████████| 2/2 [00:00<00:00,  3.97it/s]\n",
            "Epoch ... 30/30: 100%|██████████████████████████| 30/30 [01:13<00:00,  2.45s/it]\n",
            "Evaluating on Test Data ...: 100%|████████████████| 1/1 [00:00<00:00, 19.72it/s]\n",
            "INFO:__main__: test results : {'test_pearsonr': 0.8966910113799689}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/tokcls/BC5CDR-chem_hf/train.json \\\n",
        "--validation_file data/tokcls/BC5CDR-chem_hf/dev.json \\\n",
        "--test_file data/tokcls/BC5CDR-chem_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC5CDR-chem \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 10000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89f581d-857e-4210-f838-b5755475aa60",
        "id": "DxlxBF2b18OY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-a8f8745a9e3dc8a7\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-a8f8745a9e3dc8a7/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 744.15it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense_prediction', 'bias')}\n",
            "- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  3.18ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.79ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  3.01ba/s]\n",
            "INFO:__main__:Sample 4200 of the training set: {'input_ids': [2, 1680, 2587, 1685, 1805, 1901, 1734, 1701, 3240, 1680, 2953, 1955, 19834, 5712, 1690, 9630, 8782, 1682, 1808, 2386, 2105, 5452, 2121, 15016, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 504 of the training set: {'input_ids': [2, 3586, 19530, 1690, 1680, 2209, 1685, 4507, 5703, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, 1, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 1928 of the training set: {'input_ids': [2, 1680, 6361, 2727, 2804, 1685, 25224, 6680, 3921, 1034, 15, 2630, 17052, 1690, 14619, 17511, 5368, 2602, 1748, 5539, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 0, -100, -100, -100, 2, 0, -100, 2, 0, -100, -100, -100, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (20 epochs) =====\n",
            "Step... (500/11400 | Training Loss: 0.0001674318191362545, Learning Rate: 3.824912346317433e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:12<00:00,  5.57it/s]\n",
            "INFO:__main__:Step... (570/11400 | Validation metrics: {'__precision': 0.9202645599853022, '__recall': 0.9367869833551524, '__f1': 0.9284522706209453, '__number': 5347, 'overall_precision': 0.9202645599853022, 'overall_recall': 0.9367869833551524, 'overall_f1': 0.9284522706209453, 'overall_accuracy': 0.9921841076856275}\n",
            "Training...: 100%|████████████████████████████| 570/570 [01:53<00:00,  5.01it/s]\n",
            "Step... (1000/11400 | Training Loss: 1.1869837180711329e-05, Learning Rate: 3.6494737287284806e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.80it/s]\n",
            "INFO:__main__:Step... (1140/11400 | Validation metrics: {'__precision': 0.9419468334636435, '__recall': 0.9012530390873387, '__f1': 0.921150721590366, '__number': 5347, 'overall_precision': 0.9419468334636435, 'overall_recall': 0.9012530390873387, 'overall_f1': 0.921150721590366, 'overall_accuracy': 0.9898853158284591}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.34it/s]\n",
            "Step... (1500/11400 | Training Loss: 4.107940185349435e-06, Learning Rate: 3.4740351111395285e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.71it/s]\n",
            "INFO:__main__:Step... (1710/11400 | Validation metrics: {'__precision': 0.9294205052005944, '__recall': 0.935851879558631, '__f1': 0.9326251048364551, '__number': 5347, 'overall_precision': 0.9294205052005944, 'overall_recall': 0.935851879558631, 'overall_f1': 0.9326251048364551, 'overall_accuracy': 0.9914604139528151}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.18it/s]\n",
            "Step... (2000/11400 | Training Loss: 0.00010184480925090611, Learning Rate: 3.2985961297526956e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.75it/s]\n",
            "INFO:__main__:Step... (2280/11400 | Validation metrics: {'__precision': 0.9392193308550186, '__recall': 0.9450158967645409, '__f1': 0.9421086976787546, '__number': 5347, 'overall_precision': 0.9392193308550186, 'overall_recall': 0.9450158967645409, 'overall_f1': 0.9421086976787546, 'overall_accuracy': 0.9923629026078516}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.32it/s]\n",
            "Step... (2500/11400 | Training Loss: 7.547827863163548e-06, Learning Rate: 3.123157875961624e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.72it/s]\n",
            "INFO:__main__:Step... (2850/11400 | Validation metrics: {'__precision': 0.9250970963565748, '__recall': 0.9354778380400225, '__f1': 0.9302585084619677, '__number': 5347, 'overall_precision': 0.9250970963565748, 'overall_recall': 0.9354778380400225, 'overall_f1': 0.9302585084619677, 'overall_accuracy': 0.9911113381522821}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.34it/s]\n",
            "Step... (3000/11400 | Training Loss: 1.6207222870434634e-06, Learning Rate: 2.9477190764737315e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.79it/s]\n",
            "INFO:__main__:Step... (3420/11400 | Validation metrics: {'__precision': 0.8941032798325191, '__recall': 0.9584813914344492, '__f1': 0.9251737521436952, '__number': 5347, 'overall_precision': 0.8941032798325191, 'overall_recall': 0.9584813914344492, 'overall_f1': 0.9251737521436952, 'overall_accuracy': 0.9904217005951317}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.33it/s]\n",
            "Step... (3500/11400 | Training Loss: 4.3339912281226134e-07, Learning Rate: 2.7722806407837197e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.85it/s]\n",
            "INFO:__main__:Step... (3990/11400 | Validation metrics: {'__precision': 0.9303354220420199, '__recall': 0.9440807929680195, '__f1': 0.9371577090875336, '__number': 5347, 'overall_precision': 0.9303354220420199, 'overall_recall': 0.9440807929680195, 'overall_f1': 0.9371577090875336, 'overall_accuracy': 0.9919882846755723}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.40it/s]\n",
            "Step... (4000/11400 | Training Loss: 2.8162466492176463e-07, Learning Rate: 2.5968420231947675e-05)\n",
            "Step... (4500/11400 | Training Loss: 1.8303620663573383e-06, Learning Rate: 2.4214034056058154e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.91it/s]\n",
            "INFO:__main__:Step... (4560/11400 | Validation metrics: {'__precision': 0.9327512539476128, '__recall': 0.9390312324668039, '__f1': 0.9358807082945014, '__number': 5347, 'overall_precision': 0.9327512539476128, 'overall_recall': 0.9390312324668039, 'overall_f1': 0.9358807082945014, 'overall_accuracy': 0.9918435459290099}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.41it/s]\n",
            "Step... (5000/11400 | Training Loss: 9.970803148462437e-06, Learning Rate: 2.2459649699158035e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.89it/s]\n",
            "INFO:__main__:Step... (5130/11400 | Validation metrics: {'__precision': 0.9281404278661547, '__recall': 0.9493173742285393, '__f1': 0.9386094674556213, '__number': 5347, 'overall_precision': 0.9281404278661547, 'overall_recall': 0.9493173742285393, 'overall_f1': 0.9386094674556213, 'overall_accuracy': 0.9921415374660503}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.24it/s]\n",
            "Step... (5500/11400 | Training Loss: 3.1111576390685514e-05, Learning Rate: 2.0705259885289706e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.95it/s]\n",
            "INFO:__main__:Step... (5700/11400 | Validation metrics: {'__precision': 0.9129572896017301, '__recall': 0.9474471666354966, '__f1': 0.9298825256975036, '__number': 5347, 'overall_precision': 0.9129572896017301, 'overall_recall': 0.9474471666354966, 'overall_f1': 0.9298825256975036, 'overall_accuracy': 0.9913412173379991}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.40it/s]\n",
            "Step... (6000/11400 | Training Loss: 1.4889707244947203e-06, Learning Rate: 1.895087734737899e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.66it/s]\n",
            "INFO:__main__:Step... (6270/11400 | Validation metrics: {'__precision': 0.9294399410464259, '__recall': 0.9435197306901066, '__f1': 0.9364269141531323, '__number': 5347, 'overall_precision': 0.9294399410464259, 'overall_recall': 0.9435197306901066, 'overall_f1': 0.9364269141531323, 'overall_accuracy': 0.9919372004120798}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.12it/s]\n",
            "Step... (6500/11400 | Training Loss: 1.8142763735795597e-07, Learning Rate: 1.719649117148947e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.78it/s]\n",
            "INFO:__main__:Step... (6840/11400 | Validation metrics: {'__precision': 0.9304556354916067, '__recall': 0.9433327099308023, '__f1': 0.9368499257057948, '__number': 5347, 'overall_precision': 0.9304556354916067, 'overall_recall': 0.9433327099308023, 'overall_f1': 0.9368499257057948, 'overall_accuracy': 0.9921415374660503}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.24it/s]\n",
            "Step... (7000/11400 | Training Loss: 4.4674322907667374e-07, Learning Rate: 1.5442103176610544e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.69it/s]\n",
            "INFO:__main__:Step... (7410/11400 | Validation metrics: {'__precision': 0.9463572379684231, '__recall': 0.9304282775388069, '__f1': 0.9383251603168615, '__number': 5347, 'overall_precision': 0.9463572379684231, 'overall_recall': 0.9304282775388069, 'overall_f1': 0.9383251603168615, 'overall_accuracy': 0.9922437059930355}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.34it/s]\n",
            "Step... (7500/11400 | Training Loss: 0.0008872230537235737, Learning Rate: 1.3687717910215724e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.81it/s]\n",
            "INFO:__main__:Step... (7980/11400 | Validation metrics: {'__precision': 0.9434069043576684, '__recall': 0.9352908172807182, '__f1': 0.9393313298271975, '__number': 5347, 'overall_precision': 0.9434069043576684, 'overall_recall': 0.9352908172807182, 'overall_f1': 0.9393313298271975, 'overall_accuracy': 0.9924395290030906}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.30it/s]\n",
            "Step... (8000/11400 | Training Loss: 4.869456233791425e-07, Learning Rate: 1.19333308248315e-05)\n",
            "Step... (8500/11400 | Training Loss: 1.6837676639624988e-06, Learning Rate: 1.0178944648941979e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.59it/s]\n",
            "INFO:__main__:Step... (8550/11400 | Validation metrics: {'__precision': 0.9342670401493931, '__recall': 0.9356648587993267, '__f1': 0.9349654270229864, '__number': 5347, 'overall_precision': 0.9342670401493931, 'overall_recall': 0.9356648587993267, 'overall_f1': 0.9349654270229864, 'overall_accuracy': 0.9916221807872085}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.24it/s]\n",
            "Step... (9000/11400 | Training Loss: 2.772190157429577e-07, Learning Rate: 8.424561201536562e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.63it/s]\n",
            "INFO:__main__:Step... (9120/11400 | Validation metrics: {'__precision': 0.9394618834080718, '__recall': 0.9403403777819338, '__f1': 0.9399009253201234, '__number': 5347, 'overall_precision': 0.9394618834080718, 'overall_recall': 0.9403403777819338, 'overall_f1': 0.9399009253201234, 'overall_accuracy': 0.9923458745200208}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.18it/s]\n",
            "Step... (9500/11400 | Training Loss: 5.5762054529395755e-08, Learning Rate: 6.6701745708996896e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.60it/s]\n",
            "INFO:__main__:Step... (9690/11400 | Validation metrics: {'__precision': 0.9355197331851028, '__recall': 0.9442678137273237, '__f1': 0.939873417721519, '__number': 5347, 'overall_precision': 0.9355197331851028, 'overall_recall': 0.9442678137273237, 'overall_f1': 0.939873417721519, 'overall_accuracy': 0.9922692481247818}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.20it/s]\n",
            "Step... (10000/11400 | Training Loss: 5.599501307074206e-08, Learning Rate: 4.915787940262817e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.64it/s]\n",
            "INFO:__main__:Step... (10260/11400 | Validation metrics: {'__precision': 0.9390721073225266, '__recall': 0.9425846268935852, '__f1': 0.9408250886690311, '__number': 5347, 'overall_precision': 0.9390721073225266, 'overall_recall': 0.9425846268935852, 'overall_f1': 0.9408250886690311, 'overall_accuracy': 0.9924054728274289}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.21it/s]\n",
            "Step... (10500/11400 | Training Loss: 8.140132479184103e-08, Learning Rate: 3.161401764373295e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.55it/s]\n",
            "INFO:__main__:Step... (10830/11400 | Validation metrics: {'__precision': 0.93587842846553, '__recall': 0.944454834486628, '__f1': 0.9401470725123336, '__number': 5347, 'overall_precision': 0.93587842846553, 'overall_recall': 0.944454834486628, 'overall_f1': 0.9401470725123336, 'overall_accuracy': 0.9923969587835134}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:43<00:00, 13.07it/s]\n",
            "Step... (11000/11400 | Training Loss: 4.982591406132997e-08, Learning Rate: 1.40701524742326e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.63it/s]\n",
            "INFO:__main__:Step... (11400/11400 | Validation metrics: {'__precision': 0.936351827797365, '__recall': 0.9437067514494109, '__f1': 0.9400149031296572, '__number': 5347, 'overall_precision': 0.936351827797365, 'overall_recall': 0.9437067514494109, 'overall_f1': 0.9400149031296572, 'overall_accuracy': 0.992388444739598}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.29it/s]\n",
            "Epoch ... 20/20: 100%|██████████████████████████| 20/20 [15:30<00:00, 46.51s/it]\n",
            "Evaluating on Dev Set ...: 72it [00:07,  9.73it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.936351827797365, '__recall': 0.9437067514494109, '__f1': 0.9400149031296572, '__number': 5347, 'overall_precision': 0.936351827797365, 'overall_recall': 0.9437067514494109, 'overall_f1': 0.9400149031296572, 'overall_accuracy': 0.992388444739598}\n",
            "Evaluating on Test Set...: 75it [00:07,  9.72it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.928834808259587, '__recall': 0.9355617455896007, '__f1': 0.9321861411786475, '__number': 5385, 'overall_precision': 0.928834808259587, 'overall_recall': 0.9355617455896007, 'overall_f1': 0.9321861411786475, 'overall_accuracy': 0.9916472945891783}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/tokcls/BC5CDR-disease_hf/train.json \\\n",
        "--validation_file data/tokcls/BC5CDR-disease_hf/dev.json \\\n",
        "--test_file data/tokcls/BC5CDR-disease_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 7e-5 \\\n",
        "--num_train_epochs 7 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC5-disease \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39a4b69-d6de-48ef-c659-5f54c628fc8e",
        "id": "X3ZBVdEx18OZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-741663b27c9f6158\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-741663b27c9f6158/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 687.78it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  3.17ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.80ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 5/5 [00:01<00:00,  2.99ba/s]\n",
            "INFO:__main__:Sample 1591 of the training set: {'input_ids': [2, 3058, 24833, 4042, 1888, 9233, 1680, 24986, 3296, 4246, 2228, 4328, 1772, 20440, 4299, 15, 2021, 2029, 2030, 10031, 42, 10, 2175, 16, 16413, 7759, 2445, 10, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, -100, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 2193 of the training set: {'input_ids': [2, 10248, 4705, 2031, 6959, 2029, 2030, 2336, 1970, 6396, 1814, 20440, 4299, 1701, 6467, 2865, 1685, 16051, 11, 12742, 12, 4299, 15, 3882, 24833, 15, 1682, 3296, 3650, 7100, 1715, 2961, 11044, 11160, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 2879 of the training set: {'input_ids': [2, 3950, 2228, 10118, 1701, 2254, 1035, 2888, 1705, 2990, 2762, 17074, 11, 3698, 8, 12, 15, 6886, 9989, 11, 2073, 8, 12, 15, 5253, 3113, 11, 3184, 8, 12, 15, 1690, 6141, 3037, 11, 2651, 8, 12, 30, 2254, 1035, 2888, 1705, 8036, 1734, 2716, 1682, 4289, 8, 1685, 1808, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, -100, -100, -100, 2, 0, 1, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (7 epochs) =====\n",
            "Step... (500/3990 | Training Loss: 0.0022799272555857897, Learning Rate: 6.124561332399026e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:12<00:00,  5.60it/s]\n",
            "Training...: 100%|███████████████████████████▉| 568/570 [01:57<00:00, 16.83it/s]INFO:__main__:Step... (570/3990 | Validation metrics: {'__precision': 0.7312220868859115, '__recall': 0.8483278379651437, '__f1': 0.7854339293501963, '__number': 4246, 'overall_precision': 0.7312220868859115, 'overall_recall': 0.8483278379651437, 'overall_f1': 0.7854339293501963, 'overall_accuracy': 0.9773356150971027}\n",
            "Training...: 100%|████████████████████████████| 570/570 [01:58<00:00,  4.80it/s]\n",
            "Step... (1000/3990 | Training Loss: 0.0005363335949368775, Learning Rate: 5.2473686082521453e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.67it/s]\n",
            "INFO:__main__:Step... (1000/3990 | Validation metrics: {'__precision': 0.8397435897435898, '__recall': 0.8330193122939237, '__f1': 0.8363679356821944, '__number': 4246, 'overall_precision': 0.8397435897435898, 'overall_recall': 0.8330193122939237, 'overall_f1': 0.8363679356821944, 'overall_accuracy': 0.9836870918580198}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.83it/s]\n",
            "INFO:__main__:Step... (1140/3990 | Validation metrics: {'__precision': 0.8349078885214927, '__recall': 0.8325482807348092, '__f1': 0.8337264150943396, '__number': 4246, 'overall_precision': 0.8349078885214927, 'overall_recall': 0.8325482807348092, 'overall_f1': 0.8337264150943396, 'overall_accuracy': 0.9836445216384426}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:51<00:00, 11.02it/s]\n",
            "Step... (1500/3990 | Training Loss: 0.0007472452707588673, Learning Rate: 4.3701755203073844e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.69it/s]\n",
            "INFO:__main__:Step... (1710/3990 | Validation metrics: {'__precision': 0.815236818588025, '__recall': 0.8593970796043335, '__f1': 0.8367346938775511, '__number': 4246, 'overall_precision': 0.815236818588025, 'overall_recall': 0.8593970796043335, 'overall_f1': 0.8367346938775511, 'overall_accuracy': 0.983380586277064}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.28it/s]\n",
            "Step... (2000/3990 | Training Loss: 5.800432700198144e-05, Learning Rate: 3.492982796160504e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.77it/s]\n",
            "INFO:__main__:Step... (2000/3990 | Validation metrics: {'__precision': 0.8416744621141253, '__recall': 0.847621290626472, '__f1': 0.8446374090589063, '__number': 4246, 'overall_precision': 0.8416744621141253, 'overall_recall': 0.847621290626472, 'overall_f1': 0.8446374090589063, 'overall_accuracy': 0.9845470102934791}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.81it/s]\n",
            "INFO:__main__:Step... (2280/3990 | Validation metrics: {'__precision': 0.8606046065259118, '__recall': 0.8447951012717853, '__f1': 0.8526265747563585, '__number': 4246, 'overall_precision': 0.8606046065259118, 'overall_recall': 0.8447951012717853, 'overall_f1': 0.8526265747563585, 'overall_accuracy': 0.985398414685023}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:51<00:00, 11.08it/s]\n",
            "Step... (2500/3990 | Training Loss: 4.856973646383267e-06, Learning Rate: 2.6157897082157433e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.85it/s]\n",
            "INFO:__main__:Step... (2850/3990 | Validation metrics: {'__precision': 0.8370812299219825, '__recall': 0.8591615638247763, '__f1': 0.8479776847977685, '__number': 4246, 'overall_precision': 0.8370812299219825, 'overall_recall': 0.8591615638247763, 'overall_f1': 0.8479776847977685, 'overall_accuracy': 0.984896086094012}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.35it/s]\n",
            "Step... (3000/3990 | Training Loss: 0.0002505084266886115, Learning Rate: 1.7385966202709824e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.68it/s]\n",
            "INFO:__main__:Step... (3000/3990 | Validation metrics: {'__precision': 0.8428933425478, '__recall': 0.8617522373999058, '__f1': 0.8522184697798998, '__number': 4246, 'overall_precision': 0.8428933425478, 'overall_recall': 0.8617522373999058, 'overall_f1': 0.8522184697798998, 'overall_accuracy': 0.9850152827088282}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.78it/s]\n",
            "INFO:__main__:Step... (3420/3990 | Validation metrics: {'__precision': 0.8453488372093023, '__recall': 0.8560998586905323, '__f1': 0.8506903814650129, '__number': 4246, 'overall_precision': 0.8453488372093023, 'overall_recall': 0.8560998586905323, 'overall_f1': 0.8506903814650129, 'overall_accuracy': 0.9853728725532767}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:51<00:00, 10.99it/s]\n",
            "Step... (3500/3990 | Training Loss: 1.2984996828890871e-06, Learning Rate: 8.614036232756916e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 72/72 [00:07<00:00,  9.74it/s]\n",
            "INFO:__main__:Step... (3990/3990 | Validation metrics: {'__precision': 0.8484356894553882, '__recall': 0.8622232689590202, '__f1': 0.8552739165985282, '__number': 4246, 'overall_precision': 0.8484356894553882, 'overall_recall': 0.8622232689590202, 'overall_f1': 0.8552739165985282, 'overall_accuracy': 0.9851855635871369}\n",
            "Training...: 100%|████████████████████████████| 570/570 [00:42<00:00, 13.33it/s]\n",
            "Epoch ... 7/7: 100%|██████████████████████████████| 7/7 [06:42<00:00, 57.48s/it]\n",
            "Evaluating on Dev Set ...: 72it [00:07,  9.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8484356894553882, '__recall': 0.8622232689590202, '__f1': 0.8552739165985282, '__number': 4246, 'overall_precision': 0.8484356894553882, 'overall_recall': 0.8622232689590202, 'overall_f1': 0.8552739165985282, 'overall_accuracy': 0.9851855635871369}\n",
            "Evaluating on Test Set...: 75it [00:07,  9.76it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8315404475043029, '__recall': 0.8736437613019892, '__f1': 0.8520723104056437, '__number': 4424, 'overall_precision': 0.8315404475043029, 'overall_recall': 0.8736437613019892, 'overall_f1': 0.8520723104056437, 'overall_accuracy': 0.9851462925851704}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/tokcls/BC2GM_hf/train.json \\\n",
        "--validation_file data/tokcls/BC2GM_hf/dev.json \\\n",
        "--test_file data/tokcls/BC2GM_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--learning_rate 4e-5 \\\n",
        "--num_train_epochs 20 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task BC2GM \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6efe6b50-b44e-4765-ed82-dd3242e19936",
        "id": "95YE3CQ618OZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1924c477ac5488ad\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-1924c477ac5488ad/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 706.23it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('electra', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 13/13 [00:04<00:00,  2.84ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00,  3.38ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 6/6 [00:01<00:00,  3.36ba/s]\n",
            "INFO:__main__:Sample 5860 of the training set: {'input_ids': [2, 1682, 1805, 3618, 1680, 6671, 18157, 1963, 4810, 1862, 4673, 15, 1680, 2031, 16, 6671, 21794, 11, 19024, 1026, 12, 15, 1810, 2801, 1950, 1883, 1765, 12511, 2295, 2033, 28398, 15, 1680, 5697, 19024, 1026, 1690, 1680, 2767, 19024, 1026, 11, 6565, 5599, 1026, 12, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, -100, -100, -100, 2, 2, 2, 0, 1, 1, 1, 2, 0, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, -100, 2, 2, 0, 1, -100, 2, 0, -100, -100, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 3411 of the training set: {'input_ids': [2, 1682, 2651, 8, 28890, 10040, 1734, 6072, 2595, 1690, 3215, 3055, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 286 of the training set: {'input_ids': [2, 20, 12, 5097, 1022, 1024, 15, 6532, 1015, 1690, 5710, 1035, 2229, 42, 5387, 6785, 2169, 3012, 49, 17, 8058, 15, 1690, 3882, 15, 6532, 1015, 2229, 42, 4096, 15070, 2169, 1682, 42, 3274, 2194, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, -100, -100, 2, 2, -100, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (20 epochs) =====\n",
            "Step... (500/15700 | Training Loss: 0.0015996170695871115, Learning Rate: 3.8728660001652315e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:09<00:00,  4.10it/s]\n",
            "INFO:__main__:Step... (785/15700 | Validation metrics: {'__precision': 0.778263585259213, '__recall': 0.8141130349558968, '__f1': 0.7957847676832189, '__number': 3061, 'overall_precision': 0.778263585259213, 'overall_recall': 0.8141130349558968, 'overall_f1': 0.7957847676832189, 'overall_accuracy': 0.978153768193463}\n",
            "Training...: 100%|████████████████████████████| 785/785 [02:18<00:00,  5.68it/s]\n",
            "Step... (1000/15700 | Training Loss: 0.0035082658287137747, Learning Rate: 3.745477442862466e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.26it/s]\n",
            "INFO:__main__:Step... (1000/15700 | Validation metrics: {'__precision': 0.8135483870967742, '__recall': 0.8239137536752695, '__f1': 0.8186982632689499, '__number': 3061, 'overall_precision': 0.8135483870967742, 'overall_recall': 0.8239137536752695, 'overall_f1': 0.8186982632689499, 'overall_accuracy': 0.9793220911573436}\n",
            "Step... (1500/15700 | Training Loss: 0.0036001880653202534, Learning Rate: 3.618089249357581e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.38it/s]\n",
            "INFO:__main__:Step... (1570/15700 | Validation metrics: {'__precision': 0.7856499549684779, '__recall': 0.8549493629532833, '__f1': 0.8188360450563203, '__number': 3061, 'overall_precision': 0.7856499549684779, 'overall_recall': 0.8549493629532833, 'overall_f1': 0.8188360450563203, 'overall_accuracy': 0.9782241490949016}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.17it/s]\n",
            "Step... (2000/15700 | Training Loss: 0.0012048137141391635, Learning Rate: 3.4907006920548156e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.38it/s]\n",
            "INFO:__main__:Step... (2000/15700 | Validation metrics: {'__precision': 0.8332783096731594, '__recall': 0.8245671349232278, '__f1': 0.8288998357963874, '__number': 3061, 'overall_precision': 0.8332783096731594, 'overall_recall': 0.8245671349232278, 'overall_f1': 0.8288998357963874, 'overall_accuracy': 0.9789842628304384}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.38it/s]\n",
            "INFO:__main__:Step... (2355/15700 | Validation metrics: {'__precision': 0.8263123784834737, '__recall': 0.8330610911466841, '__f1': 0.8296730112249878, '__number': 3061, 'overall_precision': 0.8263123784834737, 'overall_recall': 0.8330610911466841, 'overall_f1': 0.8296730112249878, 'overall_accuracy': 0.9793924720587821}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:16<00:00, 10.20it/s]\n",
            "Step... (2500/15700 | Training Loss: 0.001633170642890036, Learning Rate: 3.36331213475205e-05)\n",
            "Step... (3000/15700 | Training Loss: 0.0018413268262520432, Learning Rate: 3.2359235774492845e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.33it/s]\n",
            "INFO:__main__:Step... (3000/15700 | Validation metrics: {'__precision': 0.8136422976501305, '__recall': 0.8144397255798759, '__f1': 0.8140408163265306, '__number': 3061, 'overall_precision': 0.8136422976501305, 'overall_recall': 0.8144397255798759, 'overall_f1': 0.8140408163265306, 'overall_accuracy': 0.9779567016694349}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.35it/s]\n",
            "INFO:__main__:Step... (3140/15700 | Validation metrics: {'__precision': 0.8131937836980654, '__recall': 0.8376347598823913, '__f1': 0.8252333440617958, '__number': 3061, 'overall_precision': 0.8131937836980654, 'overall_recall': 0.8376347598823913, 'overall_f1': 0.8252333440617958, 'overall_accuracy': 0.9787871963064103}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.18it/s]\n",
            "Step... (3500/15700 | Training Loss: 0.0018148504896089435, Learning Rate: 3.108535020146519e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.37it/s]\n",
            "INFO:__main__:Step... (3925/15700 | Validation metrics: {'__precision': 0.8141536499840613, '__recall': 0.8343678536426005, '__f1': 0.8241368183284931, '__number': 3061, 'overall_precision': 0.8141536499840613, 'overall_recall': 0.8343678536426005, 'overall_f1': 0.8241368183284931, 'overall_accuracy': 0.9785338250612314}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:12<00:00, 10.80it/s]\n",
            "Step... (4000/15700 | Training Loss: 0.0007356045534834266, Learning Rate: 2.9811464628437534e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.42it/s]\n",
            "INFO:__main__:Step... (4000/15700 | Validation metrics: {'__precision': 0.8054054054054054, '__recall': 0.8275073505390396, '__f1': 0.8163067998710924, '__number': 3061, 'overall_precision': 0.8054054054054054, 'overall_recall': 0.8275073505390396, 'overall_f1': 0.8163067998710924, 'overall_accuracy': 0.9781678443737507}\n",
            "Step... (4500/15700 | Training Loss: 0.0003625218232627958, Learning Rate: 2.8537577236420475e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.30it/s]\n",
            "INFO:__main__:Step... (4710/15700 | Validation metrics: {'__precision': 0.826784544859201, '__recall': 0.8248938255472068, '__f1': 0.8258381030253475, '__number': 3061, 'overall_precision': 0.826784544859201, 'overall_recall': 0.8248938255472068, 'overall_f1': 0.8258381030253475, 'overall_accuracy': 0.9794065482390698}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.18it/s]\n",
            "Step... (5000/15700 | Training Loss: 0.00014490747707895935, Learning Rate: 2.7263693482382223e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.41it/s]\n",
            "INFO:__main__:Step... (5000/15700 | Validation metrics: {'__precision': 0.8192429022082018, '__recall': 0.8484155504737014, '__f1': 0.8335740651580805, '__number': 3061, 'overall_precision': 0.8192429022082018, 'overall_recall': 0.8484155504737014, 'overall_f1': 0.8335740651580805, 'overall_accuracy': 0.9796176909433856}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.41it/s]\n",
            "INFO:__main__:Step... (5495/15700 | Validation metrics: {'__precision': 0.8157147365099401, '__recall': 0.8444952629859523, '__f1': 0.8298555377207063, '__number': 3061, 'overall_precision': 0.8157147365099401, 'overall_recall': 0.8444952629859523, 'overall_f1': 0.8298555377207063, 'overall_accuracy': 0.9792235578953296}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.19it/s]\n",
            "Step... (5500/15700 | Training Loss: 4.703642116510309e-05, Learning Rate: 2.5989807909354568e-05)\n",
            "Step... (6000/15700 | Training Loss: 7.47113226680085e-05, Learning Rate: 2.4715924155316316e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.09it/s]\n",
            "INFO:__main__:Step... (6000/15700 | Validation metrics: {'__precision': 0.8290322580645161, '__recall': 0.8395949036262659, '__f1': 0.834280149326408, '__number': 3061, 'overall_precision': 0.8290322580645161, 'overall_recall': 0.8395949036262659, 'overall_f1': 0.834280149326408, 'overall_accuracy': 0.9803778046789223}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.23it/s]\n",
            "INFO:__main__:Step... (6280/15700 | Validation metrics: {'__precision': 0.8153266331658291, '__recall': 0.8480888598497223, '__f1': 0.8313851080864691, '__number': 3061, 'overall_precision': 0.8153266331658291, 'overall_recall': 0.8480888598497223, 'overall_f1': 0.8313851080864691, 'overall_accuracy': 0.9793783958784944}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:16<00:00, 10.23it/s]\n",
            "Step... (6500/15700 | Training Loss: 1.757363133947365e-05, Learning Rate: 2.344203858228866e-05)\n",
            "Step... (7000/15700 | Training Loss: 3.118698441539891e-05, Learning Rate: 2.2168154828250408e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.24it/s]\n",
            "INFO:__main__:Step... (7000/15700 | Validation metrics: {'__precision': 0.8088188976377952, '__recall': 0.8389415223783078, '__f1': 0.8236048749198204, '__number': 3061, 'overall_precision': 0.8088188976377952, 'overall_recall': 0.8389415223783078, 'overall_f1': 0.8236048749198204, 'overall_accuracy': 0.9786182821429577}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.37it/s]\n",
            "INFO:__main__:Step... (7065/15700 | Validation metrics: {'__precision': 0.8096266584387535, '__recall': 0.8572361973211369, '__f1': 0.8327515074579498, '__number': 3061, 'overall_precision': 0.8096266584387535, 'overall_recall': 0.8572361973211369, 'overall_f1': 0.8327515074579498, 'overall_accuracy': 0.9799132907294277}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.12it/s]\n",
            "Step... (7500/15700 | Training Loss: 7.7475360740209e-06, Learning Rate: 2.0894269255222753e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.28it/s]\n",
            "INFO:__main__:Step... (7850/15700 | Validation metrics: {'__precision': 0.8201393286890437, '__recall': 0.8461287161058477, '__f1': 0.8329313394436404, '__number': 3061, 'overall_precision': 0.8201393286890437, 'overall_recall': 0.8461287161058477, 'overall_f1': 0.8329313394436404, 'overall_accuracy': 0.9795895385828102}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:12<00:00, 10.88it/s]\n",
            "Step... (8000/15700 | Training Loss: 6.095258140703663e-05, Learning Rate: 1.9620381863205694e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.22it/s]\n",
            "INFO:__main__:Step... (8000/15700 | Validation metrics: {'__precision': 0.8218572331017057, '__recall': 0.8500490035935969, '__f1': 0.8357154327926771, '__number': 3061, 'overall_precision': 0.8218572331017057, 'overall_recall': 0.8500490035935969, 'overall_f1': 0.8357154327926771, 'overall_accuracy': 0.9799414430900031}\n",
            "Step... (8500/15700 | Training Loss: 1.2454584066290408e-05, Learning Rate: 1.8346498109167442e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.31it/s]\n",
            "INFO:__main__:Step... (8635/15700 | Validation metrics: {'__precision': 0.827120822622108, '__recall': 0.8409016661221823, '__f1': 0.8339543171877531, '__number': 3061, 'overall_precision': 0.827120822622108, 'overall_recall': 0.8409016661221823, 'overall_f1': 0.8339543171877531, 'overall_accuracy': 0.9797866051068382}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.13it/s]\n",
            "Step... (9000/15700 | Training Loss: 6.521410341520095e-06, Learning Rate: 1.7072612536139786e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.41it/s]\n",
            "INFO:__main__:Step... (9000/15700 | Validation metrics: {'__precision': 0.8288838854937279, '__recall': 0.8418817379941196, '__f1': 0.8353322528363049, '__number': 3061, 'overall_precision': 0.8288838854937279, 'overall_recall': 0.8418817379941196, 'overall_f1': 0.8353322528363049, 'overall_accuracy': 0.979997747811154}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.27it/s]\n",
            "INFO:__main__:Step... (9420/15700 | Validation metrics: {'__precision': 0.818785578747628, '__recall': 0.8458020254818687, '__f1': 0.8320745621083078, '__number': 3061, 'overall_precision': 0.818785578747628, 'overall_recall': 0.8458020254818687, 'overall_f1': 0.8320745621083078, 'overall_accuracy': 0.9796317671236733}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:16<00:00, 10.24it/s]\n",
            "Step... (9500/15700 | Training Loss: 5.649708327837288e-06, Learning Rate: 1.5798728782101534e-05)\n",
            "Step... (10000/15700 | Training Loss: 9.407747711520642e-05, Learning Rate: 1.4524841390084475e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.40it/s]\n",
            "INFO:__main__:Step... (10000/15700 | Validation metrics: {'__precision': 0.8282345442957297, '__recall': 0.8490689317216595, '__f1': 0.8385223423132763, '__number': 3061, 'overall_precision': 0.8282345442957297, 'overall_recall': 0.8490689317216595, 'overall_f1': 0.8385223423132763, 'overall_accuracy': 0.9804622617606487}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.33it/s]\n",
            "INFO:__main__:Step... (10205/15700 | Validation metrics: {'__precision': 0.8285070785070785, '__recall': 0.8412283567461614, '__f1': 0.8348192575782136, '__number': 3061, 'overall_precision': 0.8285070785070785, 'overall_recall': 0.8412283567461614, 'overall_f1': 0.8348192575782136, 'overall_accuracy': 0.9796317671236733}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.14it/s]\n",
            "Step... (10500/15700 | Training Loss: 1.1571902177820448e-05, Learning Rate: 1.325095581705682e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.32it/s]\n",
            "INFO:__main__:Step... (10990/15700 | Validation metrics: {'__precision': 0.8307839388145315, '__recall': 0.8516824567134923, '__f1': 0.8411034037748023, '__number': 3061, 'overall_precision': 0.8307839388145315, 'overall_recall': 0.8516824567134923, 'overall_f1': 0.8411034037748023, 'overall_accuracy': 0.9802370428760452}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:12<00:00, 10.83it/s]\n",
            "Step... (11000/15700 | Training Loss: 1.3607477740151808e-06, Learning Rate: 1.1977071153523866e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.42it/s]\n",
            "INFO:__main__:Step... (11000/15700 | Validation metrics: {'__precision': 0.8278818672594475, '__recall': 0.8516824567134923, '__f1': 0.8396135265700483, '__number': 3061, 'overall_precision': 0.8278818672594475, 'overall_recall': 0.8516824567134923, 'overall_f1': 0.8396135265700483, 'overall_accuracy': 0.980293347597196}\n",
            "Step... (11500/15700 | Training Loss: 4.0554496649747307e-07, Learning Rate: 1.0703186489990912e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.42it/s]\n",
            "INFO:__main__:Step... (11775/15700 | Validation metrics: {'__precision': 0.8229199372056515, '__recall': 0.8562561254491996, '__f1': 0.8392571245597182, '__number': 3061, 'overall_precision': 0.8229199372056515, 'overall_recall': 0.8562561254491996, 'overall_f1': 0.8392571245597182, 'overall_accuracy': 0.9800540525323048}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.19it/s]\n",
            "Step... (12000/15700 | Training Loss: 0.002544290851801634, Learning Rate: 9.429300007468555e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.01it/s]\n",
            "INFO:__main__:Step... (12000/15700 | Validation metrics: {'__precision': 0.8290435335239911, '__recall': 0.8523358379614505, '__f1': 0.8405283505154638, '__number': 3061, 'overall_precision': 0.8290435335239911, 'overall_recall': 0.8523358379614505, 'overall_f1': 0.8405283505154638, 'overall_accuracy': 0.9800822048928802}\n",
            "Step... (12500/15700 | Training Loss: 1.0512238986848388e-05, Learning Rate: 8.1554144344409e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.26it/s]\n",
            "INFO:__main__:Step... (12560/15700 | Validation metrics: {'__precision': 0.8286624203821656, '__recall': 0.8500490035935969, '__f1': 0.8392194807289147, '__number': 3061, 'overall_precision': 0.8286624203821656, 'overall_recall': 0.8500490035935969, 'overall_f1': 0.8392194807289147, 'overall_accuracy': 0.9802511190563329}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.08it/s]\n",
            "Step... (13000/15700 | Training Loss: 1.1176731362638748e-07, Learning Rate: 6.881530225655297e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.20it/s]\n",
            "INFO:__main__:Step... (13000/15700 | Validation metrics: {'__precision': 0.826904686005738, '__recall': 0.8474354786017642, '__f1': 0.8370442078089706, '__number': 3061, 'overall_precision': 0.826904686005738, 'overall_recall': 0.8474354786017642, 'overall_f1': 0.8370442078089706, 'overall_accuracy': 0.9802792714169083}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.37it/s]\n",
            "INFO:__main__:Step... (13345/15700 | Validation metrics: {'__precision': 0.831629392971246, '__recall': 0.850375694217576, '__f1': 0.8408980778549507, '__number': 3061, 'overall_precision': 0.831629392971246, 'overall_recall': 0.850375694217576, 'overall_f1': 0.8408980778549507, 'overall_accuracy': 0.98039188085921}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:17<00:00, 10.08it/s]\n",
            "Step... (13500/15700 | Training Loss: 2.5108756744884886e-06, Learning Rate: 5.607645562122343e-06)\n",
            "Step... (14000/15700 | Training Loss: 7.031329460005509e-06, Learning Rate: 4.3337604438420385e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.22it/s]\n",
            "INFO:__main__:Step... (14000/15700 | Validation metrics: {'__precision': 0.8282988871224165, '__recall': 0.8510290754655342, '__f1': 0.8395101514663229, '__number': 3061, 'overall_precision': 0.8282988871224165, 'overall_recall': 0.8510290754655342, 'overall_f1': 0.8395101514663229, 'overall_accuracy': 0.98039188085921}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.28it/s]\n",
            "INFO:__main__:Step... (14130/15700 | Validation metrics: {'__precision': 0.8289808917197452, '__recall': 0.850375694217576, '__f1': 0.8395420093533301, '__number': 3061, 'overall_precision': 0.8289808917197452, 'overall_recall': 0.850375694217576, 'overall_f1': 0.8395420093533301, 'overall_accuracy': 0.9806170997438135}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:16<00:00, 10.23it/s]\n",
            "Step... (14500/15700 | Training Loss: 1.6296826288453303e-07, Learning Rate: 3.0598735065723304e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.26it/s]\n",
            "INFO:__main__:Step... (14915/15700 | Validation metrics: {'__precision': 0.8302370275464446, '__recall': 0.8467820973538059, '__f1': 0.8384279475982532, '__number': 3061, 'overall_precision': 0.8302370275464446, 'overall_recall': 0.8467820973538059, 'overall_f1': 0.8384279475982532, 'overall_accuracy': 0.980293347597196}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:11<00:00, 11.02it/s]\n",
            "Step... (15000/15700 | Training Loss: 2.010792513829074e-06, Learning Rate: 1.785988729352539e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.39it/s]\n",
            "INFO:__main__:Step... (15000/15700 | Validation metrics: {'__precision': 0.8312020460358056, '__recall': 0.8493956223456387, '__f1': 0.8402003554693811, '__number': 3061, 'overall_precision': 0.8312020460358056, 'overall_recall': 0.8493956223456387, 'overall_f1': 0.8402003554693811, 'overall_accuracy': 0.9802651952366206}\n",
            "Step... (15500/15700 | Training Loss: 1.058638829931624e-07, Learning Rate: 5.121040089761664e-07)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 40/40 [00:04<00:00,  9.41it/s]\n",
            "INFO:__main__:Step... (15700/15700 | Validation metrics: {'__precision': 0.8320586360739325, '__recall': 0.8529892192094087, '__f1': 0.8423939345055654, '__number': 3061, 'overall_precision': 0.8320586360739325, 'overall_recall': 0.8529892192094087, 'overall_f1': 0.8423939345055654, 'overall_accuracy': 0.98039188085921}\n",
            "Training...: 100%|████████████████████████████| 785/785 [01:16<00:00, 10.26it/s]\n",
            "Epoch ... 20/20: 100%|██████████████████████████| 20/20 [26:24<00:00, 79.21s/it]\n",
            "Evaluating on Dev Set ...: 40it [00:04,  9.31it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8320586360739325, '__recall': 0.8529892192094087, '__f1': 0.8423939345055654, '__number': 3061, 'overall_precision': 0.8320586360739325, 'overall_recall': 0.8529892192094087, 'overall_f1': 0.8423939345055654, 'overall_accuracy': 0.98039188085921}\n",
            "Evaluating on Test Set...: 79it [00:08,  9.29it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.829924650161464, '__recall': 0.8532806324110672, '__f1': 0.8414405986903649, '__number': 6325, 'overall_precision': 0.829924650161464, 'overall_recall': 0.8532806324110672, 'overall_f1': 0.8414405986903649, 'overall_accuracy': 0.9802739344090893}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/tokcls/NCBI-disease_hf/train.json \\\n",
        "--validation_file data/tokcls/NCBI-disease_hf/dev.json \\\n",
        "--test_file data/tokcls/NCBI-disease_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 7e-5 \\\n",
        "--num_train_epochs 6 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task NCBI-disease \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c23b34-a21d-4a0b-dd69-266167821265",
        "id": "MLl61bkx18OZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-4e811e02322508d1\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-4e811e02322508d1/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 730.25it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense', 'bias'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense_prediction', 'kernel')}\n",
            "- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'bias'), ('classifier', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|███████████████| 6/6 [00:01<00:00,  3.20ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  3.26ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  3.15ba/s]\n",
            "INFO:__main__:Sample 3156 of the training set: {'input_ids': [2, 42, 4598, 16, 2069, 16, 3186, 6764, 1715, 17986, 13517, 13936, 6697, 11, 7530, 1021, 12, 1734, 2189, 1701, 1953, 1683, 11196, 5306, 1682, 1680, 28384, 5181, 4243, 11, 25096, 12, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, -100, 2, 0, -100, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 2576 of the training set: {'input_ids': [2, 2019, 15068, 4130, 8217, 2147, 15, 1680, 3695, 4230, 15498, 3649, 1725, 1898, 2033, 5530, 1734, 26, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 1171 of the training set: {'input_ids': [2, 21, 16, 3193, 4048, 1955, 9774, 1037, 1026, 6881, 18670, 1690, 9774, 1037, 1026, 17819, 1037, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, 2, 2, -100, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (6 epochs) =====\n",
            "Step... (500/4068 | Training Loss: 0.0020314771682024, Learning Rate: 6.141347694210708e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:07<00:00,  2.08it/s]\n",
            "INFO:__main__:Step... (678/4068 | Validation metrics: {'__precision': 0.8414322250639387, '__recall': 0.8360864040660737, '__f1': 0.8387507966857871, '__number': 787, 'overall_precision': 0.8414322250639387, 'overall_recall': 0.8360864040660737, 'overall_f1': 0.8387507966857871, 'overall_accuracy': 0.9870249071717635}\n",
            "Training...: 100%|████████████████████████████| 678/678 [01:56<00:00,  5.83it/s]\n",
            "Step... (1000/4068 | Training Loss: 0.0025280925910919905, Learning Rate: 5.280973346089013e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.86it/s]\n",
            "INFO:__main__:Step... (1000/4068 | Validation metrics: {'__precision': 0.8456865127582017, '__recall': 0.8843710292249047, '__f1': 0.8645962732919256, '__number': 787, 'overall_precision': 0.8456865127582017, 'overall_recall': 0.8843710292249047, 'overall_f1': 0.8645962732919256, 'overall_accuracy': 0.9875255538403771}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.54it/s]\n",
            "INFO:__main__:Step... (1356/4068 | Validation metrics: {'__precision': 0.7997685185185185, '__recall': 0.8780177890724269, '__f1': 0.8370684433676558, '__number': 787, 'overall_precision': 0.7997685185185185, 'overall_recall': 0.8780177890724269, 'overall_f1': 0.8370684433676558, 'overall_accuracy': 0.9861070549459718}\n",
            "Training...: 100%|████████████████████████████| 678/678 [00:44<00:00, 15.15it/s]\n",
            "Step... (1500/4068 | Training Loss: 0.0018292766762897372, Learning Rate: 4.420599725563079e-05)\n",
            "Step... (2000/4068 | Training Loss: 0.0007084003882482648, Learning Rate: 3.560226105037145e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.82it/s]\n",
            "INFO:__main__:Step... (2000/4068 | Validation metrics: {'__precision': 0.8539325842696629, '__recall': 0.8691232528589581, '__f1': 0.8614609571788413, '__number': 787, 'overall_precision': 0.8539325842696629, 'overall_recall': 0.8691232528589581, 'overall_f1': 0.8614609571788413, 'overall_accuracy': 0.9874838332846594}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.50it/s]\n",
            "INFO:__main__:Step... (2034/4068 | Validation metrics: {'__precision': 0.8467243510506799, '__recall': 0.8703939008894537, '__f1': 0.8583959899749374, '__number': 787, 'overall_precision': 0.8467243510506799, 'overall_recall': 0.8703939008894537, 'overall_f1': 0.8583959899749374, 'overall_accuracy': 0.9868580249488923}\n",
            "Training...: 100%|████████████████████████████| 678/678 [00:44<00:00, 15.24it/s]\n",
            "Step... (2500/4068 | Training Loss: 0.00037254535709507763, Learning Rate: 2.6998524845112115e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.90it/s]\n",
            "INFO:__main__:Step... (2712/4068 | Validation metrics: {'__precision': 0.8525798525798526, '__recall': 0.8818297331639136, '__f1': 0.8669581511555278, '__number': 787, 'overall_precision': 0.8525798525798526, 'overall_recall': 0.8818297331639136, 'overall_f1': 0.8669581511555278, 'overall_accuracy': 0.9883182443990154}\n",
            "Training...: 100%|████████████████████████████| 678/678 [00:42<00:00, 15.91it/s]\n",
            "Step... (3000/4068 | Training Loss: 2.261438567074947e-05, Learning Rate: 1.8394788639852777e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.87it/s]\n",
            "INFO:__main__:Step... (3000/4068 | Validation metrics: {'__precision': 0.8357142857142857, '__recall': 0.8919949174078781, '__f1': 0.8629379225568531, '__number': 787, 'overall_precision': 0.8357142857142857, 'overall_recall': 0.8919949174078781, 'overall_f1': 0.8629379225568531, 'overall_accuracy': 0.9879010388418373}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.93it/s]\n",
            "INFO:__main__:Step... (3390/4068 | Validation metrics: {'__precision': 0.858560794044665, '__recall': 0.8792884371029225, '__f1': 0.8688010043942248, '__number': 787, 'overall_precision': 0.858560794044665, 'overall_recall': 0.8792884371029225, 'overall_f1': 0.8688010043942248, 'overall_accuracy': 0.9878175977304018}\n",
            "Training...: 100%|████████████████████████████| 678/678 [00:44<00:00, 15.33it/s]\n",
            "Step... (3500/4068 | Training Loss: 3.038273462152574e-06, Learning Rate: 9.791049706109334e-06)\n",
            "Step... (4000/4068 | Training Loss: 2.0162526197964326e-05, Learning Rate: 1.1873144103446975e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.78it/s]\n",
            "INFO:__main__:Step... (4000/4068 | Validation metrics: {'__precision': 0.8617283950617284, '__recall': 0.8869123252858958, '__f1': 0.8741390106449592, '__number': 787, 'overall_precision': 0.8617283950617284, 'overall_recall': 0.8869123252858958, 'overall_f1': 0.8741390106449592, 'overall_accuracy': 0.9884434060661688}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 15/15 [00:01<00:00,  9.92it/s]\n",
            "INFO:__main__:Step... (4068/4068 | Validation metrics: {'__precision': 0.8608374384236454, '__recall': 0.8881829733163914, '__f1': 0.874296435272045, '__number': 787, 'overall_precision': 0.8608374384236454, 'overall_recall': 0.8881829733163914, 'overall_f1': 0.874296435272045, 'overall_accuracy': 0.988401685510451}\n",
            "Training...: 100%|████████████████████████████| 678/678 [00:44<00:00, 15.19it/s]\n",
            "Epoch ... 6/6: 100%|██████████████████████████████| 6/6 [05:37<00:00, 56.19s/it]\n",
            "Evaluating on Dev Set ...: 15it [00:01,  9.96it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8608374384236454, '__recall': 0.8881829733163914, '__f1': 0.874296435272045, '__number': 787, 'overall_precision': 0.8608374384236454, 'overall_recall': 0.8881829733163914, 'overall_f1': 0.874296435272045, 'overall_accuracy': 0.988401685510451}\n",
            "Evaluating on Test Set...: 15it [00:01,  9.73it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.8770325203252033, '__recall': 0.8989583333333333, '__f1': 0.8878600823045267, '__number': 960, 'overall_precision': 0.8770325203252033, 'overall_recall': 0.8989583333333333, 'overall_f1': 0.8878600823045267, 'overall_accuracy': 0.9862023921296486}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/tokcls/JNLPBA_hf/train.json \\\n",
        "--validation_file data/tokcls/JNLPBA_hf/dev.json \\\n",
        "--test_file data/tokcls/JNLPBA_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 2 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task JNLPBA \\\n",
        "--max_seq_length 512 \\\n",
        "--return_entity_level_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151ef537-331d-4553-9fb2-1360a0ee874a",
        "id": "sjuLD-8_18OZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-1d788b965ff358e0\r\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-1d788b965ff358e0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\r\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 705.24it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('electra', 'embeddings', 'position_ids'), ('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense', 'bias')}\n",
            "- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 17/17 [00:06<00:00,  2.78ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 2/2 [00:00<00:00,  2.54ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 4/4 [00:01<00:00,  2.96ba/s]\n",
            "INFO:__main__:Sample 12494 of the training set: {'input_ids': [2, 1755, 1680, 2187, 3883, 15, 1680, 2767, 3321, 1682, 21674, 10, 60, 2174, 1808, 1748, 2520, 1680, 2488, 2824, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 12149 of the training set: {'input_ids': [2, 1682, 11996, 1933, 15, 2459, 16, 23, 2397, 2181, 1744, 13617, 3843, 1798, 1680, 1999, 1685, 3603, 1772, 1680, 11354, 2724, 1685, 3028, 3603, 2446, 1701, 4925, 5243, 1682, 1680, 5884, 5123, 2943, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 0, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 0, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 7118 of the training set: {'input_ids': [2, 2724, 1685, 42, 2264, 3762, 1701, 1680, 4229, 18, 5973, 9615, 3811, 4602, 15544, 16, 1732, 2300, 1732, 2459, 16, 2073, 3057, 15, 3053, 2021, 2165, 6226, 2169, 1690, 2459, 16, 2265, 2181, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, -100, 2, -100, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 0, -100, -100, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (2 epochs) =====\n",
            "Step... (500/4200 | Training Loss: 0.012767086736857891, Learning Rate: 4.405952131492086e-05)\n",
            "Step... (1000/4200 | Training Loss: 0.006456938572227955, Learning Rate: 3.810714406426996e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:08<00:00,  3.32it/s]\n",
            "INFO:__main__:Step... (1000/4200 | Validation metrics: {'__precision': 0.7877416700946113, '__recall': 0.8415732805976709, '__f1': 0.8137681929246787, '__number': 4551, 'overall_precision': 0.7877416700946113, 'overall_recall': 0.8415732805976709, 'overall_f1': 0.8137681929246787, 'overall_accuracy': 0.9567012915867765}\n",
            "Step... (1500/4200 | Training Loss: 0.0033643224742263556, Learning Rate: 3.2154763175640255e-05)\n",
            "Step... (2000/4200 | Training Loss: 0.008672958239912987, Learning Rate: 2.6202380468021147e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:02<00:00,  9.66it/s]\n",
            "INFO:__main__:Step... (2000/4200 | Validation metrics: {'__precision': 0.7903957131079967, '__recall': 0.8426719402329158, '__f1': 0.8156971179410827, '__number': 4551, 'overall_precision': 0.7903957131079967, 'overall_recall': 0.8426719402329158, 'overall_f1': 0.8156971179410827, 'overall_accuracy': 0.9602831798740018}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:02<00:00,  9.69it/s]\n",
            "INFO:__main__:Step... (2100/4200 | Validation metrics: {'__precision': 0.7950348789495281, '__recall': 0.8514612173148759, '__f1': 0.8222811671087532, '__number': 4551, 'overall_precision': 0.7950348789495281, 'overall_recall': 0.8514612173148759, 'overall_f1': 0.8222811671087532, 'overall_accuracy': 0.9592086133878343}\n",
            "Training...: 100%|██████████████████████████| 2100/2100 [03:33<00:00,  9.84it/s]\n",
            "Step... (2500/4200 | Training Loss: 0.00370348384603858, Learning Rate: 2.0250001398380846e-05)\n",
            "Step... (3000/4200 | Training Loss: 0.005102566909044981, Learning Rate: 1.4297618690761738e-05)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:02<00:00,  9.65it/s]\n",
            "INFO:__main__:Step... (3000/4200 | Validation metrics: {'__precision': 0.8245873889123995, '__recall': 0.8562953197099539, '__f1': 0.8401422873773849, '__number': 4551, 'overall_precision': 0.8245873889123995, 'overall_recall': 0.8562953197099539, 'overall_f1': 0.8401422873773849, 'overall_accuracy': 0.9612734666357641}\n",
            "Step... (3500/4200 | Training Loss: 0.01220330223441124, Learning Rate: 8.345236892637331e-06)\n",
            "Step... (4000/4200 | Training Loss: 0.0035323791671544313, Learning Rate: 2.3928582777443808e-06)\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:02<00:00,  9.60it/s]\n",
            "INFO:__main__:Step... (4000/4200 | Validation metrics: {'__precision': 0.8241000215563699, '__recall': 0.8400351571083279, '__f1': 0.8319912948857454, '__number': 4551, 'overall_precision': 0.8241000215563699, 'overall_recall': 0.8400351571083279, 'overall_f1': 0.8319912948857454, 'overall_accuracy': 0.9615684456711826}\n",
            "Evaluating on Dev Set...: 100%|█████████████████| 28/28 [00:02<00:00,  9.66it/s]\n",
            "INFO:__main__:Step... (4200/4200 | Validation metrics: {'__precision': 0.8204195205479452, '__recall': 0.8422324763788178, '__f1': 0.8311829122845061, '__number': 4551, 'overall_precision': 0.8204195205479452, 'overall_recall': 0.8422324763788178, 'overall_f1': 0.8311829122845061, 'overall_accuracy': 0.9609995575314468}\n",
            "Training...: 100%|██████████████████████████| 2100/2100 [02:16<00:00, 15.34it/s]\n",
            "Epoch ... 2/2: 100%|█████████████████████████████| 2/2 [05:50<00:00, 175.13s/it]\n",
            "Evaluating on Dev Set ...: 28it [00:02,  9.74it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'__precision': 0.8204195205479452, '__recall': 0.8422324763788178, '__f1': 0.8311829122845061, '__number': 4551, 'overall_precision': 0.8204195205479452, 'overall_recall': 0.8422324763788178, 'overall_f1': 0.8311829122845061, 'overall_accuracy': 0.9609995575314468}\n",
            "Evaluating on Test Set...: 61it [00:06,  9.47it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'__precision': 0.7494895875867701, '__recall': 0.8476102516739783, '__f1': 0.7955358110304473, '__number': 8662, 'overall_precision': 0.7494895875867701, 'overall_recall': 0.8476102516739783, 'overall_f1': 0.7955358110304473, 'overall_accuracy': 0.9530775245202348}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run_flax_ner.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file data/tokcls/ebmnlp_hf/train.json \\\n",
        "--validation_file data/tokcls/ebmnlp_hf/dev.json \\\n",
        "--test_file data/tokcls/ebmnlp_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_test \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--learning_rate 1e-5 \\\n",
        "--num_train_epochs 1 \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--blurb_task \"EBM PICO\" \\\n",
        "--max_seq_length 512 \\\n",
        "--return_macro_metrics \\\n",
        "--eval_steps 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16607da1-d49d-431c-e547-6d559ae1d6fe",
        "id": "XhUXZkKO18OZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-66c19f981d7e86e9\n",
            "WARNING:datasets.builder:Found cached dataset json (/home/big35manf/.cache/huggingface/datasets/json/default-66c19f981d7e86e9/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 634.12it/s]\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-INT\",\n",
            "    \"1\": \"B-OUT\",\n",
            "    \"2\": \"B-PAR\",\n",
            "    \"3\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-INT\": 0,\n",
            "    \"B-OUT\": 1,\n",
            "    \"B-PAR\": 2,\n",
            "    \"O\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-Discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "Loading PyTorch weights from /home/big35manf/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-Discriminator/snapshots/8bcc387785592aec3de94134a0c9db5ef6b633e6/pytorch_model.bin\n",
            "PyTorch checkpoint contains 108,233,985 parameters.\n",
            "Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing FlaxElectraForTokenClassification: {('discriminator_predictions', 'dense_prediction', 'kernel'), ('discriminator_predictions', 'dense_prediction', 'bias'), ('discriminator_predictions', 'dense', 'kernel'), ('discriminator_predictions', 'dense', 'bias'), ('electra', 'embeddings', 'position_ids')}\n",
            "- This IS expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of FlaxElectraForTokenClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: {('classifier', 'kernel'), ('classifier', 'bias')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100%|█████████████| 41/41 [00:14<00:00,  2.80ba/s]\n",
            "Running tokenizer on dataset: 100%|█████████████| 11/11 [00:03<00:00,  2.95ba/s]\n",
            "Running tokenizer on dataset: 100%|███████████████| 3/3 [00:00<00:00,  4.23ba/s]\n",
            "INFO:__main__:Sample 26768 of the training set: {'input_ids': [2, 1890, 2564, 1829, 3559, 3993, 1725, 20, 16, 25, 5079, 3648, 11, 3721, 22, 12, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 7619 of the training set: {'input_ids': [2, 2100, 1734, 1982, 20559, 4588, 15, 2027, 2842, 11501, 5054, 7100, 2105, 42, 1932, 9098, 1701, 5453, 1676, 1682, 2076, 1999, 1685, 16722, 1781, 1701, 5030, 4306, 1005, 2520, 1680, 23, 49, 1970, 2445, 3305, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 3, 3, 1, 3, 3, 3, -100, -100, 3, 3, 3, 3, 3, 3, 3, -100, 3, 3, 1, 1, 1, 3, 3, 1, 1, -100, 3, 3, 3, 3, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "INFO:__main__:Sample 28274 of the training set: {'input_ids': [2, 8984, 4173, 1685, 7642, 7335, 1013, 16, 5613, 2326, 2301, 8542, 7138, 1690, 2229, 2991, 6311, 2169, 1981, 8984, 4173, 1685, 2388, 1013, 16, 5613, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 0, 1, -100, -100, -100, -100, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 3, 0, 0, -100, -100, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n",
            "WARNING:__main__:Unable to display metrics through TensorBoard because some package are not installed: No module named 'tensorflow'\n",
            "INFO:__main__:===== Starting training (1 epochs) =====\n",
            "Step... (500/5116 | Training Loss: 0.05409359559416771, Learning Rate: 9.024628525367007e-06)\n",
            "Step... (1000/5116 | Training Loss: 0.02044353261590004, Learning Rate: 8.04730188974645e-06)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [00:22<00:00,  7.26it/s]\n",
            "INFO:__main__:Step... (1000/5116 | Validation metrics: {'macro_precision': 0.78764402162851, 'macro_recall': 0.5950098443701922, 'macro_f1': 0.6768105051230471}\n",
            "Step... (1500/5116 | Training Loss: 0.024349678307771683, Learning Rate: 7.069976163620595e-06)\n",
            "Step... (2000/5116 | Training Loss: 0.014615470543503761, Learning Rate: 6.092649982747389e-06)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [00:16<00:00,  9.76it/s]\n",
            "INFO:__main__:Step... (2000/5116 | Validation metrics: {'macro_precision': 0.7510603792948501, 'macro_recall': 0.6696508765277187, 'macro_f1': 0.7073714350352033}\n",
            "Step... (2500/5116 | Training Loss: 0.01758958399295807, Learning Rate: 5.115324256621534e-06)\n",
            "Step... (3000/5116 | Training Loss: 0.02495567500591278, Learning Rate: 4.137998075748328e-06)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [00:16<00:00,  9.72it/s]\n",
            "INFO:__main__:Step... (3000/5116 | Validation metrics: {'macro_precision': 0.7416805982152862, 'macro_recall': 0.6880657536890187, 'macro_f1': 0.7138467701657397}\n",
            "Step... (3500/5116 | Training Loss: 0.04039796069264412, Learning Rate: 3.1606721222487977e-06)\n",
            "Step... (4000/5116 | Training Loss: 0.012933147139847279, Learning Rate: 2.1833461687492672e-06)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [00:16<00:00,  9.76it/s]\n",
            "INFO:__main__:Step... (4000/5116 | Validation metrics: {'macro_precision': 0.7366963860768867, 'macro_recall': 0.7013282231138871, 'macro_f1': 0.7177918154471977}\n",
            "Step... (4500/5116 | Training Loss: 0.016464589163661003, Learning Rate: 1.206020101562899e-06)\n",
            "Step... (5000/5116 | Training Loss: 0.03299761191010475, Learning Rate: 2.2869407700909505e-07)\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [00:16<00:00,  9.67it/s]\n",
            "INFO:__main__:Step... (5000/5116 | Validation metrics: {'macro_precision': 0.7424941208332191, 'macro_recall': 0.6968466096787619, 'macro_f1': 0.7186257133617646}\n",
            "Evaluating on Dev Set...: 100%|███████████████| 163/163 [00:16<00:00,  9.79it/s]\n",
            "INFO:__main__:Step... (5116/5116 | Validation metrics: {'macro_precision': 0.7431805131135479, 'macro_recall': 0.6959908701003226, 'macro_f1': 0.718503570341896}\n",
            "Training...: 100%|██████████████████████████| 5116/5116 [08:29<00:00, 10.04it/s]\n",
            "Epoch ... 1/1: 100%|█████████████████████████████| 1/1 [08:29<00:00, 509.53s/it]\n",
            "Evaluating on Dev Set ...: 163it [00:16,  9.75it/s]\n",
            "INFO:__main__:Evaluation Results on Dev Set : {'macro_precision': 0.7431805131135479, 'macro_recall': 0.6959908701003226, 'macro_f1': 0.718503570341896}\n",
            "Evaluating on Test Set...: 33it [00:03,  9.82it/s]\n",
            "INFO:__main__:Evaluation Results on Test Set : {'macro_precision': 0.7392165136837524, 'macro_recall': 0.7659928116721266, 'macro_f1': 0.7423001060646482}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HoC Results**"
      ],
      "metadata": {
        "id": "QQhRkwTIpT8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we the GPU (V100) with Google Colab to generate our results for HoC."
      ],
      "metadata": {
        "id": "QrsaD5J2pW-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVW1aWuXKYTQ",
        "outputId": "c7467c7a-64b9-4281-c7c8-22f194237155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 29 23:10:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    22W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKo6KsLG2Wj3",
        "outputId": "2d6e2d09-229c-4278-cb45-54744ae4c0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LinkBERT'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 42 (delta 9), reused 5 (delta 5), pack-reused 29\u001b[K\n",
            "Unpacking objects: 100% (42/42), 1.49 MiB | 3.90 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/michiyasunaga/LinkBERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.29.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52LfPLWg26pb",
        "outputId": "913d345e-6011-48de-f5ed-54b0f2753b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.29.1\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.29.1)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.1) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp_OAGPHB3w_",
        "outputId": "ce94c7dd-95bf-473d-c255-b9d88c06e1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIBmwOcU5ezz",
        "outputId": "99a6e220-07d4-4edd-8a9e-56b314b1c96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
        "!unzip  data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUFZWhOn3PQW",
        "outputId": "58a4098e-b580-4fe5-813a-a9a0632cb689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-29 23:11:18--  https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 458233246 (437M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 437.00M  21.7MB/s    in 24s     \n",
            "\n",
            "2023-05-29 23:11:43 (18.5 MB/s) - ‘data.zip’ saved [458233246/458233246]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/tokcls/\n",
            "   creating: data/tokcls/BC5CDR-disease_hf/\n",
            "  inflating: data/tokcls/BC5CDR-disease_hf/test.json  \n",
            "  inflating: data/tokcls/BC5CDR-disease_hf/train.json  \n",
            "  inflating: data/tokcls/BC5CDR-disease_hf/dev.json  \n",
            "   creating: data/tokcls/BC2GM_hf/\n",
            "  inflating: data/tokcls/BC2GM_hf/train.json  \n",
            "  inflating: data/tokcls/BC2GM_hf/dev.json  \n",
            "  inflating: data/tokcls/BC2GM_hf/test.json  \n",
            "   creating: data/tokcls/ebmnlp_hf/\n",
            "  inflating: data/tokcls/ebmnlp_hf/test.json  \n",
            "  inflating: data/tokcls/ebmnlp_hf/train.json  \n",
            "  inflating: data/tokcls/ebmnlp_hf/dev.json  \n",
            "   creating: data/tokcls/NCBI-disease_hf/\n",
            "  inflating: data/tokcls/NCBI-disease_hf/test.json  \n",
            "  inflating: data/tokcls/NCBI-disease_hf/dev.json  \n",
            "  inflating: data/tokcls/NCBI-disease_hf/train.json  \n",
            "   creating: data/tokcls/JNLPBA_hf/\n",
            "  inflating: data/tokcls/JNLPBA_hf/dev.json  \n",
            "  inflating: data/tokcls/JNLPBA_hf/train.json  \n",
            "  inflating: data/tokcls/JNLPBA_hf/test.json  \n",
            "   creating: data/tokcls/BC5CDR-chem_hf/\n",
            "  inflating: data/tokcls/BC5CDR-chem_hf/dev.json  \n",
            "  inflating: data/tokcls/BC5CDR-chem_hf/test.json  \n",
            "  inflating: data/tokcls/BC5CDR-chem_hf/train.json  \n",
            "   creating: data/mc/\n",
            "   creating: data/mc/mmlu_hf/\n",
            "   creating: data/mc/mmlu_hf/professional_medicine/\n",
            "  inflating: data/mc/mmlu_hf/professional_medicine/dev.json  \n",
            "  inflating: data/mc/mmlu_hf/professional_medicine/test.json  \n",
            "  inflating: data/mc/mmlu_hf/professional_medicine/val.json  \n",
            "   creating: data/mc/medqa_usmle_hf/\n",
            "  inflating: data/mc/medqa_usmle_hf/test.json  \n",
            "  inflating: data/mc/medqa_usmle_hf/dev.json  \n",
            "  inflating: data/mc/medqa_usmle_hf/train.json  \n",
            "   creating: data/qa/\n",
            "   creating: data/qa/naturalqa_hf/\n",
            "  inflating: data/qa/naturalqa_hf/train_0.1.json  \n",
            "  inflating: data/qa/naturalqa_hf/train.json  \n",
            "  inflating: data/qa/naturalqa_hf/test.json  \n",
            "  inflating: data/qa/naturalqa_hf/dev.json  \n",
            "   creating: data/qa/triviaqa_hf/\n",
            "  inflating: data/qa/triviaqa_hf/train_0.1.json  \n",
            "  inflating: data/qa/triviaqa_hf/train.json  \n",
            "  inflating: data/qa/triviaqa_hf/dev.json  \n",
            "  inflating: data/qa/triviaqa_hf/test.json  \n",
            "   creating: data/qa/squad_hf/\n",
            "  inflating: data/qa/squad_hf/test.json  \n",
            "  inflating: data/qa/squad_hf/dev.json  \n",
            "  inflating: data/qa/squad_hf/train_0.1.json  \n",
            "  inflating: data/qa/squad_hf/train.json  \n",
            "   creating: data/qa/newsqa_hf/\n",
            "  inflating: data/qa/newsqa_hf/dev.json  \n",
            "  inflating: data/qa/newsqa_hf/test.json  \n",
            "  inflating: data/qa/newsqa_hf/train.json  \n",
            "   creating: data/qa/searchqa_hf/\n",
            "  inflating: data/qa/searchqa_hf/test.json  \n",
            "  inflating: data/qa/searchqa_hf/train.json  \n",
            "  inflating: data/qa/searchqa_hf/dev.json  \n",
            "   creating: data/qa/hotpot_hf/\n",
            "  inflating: data/qa/hotpot_hf/test.json  \n",
            "  inflating: data/qa/hotpot_hf/dev.json  \n",
            "  inflating: data/qa/hotpot_hf/train.json  \n",
            "  inflating: data/qa/hotpot_hf/train_0.1.json  \n",
            "   creating: data/seqcls/\n",
            "   creating: data/seqcls/hoc_hf/\n",
            "  inflating: data/seqcls/hoc_hf/dev.json  \n",
            "  inflating: data/seqcls/hoc_hf/train.json  \n",
            "  inflating: data/seqcls/hoc_hf/test.json  \n",
            "   creating: data/seqcls/BIOSSES_hf/\n",
            "  inflating: data/seqcls/BIOSSES_hf/train.json  \n",
            "  inflating: data/seqcls/BIOSSES_hf/dev.json  \n",
            "  inflating: data/seqcls/BIOSSES_hf/test.json  \n",
            "   creating: data/seqcls/GAD_hf/\n",
            "  inflating: data/seqcls/GAD_hf/train.json  \n",
            "  inflating: data/seqcls/GAD_hf/dev.json  \n",
            "  inflating: data/seqcls/GAD_hf/test.json  \n",
            "   creating: data/seqcls/HoC_hf/\n",
            "  inflating: data/seqcls/HoC_hf/dev.json  \n",
            "  inflating: data/seqcls/HoC_hf/test.json  \n",
            "  inflating: data/seqcls/HoC_hf/train.json  \n",
            "   creating: data/seqcls/chemprot_hf/\n",
            "  inflating: data/seqcls/chemprot_hf/train.json  \n",
            "  inflating: data/seqcls/chemprot_hf/dev.json  \n",
            "  inflating: data/seqcls/chemprot_hf/test.json  \n",
            "   creating: data/seqcls/bioasq_hf/\n",
            "  inflating: data/seqcls/bioasq_hf/test.json  \n",
            "  inflating: data/seqcls/bioasq_hf/dev.json  \n",
            "  inflating: data/seqcls/bioasq_hf/train.json  \n",
            "   creating: data/seqcls/pubmedqa_hf/\n",
            "  inflating: data/seqcls/pubmedqa_hf/dev.json  \n",
            "  inflating: data/seqcls/pubmedqa_hf/test.json  \n",
            "  inflating: data/seqcls/pubmedqa_hf/train.json  \n",
            "   creating: data/seqcls/DDI_hf/\n",
            "  inflating: data/seqcls/DDI_hf/train.json  \n",
            "  inflating: data/seqcls/DDI_hf/dev.json  \n",
            "  inflating: data/seqcls/DDI_hf/test.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BioM-BERT-Large"
      ],
      "metadata": {
        "id": "1x73A9IR4-OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /home/sultan/LinkBERT/src/seqcls/run_seqcls.py --model_name_or_path sultan/BioM-BERT-PubMed-PMC-Large \\\n",
        "--train_file /home/sultan/data/seqcls/hoc_hf/train.json \\\n",
        "--validation_file /home/sultan/data/seqcls/hoc_hf/dev.json \\\n",
        "--test_file /home/sultan/data/seqcls/hoc_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--metric_name hoc \\\n",
        "--per_device_train_batch_size 32 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 3 \\\n",
        "--warmup_ratio 0 \\\n",
        "--max_seq_length 128 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgluISiD47e3",
        "outputId": "678c0a75-c4fd-4d55-ec76-014e707729de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/29/2023 19:03:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "05/29/2023 19:03:51 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1375.93it/s]\n",
            "is_multiclass_binary\n",
            "[WARNING|modeling_utils.py:3175] 2023-05-29 19:03:54,772 >> Some weights of the model checkpoint at sultan/BioM-BERT-PubMed-PMC-Large were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3187] 2023-05-29 19:03:54,772 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-BERT-PubMed-PMC-Large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "05/29/2023 19:03:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e4eb7fee8886fecb.arrow\n",
            "05/29/2023 19:03:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3b48c03e7c66df32.arrow\n",
            "05/29/2023 19:03:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a9dc01b981b9496.arrow\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "{'loss': 0.1114, 'learning_rate': 1.683377308707124e-05, 'epoch': 1.32}\n",
            "{'loss': 0.0444, 'learning_rate': 3.641160949868074e-06, 'epoch': 2.64}\n",
            "{'train_runtime': 290.5133, 'train_samples_per_second': 125.147, 'train_steps_per_second': 3.914, 'train_loss': 0.07254989082182094, 'epoch': 3.0}\n",
            "100%|███████████████████████████████████████| 1137/1137 [04:49<00:00,  3.92it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0725\n",
            "  train_runtime            = 0:04:50.51\n",
            "  train_samples            =      12119\n",
            "  train_samples_per_second =    125.147\n",
            "  train_steps_per_second   =      3.914\n",
            " 98%|████████████████████████████████████████▎| 221/225 [00:05<00:00, 41.40it/s]/home/sultan/LinkBERT/src/seqcls/run_seqcls.py:475: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "There are 186 documents in the data set\n",
            "100%|█████████████████████████████████████████| 225/225 [00:06<00:00, 32.46it/s]\n",
            "***** eval metrics *****\n",
            "  epoch          =                3.0\n",
            "  eval_F1        = 0.8270377733598409\n",
            "  eval_precision =  0.803088803088803\n",
            "  eval_recall    = 0.8524590163934426\n",
            "  eval_samples   =               1798\n",
            "100%|████████████████████████████████████████▊| 442/444 [00:10<00:00, 41.40it/s]There are 371 documents in the data set\n",
            "***** test metrics *****\n",
            "  epoch          =                3.0\n",
            "  test_F1        = 0.8390918065153011\n",
            "  test_precision = 0.8034026465028355\n",
            "  test_recall    =  0.878099173553719\n",
            "  test_samples   =               3547\n",
            "100%|█████████████████████████████████████████| 444/444 [00:11<00:00, 37.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /home/sultan/LinkBERT/src/seqcls/run_seqcls.py --model_name_or_path sultan/BioM-ELECTRA-Base-Discriminator \\\n",
        "--train_file /home/sultan/data/seqcls/hoc_hf/train.json \\\n",
        "--validation_file /home/sultan/data/seqcls/hoc_hf/dev.json \\\n",
        "--test_file /home/sultan/data/seqcls/hoc_hf/test.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--metric_name hoc \\\n",
        "--per_device_train_batch_size 24 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--num_train_epochs 6 \\\n",
        "--warmup_ratio 0 \\\n",
        "--max_seq_length 128 \\\n",
        "--save_strategy no \\\n",
        "--evaluation_strategy no \\\n",
        "--output_dir out \\\n",
        "--overwrite_output_dir \\\n",
        "--fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7wDflug7CZw",
        "outputId": "becd4a35-0e9a-4a6e-d783-87e1c4d03711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/30/2023 04:02:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "05/30/2023 04:02:28 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1319.93it/s]\n",
            "is_multiclass_binary\n",
            "[WARNING|modeling_utils.py:3175] 2023-05-30 04:02:30,707 >> Some weights of the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3187] 2023-05-30 04:02:30,707 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at sultan/BioM-ELECTRA-Base-Discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "05/30/2023 04:02:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7491304363316b44.arrow\n",
            "05/30/2023 04:02:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad00dda6cd31c278.arrow\n",
            "05/30/2023 04:02:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e6daeb8973830076/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c613563f5c16d584.arrow\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "{'loss': 0.1416, 'learning_rate': 2.504950495049505e-05, 'epoch': 0.99}\n",
            "{'loss': 0.0658, 'learning_rate': 2.00990099009901e-05, 'epoch': 1.98}\n",
            "{'loss': 0.0425, 'learning_rate': 1.514851485148515e-05, 'epoch': 2.97}\n",
            "{'loss': 0.0299, 'learning_rate': 1.0198019801980198e-05, 'epoch': 3.96}\n",
            "{'loss': 0.0206, 'learning_rate': 5.2475247524752475e-06, 'epoch': 4.95}\n",
            "{'loss': 0.015, 'learning_rate': 2.9702970297029703e-07, 'epoch': 5.94}\n",
            "{'train_runtime': 216.8895, 'train_samples_per_second': 335.258, 'train_steps_per_second': 13.97, 'train_loss': 0.052211145855019195, 'epoch': 6.0}\n",
            "100%|███████████████████████████████████████| 3030/3030 [03:36<00:00, 14.01it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        6.0\n",
            "  train_loss               =     0.0522\n",
            "  train_runtime            = 0:03:36.88\n",
            "  train_samples            =      12119\n",
            "  train_samples_per_second =    335.258\n",
            "  train_steps_per_second   =      13.97\n",
            "100%|█████████████████████████████████████████| 225/225 [00:02<00:00, 87.95it/s]/home/sultan/LinkBERT/src/seqcls/run_seqcls.py:475: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "There are 186 documents in the data set\n",
            "100%|█████████████████████████████████████████| 225/225 [00:04<00:00, 54.67it/s]\n",
            "***** eval metrics *****\n",
            "  epoch          =                6.0\n",
            "  eval_F1        = 0.8201160541586073\n",
            "  eval_precision = 0.7765567765567766\n",
            "  eval_recall    = 0.8688524590163934\n",
            "  eval_samples   =               1798\n",
            "100%|█████████████████████████████████████████| 444/444 [00:05<00:00, 87.41it/s]There are 371 documents in the data set\n",
            "***** test metrics *****\n",
            "  epoch          =                6.0\n",
            "  test_F1        = 0.8413793103448275\n",
            "  test_precision = 0.8041431261770244\n",
            "  test_recall    = 0.8822314049586777\n",
            "  test_samples   =               3547\n",
            "100%|█████████████████████████████████████████| 444/444 [00:06<00:00, 69.42it/s]\n"
          ]
        }
      ]
    }
  ]
}